* Linear Regression Analysis

** 3 Linear Regression: Estimation and Distribution Theory

*** 3.6

Orthogonal designs minimize the variance of your final coefficients (regardless of error distribution, only assuming spherical errors).

See Chapter 3, Part e, Exercise 3. Follows by Cramer's Rule.

Use orthogonal designs to find optimal experiments. See Chapter 3, Part e, Exercise 3.

*** 3.7

Augmenting linear regression models with additional covariates can be done through orthogonalization.

Basically, if you have \[Y\sim X\beta+\varepsilon\] with the OLS estimate,
let 
\[P=(X^\top X)^{-1}X^\top\] be the projection matrix and \[R=I-P\] be the
residual operator.

Then to fit the new model \[Y\sim X\beta+Z\gamma+\varepsilon\], you project out \[X\] from
both the signal and the new covariates:

\[\hat\gamma = MZ^\top RY=(ZR^\top RZ)^{-1}(RZ)^\top RY\]

where \[M=Z^\top RZ\] and we rely on symmetry and idempotence of \[R\]. Then the
OLS fit of \[\gamma\] in the augmented model, \[\hat\gamma\] is the OLS fit
of the projected out setting of residuals \[RY\] on covariates \[RZ\].

Then the adjusted \[\hat\beta\] in the augmented model excludes the variance
explained by \[\hat\gamma\], i.e., we use the OLS solution \[\hat\beta=P(Y-Z\hat\gamma)\],
where without the \[Z\hat\gamma\] term we have the plain old OLS for the smaller
model \[Y\sim X\beta+\varepsilon\]


*** 3.8 Estimation with Linear Restrictions

Given a least squares problem where we also know linear constraints that the output must satisfy, i.e., \[A\beta=c\],
then we can adjust the least squares solution using Lagrange multipliers to get

\[\hat\beta'=\hat\beta+(X^\top X)^{-1}A^\top(A(X^\top X)^{-1}A^\top)^{-1}(c-A\hat\beta)\]

where \[\hat\beta\] is the OLS estimate. An example use here is if you measure the angles of a triangle with the same
measuring tool.


*** 3.9 Design Matrix of Less than Full Rank

For less-than-full-rank designs \[X\], \[\beta\] is not identifiable (since distinct values can give rise to the same observations).

Restricting to linear subspaces by adding constraints on \[\beta\] can help, as can dropping columns.

With the rank less than full rank, we can still estimate linear some functions of \[\beta\] with linear
estimators. In particular, linear reductions
\[\mathbf{a}^\top \boldsymbol\beta\] are estimable iff \[\mathbf{a}\in\mathcal{C}(X^\top)\], since any linear estimator is of the form \[\mathbf{b}^\top Y\]
which is \[\mathbf{b}^\top X \boldsymbol\beta\] in expectation.


Interestingly, Question 4 of Exercises 3i then implies that 
\[\mathbf{a}\in\mathcal{C}(X^\top)\] iff \[\mathbf{a}\] is a unit eigenvector
of \[X^\top X (X^\top X)^{-}\] where \[A^{-}\] is any generalized matrix inverse.

*** 3.10 Generalized Least Squares

For a least-squares model defined by \[\mathbb{E}[Y]= X\boldsymbol\beta\] and \[\mathrm{var}(Y)= V\],
by applying the transformation \[V^{-1/2}\], which exists assuming
positive definiteness (else, after rotation, one of the \[Y\] values
is a constant), we recover a transformed OLS model.

The transformed OLS model has all our original BLUE (best linear unbiased estimator)
guarantees. The least squares estimate for the non-homoskedastic case
can be derived from this, it's just

\[\boldsymbol\beta^*=(X^\top V^{-1} X)^{-1}X^\top V^{-1} Y\]

*** 3.11 Centering and Scaling the Explanatory Variables

Centering and scaling explanatory variables doesn't affect the model RSS or any identifiability
properties, because the relationship with \[X\] and \[\hat\beta\] is one-to-one.

But centered variables (except the intercept) do result in \[\hat\beta_0=\overline{Y}\].

Similarly, centered and scaled variables \[X^*\] (i.e., each column has unit norm and the average of
its values should be 0, starting from the design \[X\]) have a simplified projection matrix form,
where the original model \[Y= X\boldsymbol\beta+\boldsymbol\varepsilon= X^*\boldsymbol\gamma+\boldsymbol\varepsilon\]
where \[\hat{\boldsymbol\gamma}=R^{-1}X^{*\top}Y\], where \[R\] is the correlation matrix of \[X\].


*** 3.12 Bayesian Estimation

Frequentist confidence intervals (based on Student's t) can be derived for an OLS model by using a uniform
(improper) prior for \[\boldsymbol\beta,\log\sigma\] and computing the marginal posterior over \[\boldsymbol\beta\]
by marginalizing \[\sigma\]. This is an interesting coincidence and a neat alternative interpretation.

A more informed prior is specified using a conjugate prior for computational ease,
starting with a Normal distribution and ending with one in the posterior (when conditioned on the scale)
for \[\boldsymbol\beta\] and keeping   \[\sigma^2\] inverse Gamma.

When you marginalize the scale, you recover a Student's t distribution over coefficients.
 

*** 3.13 Robust Regression

Different sensitivity measures help us evaluate properties of regression algorithms.
We'll use the fixed effects model where \[\mathbb{E}[Y]=X\boldsymbol\beta\] but will allow
varying kinds of errors and even interventions on some of the explanatory variables.

All methods will apply to residuals \[e_i(\boldsymbol\beta)=\mathbf{x}^\top\boldsymbol\beta-y_i\],
and may introduce and optimize over auxiliary nuisance parameters. First we review different sensitivity
measures.

 - response breakdown point :: the proportion of the response data that must be modified (by any amount) to result in arbitrary change in the learned coefficients.
 - explanatory breakdown point :: as above, but proportion of explanatory rows
 - influence curves :: consider the convex interpolation between a single pathological point and the empirical distribution, as the weight of the pathological point tends to zero what is its influence on the learned coefficients?

A complimentary metric to the above robustness measures is *asymptotic relative efficiency*,
under the OLS model with normal errors.

The below is a summary table of discussed estimators in the chapter.



| estimator         | IC                    | heavy tails | BP  |         ARE |
|-------------------+-----------------------+-------------+-----+-------------|
| OLS               | unbounded in both x,y | no          | 1/n |           1 |
| LAD               | bdd in y not x        | yes         | 1/n |         43% |
| Huber             | bdd in y not x        | yes         | 1/n | ~1 if tuned |
| LMS               | bdd                   | yes         | 1/2 |          0% |
| LTS               | bdd                   | yes         | 1/2 |          8% |
| S-estimator       | probably not in x?    | probably    | 1/2 |        ~29% |
| R-estimator       | bdd                   | yes         | 1/2 |          7% |
| Mallows, LQD, LTD | not bdd               | probably    | 1/2 |        >50% |


 - IC :: influence curve
 - BP :: break point
 - ARE :: asymptotic relative efficiency
 - OLS :: ordinary least squares \[\sum_ie_i^2\]
 - LAD :: least absolute deviation \[\sum_i\left|e_i\right|\]
 - Huber :: a specific M-estimate of \[\sum_i\rho(e_i/s)\] with \[\rho\] interpolating between squared and absolute error in and outside the unit ball.
 - LMS :: least median square \[\mathrm{median}_ie_i\]
 - LTS :: least trimmed squares \[\sum_{i=1}^h e_{(i)}\] where \[e_{(i)}\] are ordered (ascending) residuals, typically only lowest, \[n/2\] terms
 - GM :: generalized M-estimators and one-step GM estimators merged with TMS/ LTS give best of both worlds under covariate distribution assumptions
 - S-estimators :: (biweight function in particular) solve obscure Huber-like polynomial
 - R-estimators :: only use order information, and need to use absolute residuals for guarnatees
 - Mallows, LQD, LTD :: various combinations of differences of residuals, tuned to not be affected by outliers

Moment equations \[\sum_i\psi(e_i/s)\mathbf{x}_i=0\] where \[\psi=\rho'\] and \[\sum_i\chi(e_i/s)=0\] for some scale function \[\chi\]
are generalizations of M-estimates that don't require a well-defined density to be defined.

Not discussed above is some pathological "instability" criterion (Fig. 3.2) that seems to only be
a worst-case scenario for LMS, LTS that isn't terribly realistic.

Overall, pairwise residual approaches allow optimizing high BP while maintaining ARE, but don't do
well from an IC perspective. This is consistent because IC is a "high-fidelity" measure of the impact
of a small deviation, but BP allow arbitrary differences and expect arbitrary changes, so are a crude
set of requirements.

It would be interesting to combine moderate trimming LTS with Huber to get heavy tail resistance
and bounded influence for x and y while maintaining ARE.

** 4 Hypothesis Testing

*** 4.1 Introduction

*** 4.2 Likelihood Ratio Test

Frame a hypothesis test comparing two nested models as assessing a wider linear regression and
its residual sum of squares, and then a constrained model that meets a linearity condition
\[A\boldsymbol\beta=\mathbf{c}\] (usually, \[A\] is partly an identity matrix and partly zero,
and \[\mathbf{c}\] is zero).

This can be fit with incremental OLS approaches (for the identity case) or Lagrange multipliers
from the last chapter.

*** 4.3 F-test

The test statistic of a likelihood ratio (the ratio of the fitted model's normal distributions
on the observed values) is not convenient, but equivalent to an F-statistic, which is
just a ratio of \[\chi^2\] variables via a transformation.

This can be expressed conveniently as a ratio of the residual sum of squares (RSS).
See Theorem 4.1 for an exposition under general linear constraints from [[4.2 Likelihood Ratio Test]].

From the exercises, the only neat computational observation is that if the saturated (full) model
has projection matrix \[P\] for its OLS fit and the reduced (nested) model has a projection \[P_H\]
which arises from the constrained OLS, then necessarily we have \[P_HP=PP_H=P_H\] with both matrices
idempotent.

[[4.5 Canonical Form of H]] discusses wider applications.

*** 4.4 Multiple Correlation Coefficient

Nothing terribly cool here, just that if \[R\] is correlation between \[Y,\hat Y\] then
by Thm. 4.2., \[R^2=1-(n-1)\mathrm{RSS}/S^2\].

Then for the general Thm. 4.1 setting \[F=\frac{R^2-R_H^2}{1-R^2}\cdot\frac{n-p}{q}\].

*** 4.5 Canonical form for H

\[F_{q, n-p}\] is the positive distribution of the ratios of independent \[\chi^2_q/q\] and
\[\chi^2/(n-p)\] variables.

It is equivalent to an [[4.2 Likelihood Ratio Test][LRT]] b/c for Gaussian errors the test functions are the same.

Note there isn't a multivariate or two-sided UMP test here, so when we design tests here we look
for weaker criteria like asymptotic minimax optimality.

Given a general model \[Y=X\boldsymbol\beta+\boldsymbol\varepsilon\], \[\boldsymbol\varepsilon\sim N(0, \sigma^2 I)\],
where \[\boldsymbol\beta\in\mathbb{R}^p\] and a nested hypothesis \[H:A\boldsymbol\beta=\mathbf{c}\],
over \[q\le p\le n\] non-redundant constraints, Theorem 4.1 describes how to set up the hypothesis test,
by leveraging idempotent, symmetric projection matrices \[P,P_H\] for the OLS and OLS-with-constraints
models, where they observe the nestedness \[PP_H=P_HP=P_H\].

Thus  \[\underbrace{\mathrm{RSS}_H}_{\chi^2_{n-p-q}}-\underbrace{\mathrm{RSS}}_{\chi^2_{n-p}}\] is itself an independent \[\chi^2_q\] rv.

Then the statistic \[F=\frac{\mathrm{RSS}_H-\mathrm{RSS}}{\mathrm{RSS}}\cdot \frac{n-p}{q}\] is \[F_{q, n-p}\]-distributed. We also have simplifications

\[\mathrm{RSS}_H-\mathrm{RSS}=\|\hat Y-\hat Y_H\|^2=(\hat{\boldsymbol\beta}-\hat{\boldsymbol\beta_H})^\top X^\top X(\hat{\boldsymbol\beta}-\hat{\boldsymbol\beta}_H)=(A\hat{\boldsymbol\beta}-\mathbf{c})^\top(A^\top (X^\top X)^{-1} A)^{-1}(A\hat{\boldsymbol\beta}-\mathbf{c})\]

and

\[\mathrm{RSS}=\|Y-\hat Y\|^2=Y^\top (I-P)Y\]

By canonicalization, for \[\mathbf{c}=\mathbf{0}\] and \[q<p\] for \[A=\left(\begin{matrix}A_1&A_2\end{matrix}\right)\]
where \[A_2\] is \[q\times q\] with rank \[q\], breaking up \[\boldsymbol\beta=\left(\begin{matrix}\boldsymbol\beta_1\\ \boldsymbol\beta_2\end{matrix}\right)\]
\[X=\left(\begin{matrix}X_1&X_2\end{matrix}\right)\] conformably, we can set \[\boldsymbol\beta_2=-A_2^{-1}A_1\boldsymbol\beta_1\].

This lets us define \[P_H\] directly as the RSS of the equivalent model under \[H\], \[Y=X_H\boldsymbol\gamma+\boldsymbol\varepsilon\],
where \[X_H=X_1-X_2A_2^{-1}A_1\]. Compare this to the method of orthogonal projections in Section 3.8.2 and
the Pythagorean law application of equation (3.44). Pretty neat matrix equality.

Examples 4.4-4.6 have some typical derivations of tests, especially for the common marginal hypothesis
case. Here's another.

**** Chapter 4, Part d, Exercise 2

\[n_i\] observations \[Y^{(i)}=W^{(i)}\boldsymbol\gamma^{(i)}+\boldsymbol\varepsilon^{(i)}\] for \[i\in[2]\]
and \[\boldsymbol\varepsilon=\begin{pmatrix}\boldsymbol\varepsilon^{(1)}&\boldsymbol\varepsilon^{(2)}\end{pmatrix}\sim N(0, \sigma^2 I)\]. Let's create a test that \[Y^{(i)}\] came from the same linear model.

\[H:\boldsymbol\gamma^{(1)}=\boldsymbol\gamma^{(2)}\]. Then the full model is
\[Y=\begin{pmatrix}Y^{(1)}&Y^{(2)}\end{pmatrix}=X\boldsymbol\gamma+\boldsymbol\varepsilon\] where \[\boldsymbol\gamma=\begin{pmatrix}\boldsymbol\gamma^{(1)}\\\boldsymbol\gamma^{(2)}\end{pmatrix}\] and \[X=\begin{pmatrix}W^{(1)}&0\\0&W^{(2)}\end{pmatrix}\].

But our constraint for \[H\] can be expressed as \[\begin{pmatrix}I&-I\end{pmatrix}\boldsymbol\gamma=\mathbf{0}\] so by
canonicalization \[X_H=\begin{pmatrix}W^{(1)}\\W^{(2)}\end{pmatrix}\]. Then our statistic based on projection
matrices from design matrices will satisfy the null distribution:

\[\frac{Y^\top(P_X-P_{X_H})Y}{Y^\top(I-P)Y}\cdot\frac{n_1+n_2-2p}{p}\sim F_{p, n_1+n_2-2p}\]


*** 4.6 Goodness-of-fit Test

This chapter presents a neat technique for repeat observations:
use tied normal means we can average across observations.

*** 4.7 F-test and Projection Matrices

Theorem 4.3 generalizes 4.1 to linearly dependent designs and constraints through
projections.

** 5 Confidence Intervals and Regions

**** 5.1 Simultaneous Interval Estimation

We are interested in estimating CIs for linear transformations \[\mathbf{a}_j^\top\boldsymbol\beta\].

We'd like to do this simultaneously for a set of linear transforms \[\mathcal{A}=\{\mathbf{a}_1,\cdots,\mathbf{a}_k\}\] where
the rank of this set is \[d\].

This applies to:
 - inference, when \[\mathbf{a}_j\] are standard basis vectors and \[d=k=p\]
 - mean prediction, when \[\mathbf{a}_j\] are standard basis vectors and \[k\gg d=p\]
 - other specialized hypotheses, which can have different dimensionality.

It's insufficient to simply use the t-intervals from the marginal distribution of \[\hat\beta_j\],
for instance, because the probability of at least one failure of \[(1-\alpha)\]-CI coverage among
\[k\] will look closer to \[k\alpha\] for small \[\alpha,k\].

Since \[\hat{\boldsymbol\beta}\sim N(\beta, \sigma^2(X^\top X)^{-1})\], given that we can estimate
\[\mathbb{E}S^2=\sigma^2\], \[A\hat{\boldsymbol\beta}\] follows a multivariate t-distribution, where
\[A^\top\] consists of column vectors from \[\mathcal{A}\].

Given this setup, we have several solutions for simultaneity:

 - Bonferonni :: Use \[\alpha/k\] CIs, appealing to union bound.
 - Maximum Modulus (MM) :: Useful computation accounting for \[\mathrm{cov}(\hat\beta_0,\hat\beta_1)\] for \[k=2\], based on max-of-t-distributions
 - S-method :: Bound simultaneous CI for \[\mathbf{h}^\top\boldsymbol\beta\] for all \[\mathbf{h}\in \mathcal{C}(A)\].

MM also applies to larger \[k\] for independent \[\mathbf{a}_j^\top\boldsymbol\beta\], but this
requires hypothesis testing on eigenvectors of \[X^\top X\], which is not really typical. One can still
use MM for non-independent t-distributions (the usual case), because the maximum decreases
as correlations increase.

S-method is really neat: it constructs an ellipse around \[\beta\] directly and bounds the
Rayleigh quotient of \[A\boldsymbol\beta\]. There's an important reduction from
\[\mathcal{A}\] to just its basis, as well.

The S-method's generality (full linear space CI coverage) makes it great for prediction settings
where we estimate CIs for \[k\gg p\], but it's not good for inference settings with \[d\approx k\],
where MM performs better (narrower CIs).

That said, it's unclear to me why you wouldn't want a Monte-Carlo like method to account for the
nonzero correlations between our linear transforms. This should in principle allow much narrower
CIs.

***** Monte Carlo Inference

Suppose we have a full rank OLS setup \[Y=X\boldsymbol\beta+\boldsymbol\varepsilon\] with
 \[\boldsymbol\varepsilon\sim N(\mathbf{0},\sigma^2 I)\], known variance for simplicity.

Say we're interested in the simultaneous CIs of \[A\boldsymbol\beta\], where wlog by the S-method
trick \[A\] has full crank.

In the case for known variance, why don't we use \[\hat{\boldsymbol\beta}-\boldsymbol\beta\sim N(0,\sigma^2(X^\top X)^{-1})\] directly?

Let \[D^{1/2}\] be the diagonal matrix containing the inverse standard deviation of each element
of \[A(\hat{\boldsymbol\beta}-\boldsymbol\beta)\].

Then consider \[\mathbf{z}=D^{-1/2}A(\hat{\boldsymbol\beta}-\boldsymbol\beta)\], which then follows
the known distribution \[N(\mathbf{0},D^{-1/2}A^\top (X^\top X)^{-1} AD^{-1/2})\].

We can compute the inverse survival function of \[\max_i\left|z_i\right|\] somewhat easily with monte carlo
by sampling normals and looking at the upper \[\alpha\] quantile; this gives the maximum of normals
correlated exactly how we expect \[A(\hat{\boldsymbol\beta}-\boldsymbol\beta)\] to be.

This yields a max-of-z-scores \[m_\alpha\]; then our CIs for \[\mathbf{a}_j^\top\boldsymbol\beta\]
are given by \[\mathbf{a}_j^\top\hat{\boldsymbol\beta}\pm D_{jj}^{1/2}m_\alpha\]

For unknown variance, I'm sure the above can be studentized in some way,
and it seems extensions should be possible to heterskedastic settings.

Perhaps this is very expensive, so it was not suggested in the book (which was, after all, published in 1977).

This would be essentially an extension of [[https://www.jstor.org/stable/1266931?seq=1][Hahn 1972]].


**** 5.2 Confidence Bands for the Regression Surface
**** 5.3 Prediction Intervals and Bands for the Response

For CIs for the response, rather than the mean of the response, we simply observe
that \[Y(\mathbf{x}_0)=\mathbf{x}_0^\top\boldsymbol\beta + \varepsilon\], so we need to increase
variance accordingly.

I wonder what we give up by using this fixed-X model for prediction: in the anti-causal setting
\[Y\rightarrow X\] the marginal distribution of \[X\] contains information about \[Y|X\] which
we're explicitly ignoring by using a fixed-X setup. This is, in principle, something ML methods
can take advantage of.

**** 5.4 Enlarging the Regression Matrix

*** 6 Straight-line Regression

** 6 Straight-line Regression

*** 6.1 The Straight Line

We have the two-variable model \[Y=\beta_0+\beta_1 x + \varepsilon\], 
which allows us to compute exact simultaneous CIs using correlated
t-distributions directly.

The x-intercept can be analyzed as the ratio of two correlated normal variables,
there exist both convenient or exact forms of CIs.

The Working-Hotelling confidence band can be applied to the entire line
to provide simultaneous CIs across the full range using the Scheffe's method.
This won't give straight lines, but is more narrow in applicable ranges (and
is narrowest at the point \[\overline{x},\overline{Y}\]).

An approach by [[https://www.jstor.org/stable/1269524?seq=1][Wynn and Bloomfield]] allows one to narrow the bands further for specific
ranges; this is a neat extension of the Scheffe method, and doesn't require
complex re-derivation for analyzing interval CIs like other mentioned approaches,
and it tends to be narrower anyway.

*** 6.2 Straight Line through the Origin

Of course, this simplifies the CI for the slope to a simple t-interval, but
the entire band across the full x range is now also defined by the same
t-interval by homogeneity.

*** 6.3 Weighted Least Squares for the Straight Line

With known weights, this reduces to the original model per the usual transform.

With unknown weights, one can use the MLE and rely on asymptotics or
solve with least squares and get more conservative intervals.

It seems strange to me to use unknown, mean-dependent weights. They don't talk
much about it or why one would really want this or be in this setting, but
[[https://www.jstor.org/stable/2983809?seq=1][Williams 1959]] is the reference.

From Exercises 1, Part b here, we get a setting for known weights:
gamma-distributed or poisson-distributed positive-only regression
(if we're regressing sums or counts, or instance, then it might make sense
 to scale variance by mean squared or mean, respectively).

*** 6.4 Comparing Straight Lines
Use a higher-dimensional normal means model to simplify a comparison
across multiple lines into the form of a linear test.

*** 6.5 Two-Phase Linear Regression

If a one-covariate linear regression undergoes a change of phase,
then this can be modelled directly by appropriately encoding the change of
phase directly into the model.

I.e., instead of 

\[Y=\begin{cases}\alpha_0+\alpha_1x+\varepsilon & x<\gamma\\\beta_0+\beta_1x+\varepsilon&x\ge \gamma\end{cases}\]

with a continuity condition that \[\alpha_0+\alpha_1\gamma=\beta_0+\beta_1\gamma\], one could instead
fit

\[Y=\varepsilon+\theta+\begin{cases}\beta_1 (x-\gamma) & x<\gamma\\\beta_2(x-\gamma) & x\ge \gamma\end{cases}\]

so that we don't have redundant parameters.

For a complete answer to the problem see [[https://onlinelibrary.wiley.com/doi/10.1002/0471725315.ch9][Seber and Wild 1989]], Chapter 9.3.

*** 6.6 Local Linear Regression 

** 7 Polynomial Regression
*** 7.1 Polynomials in One Variable

Naively applying least squares to the model \[y\sim \boldsymbol\beta^\top\mathrm{poly}(x)\]
where \[\mathrm{poly}(x)_i=x^i\] for single-variable polynomial regression
may work mathematically to reuse the same machinery but results in a Vandermonde
design which in turn has an ill-conditioned Hilbert \[X^\top X\] Gram matrix.

Since LS is not practically feasible, stable solutions are needed.
Indeed, the desired approach is to construct an orthogonal basis dynamically,
with \[\phi(x)\] replacing \[\mathrm{poly}\] above such that \[\phi(x)_i=\phi_i(x)\]
with \[\{\phi_i\}_i\] an orthogonal basis with respect to the norm defined by 
the covariates themselves. For our simple two-variable dataset \[\{x_i,y_i\}\],
define the product \[\langle f, g\rangle=\sum_if(x_i)g(x_i)\]. This can be dynamically constructed using
the recurrence

\[\phi_0=1\], \[\phi_1(x)=2(x-a_1)\], \[\phi_{r+1}(x)=2(x-a_{r+1})\phi_r(x)-b_r\phi_{r-1}(x)\]

Such a basis takes \[O(nk)\] time to construct for degree \[k\], since the polynomials can be defined
through their values at the points or coefficients. Moreover, the Gram matrix
\[\phi(\mathbf{x})^\top\phi(\mathbf{x})\] is diagonal, so the system can be efficiently
solved.

The Chebyshev basis can be used to represent \[\phi_k\], which comes with its own
set of recurrence relations in terms of Chebyshev coefficients in the text.
The primary win here is that you can perform inference outside of the training
set efficiently, too. See [[https://en.wikipedia.org/wiki/Clenshaw%E2%80%93Curtis_quadrature][Clenshaw 1960]].

To add constraints to the construction, see, e.g., [[https://ieeexplore.ieee.org/document/1099532][Payne 1970]], [[https://academic.oup.com/imamat/article-abstract/1/2/164/656295][Clenshaw and Hayes 1965]]

*** 7.2 Piecewise Polynomial Fitting

Regular polynomials may have very slowly decreasing RSS as degree increases
or systemic residuals.

This is especially the case when dealing with functions that are piecewise
varying across stages.

Splines of order \[M\] with \[K\] knots \[\boldsymbol\xi_k\] for \[k\in[K]\] are:

 - order \[M-1\] polynomials on each piece \[\xi_k, \xi_{k+1}\]
 - globally \[\mathcal{C}^{M-2}\]

I.e., continuity up to the last order at knots. Repeat knots lower breakpoint
continuity constraints.

For visual purposes, [[https://web.stanford.edu/~hastie/ElemStatLearn/][ESL - Splines]], \[M=4\] or cubic splines typically suffice.

While splines admit a parsimonious \[K+M\] parameter representation, it's
unstable to compute. A redundant \[K+2M\] representation allows for a stable,
recursive formula.

Various forms of regularization are possible, but we're essentially
entering ML territory, with methods becoming heuristic and reliance on CV
for seleciton.

*** 7.3 Polynomial Regression in Several Variables

This section considers extensions to both splines and polynomials
to higher dimensions, but the curse of dimensionality is not considered.

Approaches mentioned here are either tensor decomposition or exponential-in-dim
methods that naively extend lower-dimensional approaches.

** 8 Analysis of Variance

*** 8.1 Introduction

*** 8.2 One-way Classification
    
The main reason ANOVA works (more efficient test at whether all means for
separate populations are equal the full pairwise implication), i.e.,
can efficiently reject \[H_0:\mu_1=\mu_2=\cdots=\mu_n\] is framing in terms
of contrasts, or linear transformations of the means. Transforming the
problem into \[H_0:\mu_1-\mu_n=\mu_2-\mu_n=\cdots=0\], we can equivalently
ask if the vector of the above mean differences is uniformly 0.

This holds under all linear transformations being, so Scheffé's method is
appropriate.

Specializations for particular contrasts rather than all are available too.

Balanced designs help simplify the intervals, but don't afford significant
efficiency gains because of it (balanced means every group has same number
of participants).

However, per Miscellaneous Exercises 7, balanced designs DO get rid of
interaction effects, which is helpful.

The F-test assumptions are the same as OLS.

Balanced tests absolve (partly) normality and homogeneity requirements for
the F-test. That said [[https://www.jstor.org/stable/2332579][Welch's test]] directly allows heteroscedasticity,
which seems to be a set of approximations given the stochastically estimated
noise [[https://math.stackexchange.com/questions/1746329/proof-and-precise-formulation-of-welch-satterthwaite-equation][for sample variances]].

Heteroscedasticity checks (though they shouldn't be run to decide on
whether to use Welch's) can be done by looking at Levene's test, which
has a robust version based on median and LAD.

*** 8.3 Two-Way Classification (Unbalanced)

Two-way ANOVA can be equivalently modeled by an OLS setting
with indicators for two possibly multi-level factors (which can
all be dummy coded). The model includes interaction terms.

I.e., for \[I\] levels on the first and \[J\] levels on the second,
the design matrix here has \[1 + I + J + IJ\] terms, which is of course
overparameterized (there are \[IJ\] mean parameters and a single variance
parameter). The overparameterization can be dealt with with many different
ways, either by coding contrasts or using constraints. The constraint
is that all population means are the same across the same main effect.

The presence of interaction terms signifies that there is a "difference
in differences": there is no interaction (across any of the levels) when the
term \[(\mu_{i_1j_1}-\mu_{i_1j_2})-(\mu_{i_1j_2}-\mu_{i_2j_2})\] is uniformly
\[0\] across any two levels \[i_1,i_2\] of \[I\] and \[j_1,j_2\] of \[J\]. This set of constraints can, wlog,
be taken "relative to" base values \[i_1,j_1=1\] so it's actually just
\[(I-1)(J-1)\] independent constraints total.

This "difference in differences" is what gets tested for interaction coefficients.

Then there are several procedures when trying to fit a model in this setting:
first the grand mean (intercept) is tested against zero. Assuming this
succeeds, then there are several approaches.

Type I ANOVA. Test for significance of including the first factor in the model,
next the other factor, and finally interactions. This test has the advantage
of creating independent RSS tests (which we can see by the usual \[\chi^2\]
analysis of the residual OLS fit operators that are idempotent matrices \[(I-P)\]
for each fit.

Type II ANOVA. Two tests for main effects given other main effect and interactions.
Not independent, but symmetric. Final Type I ANOVA model depends on ordering.
Dubious, since the models compared here are not nested, but in principle
this is possible, if a factor is "activated" by another.

Type III ANOVA. Tests for significance of each of the two main effects and
the single interaction effect give the rest (the other main and the interaction, or
the two mains). Also not independent, but nested.

To be frank, using the above methods blindly seems cargo-culty. If you're coming
in with a hypothesis, then just test that hypothesis.

*** 8.4 Two-Way Classification (Balanced)

Math simplifies

*** 8.5 Two-Way Classification (One Observation per Mean)

Under-defined, b/c you have \[IJ\] observations but \[IJ + 1\] unknowns
(means for every cell in the 2-way table and the overall variance).

Usually, only an additive model is assumed here, since this setting
mostly appears for RCTs. Other richer interactions can be modelled with
lower-rank representations of the interaction matrix in multilevel settings.

*** 8.6 Higher-Way Classifications with Equal Numbers per Mean

Higher-order ANOVAs generalize the notion of differences in differences.

Below are more my notes for this topic, I think the recursive case is simpler
than going through 3-way and 4-way manually like this book does.

For a factor \[A\], and some response \[Y\], define the delta operator
\[\Delta_AY=\mathbb{E}[Y|A]-\mathbb{E}[Y|\lnot A]\]. In other words, the lift of \[A\]. This gives the coefficient
for \[A\] in a binary factor and is the effect tested for in one-way ANOVA.
(for multilevel, it's all relative to some base null factor value).

The delta \[\Delta_AY\] is itself a random variable, so we can ask
\[\Delta_B\Delta_AY=\mathbb{E}[\mathbb{E}[Y|A]-\mathbb{E}[Y|\lnot A]|B]-\mathbb{E}[\mathbb{E}[Y|A]-\mathbb{E}[Y|\lnot A]|\lnot B]\]
which boils down to \[(\mathbb{E}[\mathbb{E}[Y|A, B]-\mathbb{E}[\mathbb{E}[Y|A, \lnot B])-(\mathbb{E}[\mathbb{E}[Y|\lnot A, B]-\mathbb{E}[\mathbb{E}[Y|\lnot A, \lnot B])\], the
difference in differences. This is equal to \[\Delta_A\Delta_B Y\] (the
delta operator is commutative).

This, for OLS, also corresponds to the appropriate corresponding interaction
coefficient for \[A,B\].

Higher-order ANOVAs, over \[k\] factors \[F_1,\cdots,F_k\] are correspondingly linear models, which have
means of the recursive difference-in-differences \[\Delta_{F_1}\cdots\Delta_{F_k}Y\] as OLS parameters.

*** 8.8 Analysis of Covariance

Again, just reduce to the OLS model to come up with a hypothesis.
ANCOVA is when you do ANOVA but also throw continuous covariates in there
in addition to the factors.

Chapter 8 Part e Excercise 1 is interesting, because it can use the Theorem 3.6
partial fit result to perform regular ANOVA followed by OLS residual fitting
of the covariates.

** 9 Departures from Underlying Assumptions
*** 9.1 Introduction
*** 9.2 Bias

Underfitting occurs when you're missing covariates relative to the new model.

I.e., fitting \[Y\sim X\beta+\varepsilon\] but the true model is \[Y\sim X\beta + Z\gamma+\varepsilon\]

 - Fitted model becomes what you'd expect if you regress the missing covariates on the present ones
 - Bias term for the OLS predictor \[\hat\beta\] is \[(X^\top X)^{-1}X^\top Z\gamma\]
 - The variance estimator \[S^2\] becomes an overestimate
 - \[\mathrm{var}\hat\beta\] remains correct.

Overfitting occurs when additional regressors are added. This inflates the \[\mathrm{var}\hat\beta\],
but variance of noise and the estimate are unbiased.

The fact that during underfitting variance stays the same allows for creating a test for underfit,
see [[https://www.jstor.org/stable/2984219?seq=1][Ramsey 1989]].

*** 9.3 Incorrect Variance Matrix

In this scenario, homoskedasticity is violated but WLS is not used instead of OLS.

The coefficients remain unbiased but variance for both the coefficients and the noise
(and thus the fit) can become biased in either direction.

*** 9.4 Effect of Outliers

The previous section [[3.13 Robust Regression]] covered OLS alternatives
and evaluation measures. This section looks precisely on a single point's
effect on the OLS fit.

For a projection matrix \[P\], let the projected fit \[\hat Y = PY\] so that
\[\hat y_i=p_{ii}y_i+\sum_{j\neq i}p_{ij} y_j\]. Assuming centered covariates, per (3.53),
\[p_{ii}=n^{-1}+(n-1)^{-1}\|\mathbf{x}_i-\overline{\mathbf{x}}\|_{S_{xx}}^2\|\ge n^{-1}\], where \[S_{xx}\] is the sample
variance-covariance of the covariates and the corresponding norm
\[\|\mathbf{x}\|_A^2=\mathbf{x}^\top A^{-1}\mathbf{x}\].

Further, since \[P^2=P\] it's clear \[p_{ii}\le 1\] as \[\sum_{j}p_{ij}^2\ge p_ii^2\ge p_i\].

This gives \[p_{ii}\] in the range \[[n^{-1}, 1]\], where the Mahalanobis distance
defined by the gaussian \[N(\overline{\mathbf{x}}, S_{xx})\] determines
high-leverage points. Points low in the Gaussian density defined above
are high-leverage, having \[p_{ii}\approx 1\], and thus \[\partial_{y_i}\hat y_i\approx 1\]
so that high-leverage points have their fit more or less directly influenced
by the y value at that point.

*** 9.5 Robustness of the F-Test to Non-normality

The F-test can be used for both equality of variance and equality of means.

For variances, the test is very sensitive to non-normality, but not so for means.

How can this be the case for the same distribution?

This boils down to being really careful about what we mean about sensitivity.
It's not directly stated in the chapter, but the robustness referred to here is
really robustness not under the alternative, but under some "pretend null"
where, e.g., the variance is equal between two samples, but the error distribution
is some mean-0 non-normal distribution. Similarly, for the mean equality, the "pretend null"
is that the means are equal but the noise is some mean-0 non-normal distribution.

This chapter does not analyze some alternative hypothesis for these two cases; that's to
ill-defined.

However, in the above sense (that the test still works, namely has a reasonable Type-I rate,
under a pretend null that's not exactly normal but otherwise correct), the robustness
properties hold.

The reason boils down to the different projection matrices.

For equality of variance, our setup is that we have a sample of iid \[X_i\] of size \[f_1\]
and similarly for \[Y_i\] of size \[f_2\]. The \[F=S_X^2/S_Y^2\] is the ratio of two
\[\chi^2_{f_i}\]-distributed population variance estimators. Expressed under the standard OLS model,
this corresponds to a single global \[Y=\begin{pmatrix}X_1\\X_2\end{pmatrix}\beta+\varepsilon\],
where \[X_1=\begin{pmatrix}1_{f_1}&0\end{pmatrix}\] is a \[f_1\times 2\] matrix corresponding to the
\[X_i\] observations and similarly for \[X_2=\begin{pmatrix}0&1_{f_2}\end{pmatrix}\] of the \[Y_i\].
Then the corresponding projection matrices \[P_1,P_2\] give rise to the aforementioned F-statistic.
Interestingly, here we have \[P_1P_2=0\].

Then for means, our setup is similar, but instead the design matrix compares splitting means
to having the same one, so it is a \[f_1+f_2\times 3\] matrix of the form 
\[X_1=\begin{pmatrix}1_{f_1}&0\\ 0&1_{f_2}\end{pmatrix}\] under the alternative and
\[X_2=\begin{pmatrix}1_{f_1+f_2}&0\end{pmatrix}\] under the null. Note that now, each model is considered separately, whereas above
we relied on the same OLS model. Here, we have another \[F=\frac{Y^\top (P_1 - P_2) Y}{Y^\top P_1 Y}\] following
an \[F\] distribution, but here of degrees of freedom \[1,f_1+f_2 - 2\] (this test, unlike equality of
variances, can be naturally extended to having multiple groups of comparison with equal means).
Note that here \[(P_1-P_2)P_1\neq 0\].

The different projection matrix relationships give rise to different robustness properties.

For variance, [[https://www.jstor.org/stable/2983753?seq=1][Atiqullah 1962]] provides an analysis for F-statistics where projection matrices
satisfy \[P_1P_2=0\] and under the null variances are equal. Then a condition called quadratic
balance, met by balanced (and randomized, for other reasons) designs, yields robustness to
non-normality, as measured by excess kurtosis \[\gamma_2=\mathbb{E}\left[\left(\frac{Y-\mathbb{E} Y}{\sigma}\right)^4\right]-3\],
which is the fourth centered moment less the fourth centered moment of the Gaussian. In particular, we have
the following theorem (9.2): for symmetric, idempotent projection matrices \[P_i\] with vanishing product,
assume \[\mathbb{E}[Y^\top P_i Y]=\sigma^2 f_i\] and let the diagonal of \[P_i\] be \[\mathbf{p}_i\].
If the shared excess kurtosis of each variate is \[\gamma_2\], and each \[Y_i\] further has shared
variance, third, and fourth moments (but means can differ), then \[Z=\frac{1}{2}\log F\] is approximately normal,
corresponding to \[F=\frac{Y^\top P_1 Y / f_1}{Y^\top P_2 Y / f_2}\] being approximately F-distributed
with \[Z\] being independent of \[\gamma_2\] if \[f_1\mathbf{p}_2=f_2\mathbf{p}_1\] (quadratic balance).

For means, [[http://www.biostat.jhsph.edu/~iruczins/teaching/140.752/read/papers/box.1962.pdf][Box and Watson 1962]] provide an analysis based on the 4th cumulant bound where
the resulting F-statistic for the equality-of-means test above (but generalized to \[p\] groups,
checking equality of all their individual means under the null, behaves like an F-distributed
random variable, but with \[\delta (p - 1),\delta (f_1+f_2-1)\]. The shrink factor for degrees of freedom \[\delta\]
is defined by \[\delta^{-1}=1+C_x\Gamma_y/n\], where \[C_x,\Gamma_y\] are 4th-cumulant based bounds
that are each approximately 0 the closer the (multivariate) distribution of the covariates or
noise is to the normal.

*** 9.6 Effect of Random Explanatory Variables

**** Structural Law

The assumed OLS Model describes a stochastic (almost sure) structural law of the form
\[\mathbb{E}[Y|X]=\beta^\top X +\varepsilon\] where the noise is exogenous, i.e., \[\mathbb{E}[\varepsilon|X]=0\].

This corresponds to random covariates observed without error.

The book refers to a special case that's way too narrow with \[\varepsilon=0\] exactly. But the book does
describe the fact that if extra unrelated covariates are observed then we're not actually in trouble,
as OLS will naturally feature select. The "feature selection" is not like lasso here but rather due to
the CIs including 0. This seems like a silly way of dressing up the overfit discussion earlier; note that
just like in the fixed effects case the variance will increase if extra variables are included.

**** Functional Law

Here, the assumed OLS model describes a fixed equality derived from a law \[\mathbb{E}[y]=\beta^\top \mathbb{E}[\mathbf{x}]\]
(note no noise). Such relationships arise from random variables representing unbiased measurements of an underlying
physical phenomenon. This is a fixed effects model with \[y=\beta^\top \mathbb{E}[\mathbf{x}]+\varepsilon\].

Let \[\mathbf{u}=\mathbb{E}[\mathbf{x}]\] be the underlying covariates and write \[\mathbf{x}=\mathbf{u}+\Delta\] where by construction
\[\mathbb{E}[\Delta]=0\]. Let \[\mathbf{var} \Delta= D\], say. Suppose we make several observations and stack the design 
and outcomes as \[X,Y\] and proceed with naive OLS. Let \[U=\mathbb{E}[X]\] be the expected design.

Naive OLS yields \[\hat\beta_\Delta = (X^\top X)^{-1} X^\top Y\]. By Equation (9.30),
the bias \[\mathbf{b}\] is given by \[\mathbb{E}[\hat\beta_\Delta]=\beta-\mathbf{b}\] with \[mathbf{b}\approx \left(\frac{U^\top U}{n}+D\right)^{-1}D\beta\],
which is of course unhelpful since it requires oracle knowledge.

Interestingly, though, if we have an unbiased estimate \[\hat D\] of \[D\] then the book goes that
we can get an unbiased estimate \[\hat{\mathbf{b}}=n(X^\top X)^{-1}\hat D \beta\].

Going off-script a little (above is based on [[https://www.jstor.org/stable/2335377?seq=1][Davies and Hutton 1975]]), we note first that the bias estimator
above is useless as it still depends on \[\beta\] which is what we're estimating. But since everything is linear
and unbiased, note that \[\mathbb{E}\hat{\mathbf{b}}=\mathbf{b}\] so then \[\mathbb{E}[\hat\beta_\Delta]=\mathbb{E}[\beta+\hat{\mathbf{b}}]\]. Expanding our estimator, we have

\[\mathbb{E}[\hat\beta_\Delta]=\mathbb{E}[I\beta + n(X^\top X)^{-1}\hat D \beta]=\mathbb{E}[I + n(X^\top X)^{-1}\hat D] \beta\] by linearity.

Since \[\hat D, (X^\top X)^{-1}\] are both symmetric and PSD, so their product
 (though possibly not symmetric) has [[https://math.stackexchange.com/a/113859/38471][positive eigenvalues]],
and thus the matrix in the expectation on the RHS above is invertible.

Then moving the matrix to the other side, we have

\[\mathbb{E}[I + n(X^\top X)^{-1}\hat D]^{-1}\mathbb{E}[\hat\beta_\Delta]= \beta\]

which we unfortunately can't just push into one expectation due to nonlinearity.
See [[https://mathoverflow.net/a/307168][this MO answer]] for details. That said, it can be directly seen from the continuity of the matrix
inverse and continuous mapping theorem that the psuedo-debiased (it's not actually unbiased)
estimator \[\left(I + n(X^\top X)^{-1}\hat D\right)^{-1}\hat\beta_\Delta\] is consistent as an estimator of \[\beta\].

In fact, it's basically an order-one method of moments estimator. So I bet that analysis applies here. The interesting
part here is that matrix inverse is a known operation and we might be able to do better in estimating it than a generic
nonlinear equation like method of moments usually does. You still end up having to know \[\hat D\], but in principle this
is something you can estimate by repeatedly sampling the same "setting" \[U\]. I.e., if our experimental setup
lets us sample \[U\] repeatedly, then we'd get multiple \[X\] (after which the regression could in principle even
be carried out on the average \[X\] and the average \[Y\]).

**** Other Cases

The other cases don't seem too interesting. They include fixed, non-random errors (such as round-off) which
can be analyzed as noiseless versions of the [[Functional Law]] description and random covariates measured with error
(which is undetermined, as there are now two noises, one from the stochasticity of covariates and one from
their error, but only one set of residuals to estimate average error). The last case can be resolved
with assumptions about the relative sizes of each error source.

One approach that wasn't mentioned but would be interesting to look into would be [[https://en.wikipedia.org/wiki/Total_least_squares][Total least squares]].

*** 9.7 Collinearity

Since \[\mathrm{var}\hat\beta=\sigma^2(X^\top X)^{-1}\], it's no surprise that rank deficiency
results in uncertainty.

It also affects robustness. This chapter looks at both.

**** Variance due to Covariate Collinearity

Suppose we have centered, scaled covariates, so that
\[X^\top X = \begin{pmatrix}n &0\\0&R_{xx}\end{pmatrix} = \begin{pmatrix}n &0 &0\\0 & 1 &\mathrm{r}^\top \\ 0 & \mathrm{r} & R_{22}\end{pmatrix}\]
where \[R_{xx}\] is the matrix of covariate correlations and the right hand
side is its block decomposition,such that \[r_i=\langle X_1, X_i\rangle \] for the vector \[\mathrm{r}\]
Then by linear algebra \[\mathrm{var}\hat\beta_1=\sigma^2(1-\mathrm{r}^\top R_{22}^{-1}\mathrm{r})^{-1}\].

This of course applies wlog to other covariates than the first.

We can also see that \[R_1^2=\mathrm{r}^\top R_{22}^{-1}\mathrm{r})^{-1}\] is itself the coefficient
of determination of regressing \[X_1\] on the rest of the columns of \[X\] by looking at the
RSS of that virtual regression.

More generically, we can define the variance inflation factor, for nonscaled
coefficients, which is \[\mathrm{VIF}_j=\frac{\mathrm{var}\hat\beta_j}{\sigma^2}=s_j^2(1-R_j^2)^{-1}=s_j^2(X^\top X)^{-1}_{(j+1)(j+1)}\]
where \[s_j^2\] is the (now non-unit) variance of the \[j\]-th coefficient, and we can see
Cramer's rule at play!

The book provides other approaches to variance analysis due to collinearity but
I did not find thme that illuminating.

**** Lower Robustness due to Covariate Collinearity

Suppose we have an OLS setup \[\mathbf{y}=X\beta+\varepsilon\] but we
replace \[X\] with \[X+\delta X\] and \[\mathbf{y}\] with \[\mathbf{y}+\delta \mathbf{y}\]
(where \[\delta X, \delta \mathbf{y}\] should be though of as single variables,
not products of variables. Further, these can be arbitrary perturbations,
so it's *not* necessarily the case that \[\delta \mathbf{y} = (\delta X) \beta\].

Per [[https://books.google.com/books/about/Accuracy_and_Stability_of_Numerical_Algo.html?id=7J52J4GrsJkC&source=kp_book_description][Higham 1996]] (page 392), if \[\|\delta X\|_2\le \epsilon \|X\|_2\] (by spectral norm)
\[\|\delta \mathbf{y}\|\le\epsilon\|\mathbf{y}\|\], and \[\kappa \epsilon <1\], where \[\kappa=\frac{\sigma_{\max}(X)}{\sigma_{\min}(X)}=\sqrt{\frac{\lambda_{\max}(X^\top X)}{\lambda_{\min}(X^\top X)}}\] is
the condition number of the design, then the resulting OLS applied to
the perturbed inputs gives an estimator \[\hat{\beta}_\epsilon\] whose relative error is bounded by

\[\frac{\|\hat\beta-\hat{\beta}_\epsilon\|}{\|\hat\beta\|}\le\frac{\kappa\epsilon}{1-\kappa\epsilon}\left(2+(1+\kappa)\frac{\|X\hat{\beta}_\epsilon-\mathbf{y}\|}{\|X\|_2\|\hat\beta\|}\right)\]
