* Linear Regression Analysis

** 3 Linear Regression: Estimation and Distribution Theory

*** 3.6

Orthogonal designs minimize the variance of your final coefficients (regardless of error distribution, only assuming spherical errors).

See Chapter 3, Part e, Exercise 3. Follows by Cramer's Rule.

Use orthogonal designs to find optimal experiments. See Chapter 3, Part e, Exercise 3.

*** 3.7

Augmenting linear regression models with additional covariates can be done through orthogonalization.

Basically, if you have \[Y\sim X\beta+\varepsilon\] with the OLS estimate,
let 
\[P=(X^\top X)^{-1}X^\top\] be the projection matrix and \[R=I-P\] be the
residual operator.

Then to fit the new model \[Y\sim X\beta+Z\gamma+\varepsilon\], you project out \[X\] from
both the signal and the new covariates:

\[\hat\gamma = MZ^\top RY=(ZR^\top RZ)^{-1}(RZ)^\top RY\]

where \[M=Z^\top RZ\] and we rely on symmetry and idempotence of \[R\]. Then the
OLS fit of \[\gamma\] in the augmented model, \[\hat\gamma\] is the OLS fit
of the projected out setting of residuals \[RY\] on covariates \[RZ\].

Then the adjusted \[\hat\beta\] in the augmented model excludes the variance
explained by \[\hat\gamma\], i.e., we use the OLS solution \[\hat\beta=P(Y-Z\hat\gamma)\],
where without the \[Z\hat\gamma\] term we have the plain old OLS for the smaller
model \[Y\sim X\beta+\varepsilon\]


*** 3.8 Estimation with Linear Restrictions

Given a least squares problem where we also know linear constraints that the output must satisfy, i.e., \[A\beta=c\],
then we can adjust the least squares solution using Lagrange multipliers to get

\[\hat\beta'=\hat\beta+(X^\top X)^{-1}A^\top(A(X^\top X)^{-1}A^\top)^{-1}(c-A\hat\beta)\]

where \[\hat\beta\] is the OLS estimate. An example use here is if you measure the angles of a triangle with the same
measuring tool.


*** 3.9 Design Matrix of Less than Full Rank

For less-than-full-rank designs \[X\], \[\beta\] is not identifiable (since distinct values can give rise to the same observations).

Restricting to linear subspaces by adding constraints on \[\beta\] can help, as can dropping columns.

With the rank less than full rank, we can still estimate linear some functions of \[\beta\] with linear
estimators. In particular, linear reductions
\[\mathbf{a}^\top \boldsymbol\beta\] are estimable iff \[\mathbf{a}\in\mathcal{C}(X^\top)\], since any linear estimator is of the form \[\mathbf{b}^\top Y\]
which is \[\mathbf{b}^\top X \boldsymbol\beta\] in expectation.


Interestingly, Question 4 of Exercises 3i then implies that 
\[\mathbf{a}\in\mathcal{C}(X^\top)\] iff \[\mathbf{a}\] is a unit eigenvector
of \[X^\top X (X^\top X)^{-}\] where \[A^{-}\] is any generalized matrix inverse.

*** 3.10 Generalized Least Squares

For a least-squares model defined by \[\mathbb{E}[Y]= X\boldsymbol\beta\] and \[\mathrm{var}(Y)= V\],
by applying the transformation \[V^{-1/2}\], which exists assuming
positive definiteness (else, after rotation, one of the \[Y\] values
is a constant), we recover a transformed OLS model.

The transformed OLS model has all our original BLUE (best linear unbiased estimator)
guarantees. The least squares estimate for the non-homoskedastic case
can be derived from this, it's just

\[\boldsymbol\beta^*=(X^\top V^{-1} X)^{-1}X^\top V^{-1} Y\]

*** 3.11 Centering and Scaling the Explanatory Variables

Centering and scaling explanatory variables doesn't affect the model RSS or any identifiability
properties, because the relationship with \[X\] and \[\hat\beta\] is one-to-one.

But centered variables (except the intercept) do result in \[\hat\beta_0=\overline{Y}\].

Similarly, centered and scaled variables \[X^*\] (i.e., each column has unit norm and the average of
its values should be 0, starting from the design \[X\]) have a simplified projection matrix form,
where the original model \[Y= X\boldsymbol\beta+\boldsymbol\varepsilon= X^*\boldsymbol\gamma+\boldsymbol\varepsilon\]
where \[\hat{\boldsymbol\gamma}=R^{-1}X^{*\top}Y\], where \[R\] is the correlation matrix of \[X\].


*** 3.12 Bayesian Estimation

Frequentist confidence intervals (based on Student's t) can be derived for an OLS model by using a uniform
(improper) prior for \[\boldsymbol\beta,\log\sigma\] and computing the marginal posterior over \[\boldsymbol\beta\]
by marginalizing \[\sigma\]. This is an interesting coincidence and a neat alternative interpretation.

A more informed prior is specified using a conjugate prior for computational ease,
starting with a Normal distribution and ending with one in the posterior (when conditioned on the scale)
for \[\boldsymbol\beta\] and keeping   \[\sigma^2\] inverse Gamma.

When you marginalize the scale, you recover a Student's t distribution over coefficients.
 

*** 3.13 Robust Regression

Different sensitivity measures help us evaluate properties of regression algorithms.
We'll use the fixed effects model where \[\mathbb{E}[Y]=X\boldsymbol\beta\] but will allow
varying kinds of errors and even interventions on some of the explanatory variables.

All methods will apply to residuals \[e_i(\boldsymbol\beta)=\mathbf{x}^\top\boldsymbol\beta-y_i\],
and may introduce and optimize over auxiliary nuisance parameters. First we review different sensitivity
measures.

 - response breakdown point :: the proportion of the response data that must be modified (by any amount) to result in arbitrary change in the learned coefficients.
 - explanatory breakdown point :: as above, but proportion of explanatory rows
 - influence curves :: consider the convex interpolation between a single pathological point and the empirical distribution, as the weight of the pathological point tends to zero what is its influence on the learned coefficients?

A complimentary metric to the above robustness measures is *asymptotic relative efficiency*,
under the OLS model with normal errors.

The below is a summary table of discussed estimators in the chapter.



| estimator         | IC                    | heavy tails | BP  |         ARE |
|-------------------+-----------------------+-------------+-----+-------------|
| OLS               | unbounded in both x,y | no          | 1/n |           1 |
| LAD               | bdd in y not x        | yes         | 1/n |         43% |
| Huber             | bdd in y not x        | yes         | 1/n | ~1 if tuned |
| LMS               | bdd                   | yes         | 1/2 |          0% |
| LTS               | bdd                   | yes         | 1/2 |          8% |
| S-estimator       | probably not in x?    | probably    | 1/2 |        ~29% |
| R-estimator       | bdd                   | yes         | 1/2 |          7% |
| Mallows, LQD, LTD | not bdd               | probably    | 1/2 |        >50% |


 - IC :: influence curve
 - BP :: break point
 - ARE :: asymptotic relative efficiency
 - OLS :: ordinary least squares \[\sum_ie_i^2\]
 - LAD :: least absolute deviation \[\sum_i\left|e_i\right|\]
 - Huber :: a specific M-estimate of \[\sum_i\rho(e_i/s)\] with \[\rho\] interpolating between squared and absolute error in and outside the unit ball.
 - LMS :: least median square \[\mathrm{median}_ie_i\]
 - LTS :: least trimmed squares \[\sum_{i=1}^h e_{(i)}\] where \[e_{(i)}\] are ordered (ascending) residuals, typically only lowest, \[n/2\] terms
 - GM :: generalized M-estimators and one-step GM estimators merged with TMS/ LTS give best of both worlds under covariate distribution assumptions
 - S-estimators :: (biweight function in particular) solve obscure Huber-like polynomial
 - R-estimators :: only use order information, and need to use absolute residuals for guarnatees
 - Mallows, LQD, LTD :: various combinations of differences of residuals, tuned to not be affected by outliers

Moment equations \[\sum_i\psi(e_i/s)\mathbf{x}_i=0\] where \[\psi=\rho'\] and \[\sum_i\chi(e_i/s)=0\] for some scale function \[\chi\]
are generalizations of M-estimates that don't require a well-defined density to be defined.

Not discussed above is some pathological "instability" criterion (Fig. 3.2) that seems to only be
a worst-case scenario for LMS, LTS that isn't terribly realistic.

Overall, pairwise residual approaches allow optimizing high BP while maintaining ARE, but don't do
well from an IC perspective. This is consistent because IC is a "high-fidelity" measure of the impact
of a small deviation, but BP allow arbitrary differences and expect arbitrary changes, so are a crude
set of requirements.

It would be interesting to combine moderate trimming LTS with Huber to get heavy tail resistance
and bounded influence for x and y while maintaining ARE.

** 4 Hypothesis Testing

*** 4.1 Introduction

*** 4.2 Likelihood Ratio Test

Frame a hypothesis test comparing two nested models as assessing a wider linear regression and
its residual sum of squares, and then a constrained model that meets a linearity condition
\[A\boldsymbol\beta=\mathbf{c}\] (usually, \[A\] is partly an identity matrix and partly zero,
and \[\mathbf{c}\] is zero).

This can be fit with incremental OLS approaches (for the identity case) or Lagrange multipliers
from the last chapter.

*** 4.3 F-test

The test statistic of a likelihood ratio (the ratio of the fitted model's normal distributions
on the observed values) is not convenient, but equivalent to an F-statistic, which is
just a ratio of \[\chi^2\] variables via a transformation.

This can be expressed conveniently as a ratio of the residual sum of squares (RSS).
See Theorem 4.1 for an exposition under general linear constraints from [[4.2 Likelihood Ratio Test]].

From the exercises, the only neat computational observation is that if the saturated (full) model
has projection matrix \[P\] for its OLS fit and the reduced (nested) model has a projection \[P_H\]
which arises from the constrained OLS, then necessarily we have \[P_HP=PP_H=P_H\] with both matrices
idempotent.

[[4.5 Canonical Form of H]] discusses wider applications.

*** 4.4 Multiple Correlation Coefficient

Nothing terribly cool here, just that if \[R\] is correlation between \[Y,\hat Y\] then
by Thm. 4.2., \[R^2=1-(n-1)\mathrm{RSS}/S^2\].

Then for the general Thm. 4.1 setting \[F=\frac{R^2-R_H^2}{1-R^2}\cdot\frac{n-p}{q}\].

*** 4.5 Canonical form for H

\[F_{q, n-p}\] is the positive distribution of the ratios of independent \[\chi^2_q/q\] and
\[\chi^2/(n-p)\] variables.

It is equivalent to an [[4.2 Likelihood Ratio Test][LRT]] b/c for Gaussian errors the test functions are the same.

Note there isn't a multivariate or two-sided UMP test here, so when we design tests here we look
for weaker criteria like asymptotic minimax optimality.

Given a general model \[Y=X\boldsymbol\beta+\boldsymbol\varepsilon\], \[\boldsymbol\varepsilon\sim N(0, \sigma^2 I)\],
where \[\boldsymbol\beta\in\mathbb{R}^p\] and a nested hypothesis \[H:A\boldsymbol\beta=\mathbf{c}\],
over \[q\le p\le n\] non-redundant constraints, Theorem 4.1 describes how to set up the hypothesis test,
by leveraging idempotent, symmetric projection matrices \[P,P_H\] for the OLS and OLS-with-constraints
models, where they observe the nestedness \[PP_H=P_HP=P_H\].

Thus  \[\underbrace{\mathrm{RSS}_H}_{\chi^2_{n-p-q}}-\underbrace{\mathrm{RSS}}_{\chi^2_{n-p}}\] is itself an independent \[\chi^2_q\] rv.

Then the statistic \[F=\frac{\mathrm{RSS}_H-\mathrm{RSS}}{\mathrm{RSS}}\cdot \frac{n-p}{q}\] is \[F_{q, n-p}\]-distributed. We also have simplifications

\[\mathrm{RSS}_H-\mathrm{RSS}=\|\hat Y-\hat Y_H\|^2=(\hat{\boldsymbol\beta}-\hat{\boldsymbol\beta_H})^\top X^\top X(\hat{\boldsymbol\beta}-\hat{\boldsymbol\beta}_H)=(A\hat{\boldsymbol\beta}-\mathbf{c})^\top(A^\top (X^\top X)^{-1} A)^{-1}(A\hat{\boldsymbol\beta}-\mathbf{c})\]

and

\[\mathrm{RSS}=\|Y-\hat Y\|^2=Y^\top (I-P)Y\]

By canonicalization, for \[\mathbf{c}=\mathbf{0}\] and \[q<p\] for \[A=\left(\begin{matrix}A_1&A_2\end{matrix}\right)\]
where \[A_2\] is \[q\times q\] with rank \[q\], breaking up \[\boldsymbol\beta=\left(\begin{matrix}\boldsymbol\beta_1\\ \boldsymbol\beta_2\end{matrix}\right)\]
\[X=\left(\begin{matrix}X_1&X_2\end{matrix}\right)\] conformably, we can set \[\boldsymbol\beta_2=-A_2^{-1}A_1\boldsymbol\beta_1\].

This lets us define \[P_H\] directly as the RSS of the equivalent model under \[H\], \[Y=X_H\boldsymbol\gamma+\boldsymbol\varepsilon\],
where \[X_H=X_1-X_2A_2^{-1}A_1\]. Compare this to the method of orthogonal projections in Section 3.8.2 and
the Pythagorean law application of equation (3.44). Pretty neat matrix equality.

Examples 4.4-4.6 have some typical derivations of tests, especially for the common marginal hypothesis
case. Here's another.

**** Chapter 4, Part d, Exercise 2

\[n_i\] observations \[Y^{(i)}=W^{(i)}\boldsymbol\gamma^{(i)}+\boldsymbol\varepsilon^{(i)}\] for \[i\in[2]\]
and \[\boldsymbol\varepsilon=\begin{pmatrix}\boldsymbol\varepsilon^{(1)}&\boldsymbol\varepsilon^{(2)}\end{pmatrix}\sim N(0, \sigma^2 I)\]. Let's create a test that \[Y^{(i)}\] came from the same linear model.

\[H:\boldsymbol\gamma^{(1)}=\boldsymbol\gamma^{(2)}\]. Then the full model is
\[Y=\begin{pmatrix}Y^{(1)}&Y^{(2)}\end{pmatrix}=X\boldsymbol\gamma+\boldsymbol\varepsilon\] where \[\boldsymbol\gamma=\begin{pmatrix}\boldsymbol\gamma^{(1)}\\\boldsymbol\gamma^{(2)}\end{pmatrix}\] and \[X=\begin{pmatrix}W^{(1)}&0\\0&W^{(2)}\end{pmatrix}\].

But our constraint for \[H\] can be expressed as \[\begin{pmatrix}I&-I\end{pmatrix}\boldsymbol\gamma=\mathbf{0}\] so by
canonicalization \[X_H=\begin{pmatrix}W^{(1)}\\W^{(2)}\end{pmatrix}\]. Then our statistic based on projection
matrices from design matrices will satisfy the null distribution:

\[\frac{Y^\top(P_X-P_{X_H})Y}{Y^\top(I-P)Y}\cdot\frac{n_1+n_2-2p}{p}\sim F_{p, n_1+n_2-2p}\]


*** 4.6 Goodness-of-fit Test

This chapter presents a neat technique for repeat observations:
use tied normal means we can average across observations.

*** 4.7 F-test and Projection Matrices

Theorem 4.3 generalizes 4.1 to linearly dependent designs and constraints through
projections.

** 5 Confidence Intervals and Regions

**** 5.1 Simultaneous Interval Estimation

We are interested in estimating CIs for linear transformations \[\mathbf{a}_j^\top\boldsymbol\beta\].

We'd like to do this simultaneously for a set of linear transforms \[\mathcal{A}=\{\mathbf{a}_1,\cdots,\mathbf{a}_k\}\] where
the rank of this set is \[d\].

This applies to:
 - inference, when \[\mathbf{a}_j\] are standard basis vectors and \[d=k=p\]
 - mean prediction, when \[\mathbf{a}_j\] are standard basis vectors and \[k\gg d=p\]
 - other specialized hypotheses, which can have different dimensionality.

It's insufficient to simply use the t-intervals from the marginal distribution of \[\hat\beta_j\],
for instance, because the probability of at least one failure of \[(1-\alpha)\]-CI coverage among
\[k\] will look closer to \[k\alpha\] for small \[\alpha,k\].

Since \[\hat{\boldsymbol\beta}\sim N(\beta, \sigma^2(X^\top X)^{-1})\], given that we can estimate
\[\mathbb{E}S^2=\sigma^2\], \[A\hat{\boldsymbol\beta}\] follows a multivariate t-distribution, where
\[A^\top\] consists of column vectors from \[\mathcal{A}\].

Given this setup, we have several solutions for simultaneity:

 - Bonferonni :: Use \[\alpha/k\] CIs, appealing to union bound.
 - Maximum Modulus (MM) :: Useful computation accounting for \[\mathrm{cov}(\hat\beta_0,\hat\beta_1)\] for \[k=2\], based on max-of-t-distributions
 - S-method :: Bound simultaneous CI for \[\mathbf{h}^\top\boldsymbol\beta\] for all \[\mathbf{h}\in \mathcal{C}(A)\].

MM also applies to larger \[k\] for independent \[\mathbf{a}_j^\top\boldsymbol\beta\], but this
requires hypothesis testing on eigenvectors of \[X^\top X\], which is not really typical. One can still
use MM for non-independent t-distributions (the usual case), because the maximum decreases
as correlations increase.

S-method is really neat: it constructs an ellipse around \[\beta\] directly and bounds the
Rayleigh quotient of \[A\boldsymbol\beta\]. There's an important reduction from
\[\mathcal{A}\] to just its basis, as well.

The S-method's generality (full linear space CI coverage) makes it great for prediction settings
where we estimate CIs for \[k\gg p\], but it's not good for inference settings with \[d\approx k\],
where MM performs better (narrower CIs).

That said, it's unclear to me why you wouldn't want a Monte-Carlo like method to account for the
nonzero correlations between our linear transforms. This should in principle allow much narrower
CIs.

***** Monte Carlo Inference

Suppose we have a full rank OLS setup \[Y=X\boldsymbol\beta+\boldsymbol\varepsilon\] with
 \[\boldsymbol\varepsilon\sim N(\mathbf{0},\sigma^2 I)\], known variance for simplicity.

Say we're interested in the simultaneous CIs of \[A\boldsymbol\beta\], where wlog by the S-method
trick \[A\] has full crank.

In the case for known variance, why don't we use \[\hat{\boldsymbol\beta}-\boldsymbol\beta\sim N(0,\sigma^2(X^\top X)^{-1})\] directly?

Let \[D^{1/2}\] be the diagonal matrix containing the inverse standard deviation of each element
of \[A(\hat{\boldsymbol\beta}-\boldsymbol\beta)\].

Then consider \[\mathbf{z}=D^{-1/2}A(\hat{\boldsymbol\beta}-\boldsymbol\beta)\], which then follows
the known distribution \[N(\mathbf{0},D^{-1/2}A^\top (X^\top X)^{-1} AD^{-1/2})\].

We can compute the inverse survival function of \[\max_i\left|z_i\right|\] somewhat easily with monte carlo
by sampling normals and looking at the upper \[\alpha\] quantile; this gives the maximum of normals
correlated exactly how we expect \[A(\hat{\boldsymbol\beta}-\boldsymbol\beta)\] to be.

This yields a max-of-z-scores \[m_\alpha\]; then our CIs for \[\mathbf{a}_j^\top\boldsymbol\beta\]
are given by \[\mathbf{a}_j^\top\hat{\boldsymbol\beta}\pm D_{jj}^{1/2}m_\alpha\]

For unknown variance, I'm sure the above can be studentized in some way,
and it seems extensions should be possible to heterskedastic settings.

Perhaps this is very expensive, so it was not suggested in the book (which was, after all, published in 1977).

This would be essentially an extension of [[https://www.jstor.org/stable/1266931?seq=1][Hahn 1972]].


**** 5.2 Confidence Bands for the Regression Surface
**** 5.3 Prediction Intervals and Bands for the Response

For CIs for the response, rather than the mean of the response, we simply observe
that \[Y(\mathbf{x}_0)=\mathbf{x}_0^\top\boldsymbol\beta + \varepsilon\], so we need to increase
variance accordingly.

I wonder what we give up by using this fixed-X model for prediction: in the anti-causal setting
\[Y\rightarrow X\] the marginal distribution of \[X\] contains information about \[Y|X\] which
we're explicitly ignoring by using a fixed-X setup. This is, in principle, something ML methods
can take advantage of.

**** 5.4 Enlarging the Regression Matrix

*** 6 Straight-line Regression

** 6 Straight-line Regression

*** 6.1 The Straight Line

We have the two-variable model \[Y=\beta_0+\beta_1 x + \varepsilon\], 
which allows us to compute exact simultaneous CIs using correlated
t-distributions directly.

The x-intercept can be analyzed as the ratio of two correlated normal variables,
there exist both convenient or exact forms of CIs.

The Working-Hotelling confidence band can be applied to the entire line
to provide simultaneous CIs across the full range using the Scheffe's method.
This won't give straight lines, but is more narrow in applicable ranges (and
is narrowest at the point \[\overline{x},\overline{Y}\]).

An approach by [[https://www.jstor.org/stable/1269524?seq=1][Wynn and Bloomfield]] allows one to narrow the bands further for specific
ranges; this is a neat extension of the Scheffe method, and doesn't require
complex re-derivation for analyzing interval CIs like other mentioned approaches,
and it tends to be narrower anyway.

*** 6.2 Straight Line through the Origin

Of course, this simplifies the CI for the slope to a simple t-interval, but
the entire band across the full x range is now also defined by the same
t-interval by homogeneity.

*** 6.3 Weighted Least Squares for the Straight Line

With known weights, this reduces to the original model per the usual transform.

With unknown weights, one can use the MLE and rely on asymptotics or
solve with least squares and get more conservative intervals.

It seems strange to me to use unknown, mean-dependent weights. They don't talk
much about it or why one would really want this or be in this setting, but
[[https://www.jstor.org/stable/2983809?seq=1][Williams 1959]] is the reference.

From Exercises 1, Part b here, we get a setting for known weights:
gamma-distributed or poisson-distributed positive-only regression
(if we're regressing sums or counts, or instance, then it might make sense
 to scale variance by mean squared or mean, respectively).

*** 6.4 Comparing Straight Lines
Use a higher-dimensional normal means model to simplify a comparison
across multiple lines into the form of a linear test.

*** 6.5 Two-Phase Linear Regression

If a one-covariate linear regression undergoes a change of phase,
then this can be modelled directly by appropriately encoding the change of
phase directly into the model.

I.e., instead of 

\[Y=\begin{cases}\alpha_0+\alpha_1x+\varepsilon & x<\gamma\\\beta_0+\beta_1x+\varepsilon&x\ge \gamma\end{cases}\]

with a continuity condition that \[\alpha_0+\alpha_1\gamma=\beta_0+\beta_1\gamma\], one could instead
fit

\[Y=\varepsilon+\theta+\begin{cases}\beta_1 (x-\gamma) & x<\gamma\\\beta_2(x-\gamma) & x\ge \gamma\end{cases}\]

so that we don't have redundant parameters.

For a complete answer to the problem see [[https://onlinelibrary.wiley.com/doi/10.1002/0471725315.ch9][Seber and Wild 1989]], Chapter 9.3.

*** 6.6 Local Linear Regression 

** 7 Polynomial Regression
*** 7.1 Polynomials in One Variable

Naively applying least squares to the model \[y\sim \boldsymbol\beta^\top\mathrm{poly}(x)\]
where \[\mathrm{poly}(x)_i=x^i\] for single-variable polynomial regression
may work mathematically to reuse the same machinery but results in a Vandermonde
design which in turn has an ill-conditioned Hilbert \[X^\top X\] Gram matrix.

Since LS is not practically feasible, stable solutions are needed.
Indeed, the desired approach is to construct an orthogonal basis dynamically,
with \[\phi(x)\] replacing \[\mathrm{poly}\] above such that \[\phi(x)_i=\phi_i(x)\]
with \[\{\phi_i\}_i\] an orthogonal basis with respect to the norm defined by 
the covariates themselves. For our simple two-variable dataset \[\{x_i,y_i\}\],
define the product \[\langle f, g\rangle=\sum_if(x_i)g(x_i)\]. This can be dynamically constructed using
the recurrence

\[\phi_0=1\], \[\phi_1(x)=2(x-a_1)\], \[\phi_{r+1}(x)=2(x-a_{r+1})\phi_r(x)-b_r\phi_{r-1}(x)\]

Such a basis takes \[O(nk)\] time to construct for degree \[k\], since the polynomials can be defined
through their values at the points or coefficients. Moreover, the Gram matrix
\[\phi(\mathbf{x})^\top\phi(\mathbf{x})\] is diagonal, so the system can be efficiently
solved.

The Chebyshev basis can be used to represent \[\phi_k\], which comes with its own
set of recurrence relations in terms of Chebyshev coefficients in the text.
The primary win here is that you can perform inference outside of the training
set efficiently, too. See [[https://en.wikipedia.org/wiki/Clenshaw%E2%80%93Curtis_quadrature][Clenshaw 1960]].

To add constraints to the construction, see, e.g., [[https://ieeexplore.ieee.org/document/1099532][Payne 1970]], [[https://academic.oup.com/imamat/article-abstract/1/2/164/656295][Clenshaw and Hayes 1965]]

*** 7.2 Piecewise Polynomial Fitting

Regular polynomials may have very slowly decreasing RSS as degree increases
or systemic residuals.

This is especially the case when dealing with functions that are piecewise
varying across stages.

Splines of order \[M\] with \[K\] knots \[\boldsymbol\xi_k\] for \[k\in[K]\] are:

 - order \[M-1\] polynomials on each piece \[\xi_k, \xi_{k+1}\]
 - globally \[\mathcal{C}^{M-2}\]

I.e., continuity up to the last order at knots. Repeat knots lower breakpoint
continuity constraints.

For visual purposes, [[https://web.stanford.edu/~hastie/ElemStatLearn/][ESL - Splines]], \[M=4\] or cubic splines typically suffice.

While splines admit a parsimonious \[K+M\] parameter representation, it's
unstable to compute. A redundant \[K+2M\] representation allows for a stable,
recursive formula.

Various forms of regularization are possible, but we're essentially
entering ML territory, with methods becoming heuristic and reliance on CV
for seleciton.

*** 7.3 Polynomial Regression in Several Variables

This section considers extensions to both splines and polynomials
to higher dimensions, but the curse of dimensionality is not considered.

Approaches mentioned here are either tensor decomposition or exponential-in-dim
methods that naively extend lower-dimensional approaches.

** 8 Analysis of Variance

*** 8.1 Introduction

*** 8.2 One-way Classification
    
The main reason ANOVA works (more efficient test at whether all means for
separate populations are equal the full pairwise implication), i.e.,
can efficiently reject \[H_0:\mu_1=\mu_2=\cdots=\mu_n\] is framing in terms
of contrasts, or linear transformations of the means. Transforming the
problem into \[H_0:\mu_1-\mu_n=\mu_2-\mu_n=\cdots=0\], we can equivalently
ask if the vector of the above mean differences is uniformly 0.

This holds under all linear transformations being, so ScheffÃ©'s method is
appropriate.

Specializations for particular contrasts rather than all are available too.

Balanced designs help simplify the intervals, but don't afford significant
efficiency gains because of it (balanced means every group has same number
of participants).

However, per Miscellaneous Exercises 7, balanced designs DO get rid of
interaction effects, which is helpful.

The F-test assumptions are the same as OLS.

Balanced tests absolve (partly) normality and homogeneity requirements for
the F-test. That said [[https://www.jstor.org/stable/2332579][Welch's test]] directly allows heteroscedasticity,
which seems to be a set of approximations given the stochastically estimated
noise [[https://math.stackexchange.com/questions/1746329/proof-and-precise-formulation-of-welch-satterthwaite-equation][for sample variances]].

Heteroscedasticity checks (though they shouldn't be run to decide on
whether to use Welch's) can be done by looking at Levene's test, which
has a robust version based on median and LAD.

*** 8.3 Two-Way Classification (Unbalanced)

Two-way ANOVA can be equivalently modeled by an OLS setting
with indicators for two possibly multi-level factors (which can
all be dummy coded). The model includes interaction terms.

I.e., for \[I\] levels on the first and \[J\] levels on the second,
the design matrix here has \[1 + I + J + IJ\] terms, which is of course
overparameterized (there are \[IJ\] mean parameters and a single variance
parameter). The overparameterization can be dealt with with many different
ways, either by coding contrasts or using constraints. The constraint
is that all population means are the same across the same main effect.

The presence of interaction terms signifies that there is a "difference
in differences": there is no interaction (across any of the levels) when the
term \[(\mu_{i_1j_1}-\mu_{i_1j_2})-(\mu_{i_1j_2}-\mu_{i_2j_2})\] is uniformly
\[0\] across any two levels \[i_1,i_2\] of \[I\] and \[j_1,j_2\] of \[J\]. This set of constraints can, wlog,
be taken "relative to" base values \[i_1,j_1=1\] so it's actually just
\[(I-1)(J-1)\] independent constraints total.

This "difference in differences" is what gets tested for interaction coefficients.

Then there are several procedures when trying to fit a model in this setting:
first the grand mean (intercept) is tested against zero. Assuming this
succeeds, then there are several approaches.

Type I ANOVA. Test for significance of including the first factor in the model,
next the other factor, and finally interactions. This test has the advantage
of creating independent RSS tests (which we can see by the usual \[\chi^2\]
analysis of the residual OLS fit operators that are idempotent matrices \[(I-P)\]
for each fit.

Type II ANOVA. Two tests for main effects given other main effect and interactions.
Not independent, but symmetric. Final Type I ANOVA model depends on ordering.
Dubious, since the models compared here are not nested, but in principle
this is possible, if a factor is "activated" by another.

Type III ANOVA. Tests for significance of each of the two main effects and
the single interaction effect give the rest (the other main and the interaction, or
the two mains). Also not independent, but nested.

To be frank, using the above methods blindly seems cargo-culty. If you're coming
in with a hypothesis, then just test that hypothesis.

*** 8.4 Two-Way Classification (Balanced)

Math simplifies

*** 8.5 Two-Way Classification (One Observation per Mean)

Under-defined, b/c you have \[IJ\] observations but \[IJ + 1\] unknowns
(means for every cell in the 2-way table and the overall variance).

Usually, only an additive model is assumed here, since this setting
mostly appears for RCTs. Other richer interactions can be modelled with
lower-rank representations of the interaction matrix in multilevel settings.

*** 8.6 Higher-Way Classifications with Equal Numbers per Mean

Higher-order ANOVAs generalize the notion of differences in differences.

Below are more my notes for this topic, I think the recursive case is simpler
than going through 3-way and 4-way manually like this book does.

For a factor \[A\], and some response \[Y\], define the delta operator
\[\Delta_AY=\mathbb{E}[Y|A]-\mathbb{E}[Y|\lnot A]\]. In other words, the lift of \[A\]. This gives the coefficient
for \[A\] in a binary factor and is the effect tested for in one-way ANOVA.
(for multilevel, it's all relative to some base null factor value).

The delta \[\Delta_AY\] is itself a random variable, so we can ask
\[\Delta_B\Delta_AY=\mathbb{E}[\mathbb{E}[Y|A]-\mathbb{E}[Y|\lnot A]|B]-\mathbb{E}[\mathbb{E}[Y|A]-\mathbb{E}[Y|\lnot A]|\lnot B]\]
which boils down to \[(\mathbb{E}[\mathbb{E}[Y|A, B]-\mathbb{E}[\mathbb{E}[Y|A, \lnot B])-(\mathbb{E}[\mathbb{E}[Y|\lnot A, B]-\mathbb{E}[\mathbb{E}[Y|\lnot A, \lnot B])\], the
difference in differences. This is equal to \[\Delta_A\Delta_B Y\] (the
delta operator is commutative).

This, for OLS, also corresponds to the appropriate corresponding interaction
coefficient for \[A,B\].

Higher-order ANOVAs, over \[k\] factors \[F_1,\cdots,F_k\] are correspondingly linear models, which have
means of the recursive difference-in-differences \[\Delta_{F_1}\cdots\Delta_{F_k}Y\] as OLS parameters.

*** 8.8 Analysis of Covariance

Again, just reduce to the OLS model to come up with a hypothesis.
ANCOVA is when you do ANOVA but also throw continuous covariates in there
in addition to the factors.

Chapter 8 Part e Excercise 1 is interesting, because it can use the Theorem 3.6
partial fit result to perform regular ANOVA followed by OLS residual fitting
of the covariates.

** 9 Departures from Underlying Assumptions
*** 9.1 Introduction
*** 9.2 Bias

Underfitting occurs when you're missing covariates relative to the new model.

I.e., fitting \[Y\sim X\beta+\varepsilon\] but the true model is \[Y\sim X\beta + Z\gamma+\varepsilon\]

 - Fitted model becomes what you'd expect if you regress the missing covariates on the present ones
 - Bias term for the OLS predictor \[\hat\beta\] is \[(X^\top X)^{-1}X^\top Z\gamma\]
 - The variance estimator \[S^2\] becomes an overestimate
 - \[\mathrm{var}\hat\beta\] remains correct.

Overfitting occurs when additional regressors are added. This inflates the \[\mathrm{var}\hat\beta\],
but variance of noise and the estimate are unbiased.

The fact that during underfitting variance stays the same allows for creating a test for underfit,
see [[https://www.jstor.org/stable/2984219?seq=1][Ramsey 1989]].

*** 9.3 Incorrect Variance Matrix

In this scenario, homoskedasticity is violated but WLS is not used instead of OLS.

The coefficients remain unbiased but variance for both the coefficients and the noise
(and thus the fit) can become biased in either direction.

*** 9.4 Effect of Outliers

The previous section [[3.13 Robust Regression]] covered OLS alternatives
and evaluation measures. This section looks precisely on a single point's
effect on the OLS fit.

For a projection matrix \[P\], let the projected fit \[\hat Y = PY\] so that
\[\hat y_i=p_{ii}y_i+\sum_{j\neq i}p_{ij} y_j\]. Assuming centered covariates, per (3.53),
\[p_{ii}=n^{-1}+(n-1)^{-1}\|\mathbf{x}_i-\overline{\mathbf{x}}\|_{S_{xx}}^2\|\ge n^{-1}\], where \[S_{xx}\] is the sample
variance-covariance of the covariates and the corresponding norm
\[\|\mathbf{x}\|_A^2=\mathbf{x}^\top A^{-1}\mathbf{x}\].

Further, since \[P^2=P\] it's clear \[p_{ii}\le 1\] as \[\sum_{j}p_{ij}^2\ge p_ii^2\ge p_i\].

This gives \[p_{ii}\] in the range \[[n^{-1}, 1]\], where the Mahalanobis distance
defined by the gaussian \[N(\overline{\mathbf{x}}, S_{xx})\] determines
high-leverage points. Points low in the Gaussian density defined above
are high-leverage, having \[p_{ii}\approx 1\], and thus \[\partial_{y_i}\hat y_i\approx 1\]
so that high-leverage points have their fit more or less directly influenced
by the y value at that point.

*** 9.5 Robustness of the F-Test to Non-normality

The F-test can be used for both equality of variance and equality of means.

For variances, the test is very sensitive to non-normality, but not so for means.

How can this be the case for the same distribution?

This boils down to being really careful about what we mean about sensitivity.
It's not directly stated in the chapter, but the robustness referred to here is
really robustness not under the alternative, but under some "pretend null"
where, e.g., the variance is equal between two samples, but the error distribution
is some mean-0 non-normal distribution. Similarly, for the mean equality, the "pretend null"
is that the means are equal but the noise is some mean-0 non-normal distribution.

This chapter does not analyze some alternative hypothesis for these two cases; that's to
ill-defined.

However, in the above sense (that the test still works, namely has a reasonable Type-I rate,
under a pretend null that's not exactly normal but otherwise correct), the robustness
properties hold.

The reason boils down to the different projection matrices.

For equality of variance, our setup is that we have a sample of iid \[X_i\] of size \[f_1\]
and similarly for \[Y_i\] of size \[f_2\]. The \[F=S_X^2/S_Y^2\] is the ratio of two
\[\chi^2_{f_i}\]-distributed population variance estimators. Expressed under the standard OLS model,
this corresponds to a single global \[Y=\begin{pmatrix}X_1\\X_2\end{pmatrix}\beta+\varepsilon\],
where \[X_1=\begin{pmatrix}1_{f_1}&0\end{pmatrix}\] is a \[f_1\times 2\] matrix corresponding to the
\[X_i\] observations and similarly for \[X_2=\begin{pmatrix}0&1_{f_2}\end{pmatrix}\] of the \[Y_i\].
Then the corresponding projection matrices \[P_1,P_2\] give rise to the aforementioned F-statistic.
Interestingly, here we have \[P_1P_2=0\].

Then for means, our setup is similar, but instead the design matrix compares splitting means
to having the same one, so it is a \[f_1+f_2\times 3\] matrix of the form 
\[X_1=\begin{pmatrix}1_{f_1}&0\\ 0&1_{f_2}\end{pmatrix}\] under the alternative and
\[X_2=\begin{pmatrix}1_{f_1+f_2}&0\end{pmatrix}\] under the null. Note that now, each model is considered separately, whereas above
we relied on the same OLS model. Here, we have another \[F=\frac{Y^\top (P_1 - P_2) Y}{Y^\top P_1 Y}\] following
an \[F\] distribution, but here of degrees of freedom \[1,f_1+f_2 - 2\] (this test, unlike equality of
variances, can be naturally extended to having multiple groups of comparison with equal means).
Note that here \[(P_1-P_2)P_1\neq 0\].

The different projection matrix relationships give rise to different robustness properties.

For variance, [[https://www.jstor.org/stable/2983753?seq=1][Atiqullah 1962]] provides an analysis for F-statistics where projection matrices
satisfy \[P_1P_2=0\] and under the null variances are equal. Then a condition called quadratic
balance, met by balanced (and randomized, for other reasons) designs, yields robustness to
non-normality, as measured by excess kurtosis \[\gamma_2=\mathbb{E}\left[\left(\frac{Y-\mathbb{E} Y}{\sigma}\right)^4\right]-3\],
which is the fourth centered moment less the fourth centered moment of the Gaussian. In particular, we have
the following theorem (9.2): for symmetric, idempotent projection matrices \[P_i\] with vanishing product,
assume \[\mathbb{E}[Y^\top P_i Y]=\sigma^2 f_i\] and let the diagonal of \[P_i\] be \[\mathbf{p}_i\].
If the shared excess kurtosis of each variate is \[\gamma_2\], and each \[Y_i\] further has shared
variance, third, and fourth moments (but means can differ), then \[Z=\frac{1}{2}\log F\] is approximately normal,
corresponding to \[F=\frac{Y^\top P_1 Y / f_1}{Y^\top P_2 Y / f_2}\] being approximately F-distributed
with \[Z\] being independent of \[\gamma_2\] if \[f_1\mathbf{p}_2=f_2\mathbf{p}_1\] (quadratic balance).

For means, [[http://www.biostat.jhsph.edu/~iruczins/teaching/140.752/read/papers/box.1962.pdf][Box and Watson 1962]] provide an analysis based on the 4th cumulant bound where
the resulting F-statistic for the equality-of-means test above (but generalized to \[p\] groups,
checking equality of all their individual means under the null, behaves like an F-distributed
random variable, but with \[\delta (p - 1),\delta (f_1+f_2-1)\]. The shrink factor for degrees of freedom \[\delta\]
is defined by \[\delta^{-1}=1+C_x\Gamma_y/n\], where \[C_x,\Gamma_y\] are 4th-cumulant based bounds
that are each approximately 0 the closer the (multivariate) distribution of the covariates or
noise is to the normal.

*** 9.6 Effect of Random Explanatory Variables

**** Structural Law

The assumed OLS Model describes a stochastic (almost sure) structural law of the form
\[\mathbb{E}[Y|X]=\beta^\top X +\varepsilon\] where the noise is exogenous, i.e., \[\mathbb{E}[\varepsilon|X]=0\].

This corresponds to random covariates observed without error.

The book refers to a special case that's way too narrow with \[\varepsilon=0\] exactly. But the book does
describe the fact that if extra unrelated covariates are observed then we're not actually in trouble,
as OLS will naturally feature select. The "feature selection" is not like lasso here but rather due to
the CIs including 0. This seems like a silly way of dressing up the overfit discussion earlier; note that
just like in the fixed effects case the variance will increase if extra variables are included.

**** Functional Law

Here, the assumed OLS model describes a fixed equality derived from a law \[\mathbb{E}[y]=\beta^\top \mathbb{E}[\mathbf{x}]\]
(note no noise). Such relationships arise from random variables representing unbiased measurements of an underlying
physical phenomenon. This is a fixed effects model with \[y=\beta^\top \mathbb{E}[\mathbf{x}]+\varepsilon\].

Let \[\mathbf{u}=\mathbb{E}[\mathbf{x}]\] be the underlying covariates and write \[\mathbf{x}=\mathbf{u}+\Delta\] where by construction
\[\mathbb{E}[\Delta]=0\]. Let \[\mathbf{var} \Delta= D\], say. Suppose we make several observations and stack the design 
and outcomes as \[X,Y\] and proceed with naive OLS. Let \[U=\mathbb{E}[X]\] be the expected design.

Naive OLS yields \[\hat\beta_\Delta = (X^\top X)^{-1} X^\top Y\]. By Equation (9.30),
the bias \[\mathbf{b}\] is given by \[\mathbb{E}[\hat\beta_\Delta]=\beta-\mathbf{b}\] with \[mathbf{b}\approx \left(\frac{U^\top U}{n}+D\right)^{-1}D\beta\],
which is of course unhelpful since it requires oracle knowledge.

Interestingly, though, if we have an unbiased estimate \[\hat D\] of \[D\] then the book goes that
we can get an unbiased estimate \[\hat{\mathbf{b}}=n(X^\top X)^{-1}\hat D \beta\].

Going off-script a little (above is based on [[https://www.jstor.org/stable/2335377?seq=1][Davies and Hutton 1975]]), we note first that the bias estimator
above is useless as it still depends on \[\beta\] which is what we're estimating. But since everything is linear
and unbiased, note that \[\mathbb{E}\hat{\mathbf{b}}=\mathbf{b}\] so then \[\mathbb{E}[\hat\beta_\Delta]=\mathbb{E}[\beta+\hat{\mathbf{b}}]\]. Expanding our estimator, we have

\[\mathbb{E}[\hat\beta_\Delta]=\mathbb{E}[I\beta + n(X^\top X)^{-1}\hat D \beta]=\mathbb{E}[I + n(X^\top X)^{-1}\hat D] \beta\] by linearity.

Since \[\hat D, (X^\top X)^{-1}\] are both symmetric and PSD, so their product
 (though possibly not symmetric) has [[https://math.stackexchange.com/a/113859/38471][positive eigenvalues]],
and thus the matrix in the expectation on the RHS above is invertible.

Then moving the matrix to the other side, we have

\[\mathbb{E}[I + n(X^\top X)^{-1}\hat D]^{-1}\mathbb{E}[\hat\beta_\Delta]= \beta\]

which we unfortunately can't just push into one expectation due to nonlinearity.
See [[https://mathoverflow.net/a/307168][this MO answer]] for details. That said, it can be directly seen from the continuity of the matrix
inverse and continuous mapping theorem that the psuedo-debiased (it's not actually unbiased)
estimator \[\left(I + n(X^\top X)^{-1}\hat D\right)^{-1}\hat\beta_\Delta\] is consistent as an estimator of \[\beta\].

In fact, it's basically an order-one method of moments estimator. So I bet that analysis applies here. The interesting
part here is that matrix inverse is a known operation and we might be able to do better in estimating it than a generic
nonlinear equation like method of moments usually does. You still end up having to know \[\hat D\], but in principle this
is something you can estimate by repeatedly sampling the same "setting" \[U\]. I.e., if our experimental setup
lets us sample \[U\] repeatedly, then we'd get multiple \[X\] (after which the regression could in principle even
be carried out on the average \[X\] and the average \[Y\]).

**** Other Cases

The other cases don't seem too interesting. They include fixed, non-random errors (such as round-off) which
can be analyzed as noiseless versions of the [[Functional Law]] description and random covariates measured with error
(which is undetermined, as there are now two noises, one from the stochasticity of covariates and one from
their error, but only one set of residuals to estimate average error). The last case can be resolved
with assumptions about the relative sizes of each error source.

One approach that wasn't mentioned but would be interesting to look into would be [[https://en.wikipedia.org/wiki/Total_least_squares][Total least squares]].

[[https://en.wikipedia.org/wiki/Errors-in-variables_models][Error-in-variables models]] seems like a good search term here as well.

*** 9.7 Collinearity

Since \[\mathrm{var}\hat\beta=\sigma^2(X^\top X)^{-1}\], it's no surprise that rank deficiency
results in uncertainty.

It also affects robustness. This chapter looks at both.

**** Variance due to Covariate Collinearity

Suppose we have centered, scaled covariates, so that
\[X^\top X = \begin{pmatrix}n &0\\0&R_{xx}\end{pmatrix} = \begin{pmatrix}n &0 &0\\0 & 1 &\mathrm{r}^\top \\ 0 & \mathrm{r} & R_{22}\end{pmatrix}\]
where \[R_{xx}\] is the matrix of covariate correlations and the right hand
side is its block decomposition,such that \[r_i=\langle X_1, X_i\rangle \] for the vector \[\mathrm{r}\]
Then by linear algebra \[\mathrm{var}\hat\beta_1=\sigma^2(1-\mathrm{r}^\top R_{22}^{-1}\mathrm{r})^{-1}\].

This of course applies wlog to other covariates than the first.

We can also see that \[R_1^2=\mathrm{r}^\top R_{22}^{-1}\mathrm{r})^{-1}\] is itself the coefficient
of determination of regressing \[X_1\] on the rest of the columns of \[X\] by looking at the
RSS of that virtual regression.

More generically, we can define the variance inflation factor, for nonscaled
coefficients, which is \[\mathrm{VIF}_j=\frac{\mathrm{var}\hat\beta_j}{\sigma^2}=s_j^2(1-R_j^2)^{-1}=s_j^2(X^\top X)^{-1}_{(j+1)(j+1)}\]
where \[s_j^2\] is the (now non-unit) variance of the \[j\]-th coefficient, and we can see
Cramer's rule at play!

The book provides other approaches to variance analysis due to collinearity but
I did not find thme that illuminating.

**** Lower Robustness due to Covariate Collinearity

Suppose we have an OLS setup \[\mathbf{y}=X\beta+\varepsilon\] but we
replace \[X\] with \[X+\delta X\] and \[\mathbf{y}\] with \[\mathbf{y}+\delta \mathbf{y}\]
(where \[\delta X, \delta \mathbf{y}\] should be though of as single variables,
not products of variables. Further, these can be arbitrary perturbations,
so it's *not* necessarily the case that \[\delta \mathbf{y} = (\delta X) \beta\].

Per [[https://books.google.com/books/about/Accuracy_and_Stability_of_Numerical_Algo.html?id=7J52J4GrsJkC&source=kp_book_description][Higham 1996]] (page 392), if \[\|\delta X\|_2\le \epsilon \|X\|_2\] (by spectral norm)
\[\|\delta \mathbf{y}\|\le\epsilon\|\mathbf{y}\|\], and \[\kappa \epsilon <1\], where \[\kappa=\frac{\sigma_{\max}(X)}{\sigma_{\min}(X)}=\sqrt{\frac{\lambda_{\max}(X^\top X)}{\lambda_{\min}(X^\top X)}}\] is
the condition number of the design, then the resulting OLS applied to
the perturbed inputs gives an estimator \[\hat{\beta}_\epsilon\] whose relative error is bounded by

\[\frac{\|\hat\beta-\hat{\beta}_\epsilon\|}{\|\hat\beta\|}\le\frac{\kappa\epsilon}{1-\kappa\epsilon}\left(2+(1+\kappa)\frac{\|X\hat{\beta}_\epsilon-\mathbf{y}\|}{\|X\|_2\|\hat\beta\|}\right)\]

** 10 Departures from Assumptions: Diagnosis and Remedies

*** 10.1 Introduction

There are a few common types of assumption mismatches with OLS. Consider
the stochastic OLS setup where \[Y=X\cdot\beta + \varepsilon\]

1. Nonlinearity of \[\mathbb{E}[Y|X]\]
2. Heteroscedasticity, where \[\mathrm{var}(Y|X)\] is non-constant.
3. Endogeneity \[\mathbb{E}[\varepsilon|X]\neq 0\]
4. Non-independence of errors (between training examples)
5. No pathological outliers

With (2) heterogeneity, but known variance, (diagonal) WLS applies.

With (4) error correlation, but the correlation between errors is known
and the errors are Gaussian, WLS applies.

When none of the above hold (we have linearity, homoskedasticity, exogeneity of error,
independent errors, and no corruption of the sample),
then the OLS estimator is unbiased, if inefficient in the non-Gaussian case.

*** 10.2 Residuals and Hat Matrix Diagonals

The hat matrix is the projection matrix \[P\]. In the OLS model, for residuals \[\mathbf{e}\]
that \[\mathrm{var}\mathbf{e}=\sigma^2(I-P)\]. Thus we can assess the
\[i\]-th residual under the null by inspecting the internally Studentized residual
\[\frac{e_i}{S(1-h_i)^{1/2}}\], where \[S\] is the residual population standard deviation
estimator and \[h_i=P_{ii}\].

Theorem 10.1 describes an extension based on using the external estimator \[S(i)\] which
uses all-but-the-\[i\]-th point to estimate residual variance, to support the
single-outlier adversarial scenario (5). It can be computed easily using a rank-one
update of the OLS fit.

*** 10.3 Dealing with Curvature

\[\mathrm{cov}(\mathbf{e}, \hat Y)=\sigma^2(I-P)P=0\], which can be promoted to independence under
normality. 

As a result, one can plot residuals against the sorted fitted values, and there should be no
visible correlation in the plot (or any fixed subset of the plot). Thus residual plots give
a strong visible mechanism for checking model fit.

Note that the overall correlation is forced (by the fit itself) to be 0.

This suggests a rudimentary automated mechanism for linearity testing against an alternative of
a piecewise smooth fit, which would be a permutation test of some sort: under a true linear model,
the maximum correlation of contiguous sub-blocks of \[\mathbf{e}\] with \[\hat Y\] for a given
block length is known. If a contiguous subsection of the plot has a strong correlation, that's indicative
of nonlinearity. I wonder how this performs against a more explicit test for such a spline model.
Other approaches are described in this [[https://stats.stackexchange.com/questions/405961/is-there-formal-test-of-non-linearity-in-linear-regression][Cross Validated]] thread.

Note that there are examples (the [[https://www.google.com/books/edition/_/k1esnwVBVSwC?hl=en&gbpv=0][Cook 2009]] work is referenced) where residual plots miss curvature
because they are lower dimensional.

Partial residual plots are proposed and argued against as lame.
Box-cox transforms and GAMs are called out but not in great detail.

One useful plot is the partial residual plot, which plots the residuals
of the regression with \[x_j\] removed versus \[x_j\]. This can be shown
to be linear precisely when the \[F_j\] statistic, for the corresponding
coefficient in the original regression, is large.

[[./exercises-10.3.ipynb][Python exercises]]

*** 10.4 Non-constant Variance and Serial Correlation

*NOTE, accounting for non-constant variance is not the same as accounting for non-independence.*

A branch of modeling (multi-level modeling, or mixed effects modeling, with the linear case
known as linear mixed models or LMM) deals with noise that violates independence assumptions
between data points.

For instance, suppose we are modeling blood pressure for individuals and we'd like to regress
dietary factors against blood pressure. If we take multiple measurements for individuals
on a given day, we could consider dietary factors as covariates, but would need to use
a random effects model to account for random skew to blood pressure individuals may introduce
(we can get more complicated with this example: we have possibly a full graphical model on our
hands, where we could consider time since the individual was awake as a latent variable, etc.).

Interestingly, REML fits are used for both heteroskedasticity and LMMs.

**** Parameterized Variance

One issue that may occur is that the per-datapoint variance
\[\mathrm{var}(\varepsilon_i)=w\left(\mathbf{z}_i,\boldsymbol{\lambda}\right)\] may be
a function of some other covariates \[\mathbf{z}_i\].

Define variance terms as the un-studentized residuals.

[[https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test][BreuschâPagan]] allows for arbitrary exp-linear models defined by covariates
\[\mathbf{z}\] for setting variance as the alternative for heteroskedasticity,
[[https://www3.nd.edu/~rwilliam/stats2/l25.pdf][Class notes]] describe BP, see also [[https://en.wikipedia.org/wiki/Levene%27s_test][Wikipedia]] lists others.

The gist is that the residuals \[\mathbf{e}\] after an OLS fit contain the
information necessary to assess heteroskedasticity because of their
relationship with the response variases through the hat matrix.

Intuitively, \[\mathrm{var}(\mathbf{e})=\mathrm{var}((I-P)\boldsymbol\varepsilon)\] which is known
since \[\mathrm{var}(\boldsymbol\varepsilon)\] is assumed diagonal for WLS. If \[\mathrm{var}(\varepsilon)=\sigma^2 I\] this lets
 us derive \[\mathrm{var}(e_i)=(1-p_{ii})\sigma^2+\sum_{k\neq i}\frac{p_{ik}}{1-p_{ii}}\sigma^2=\sigma^2\] by idempotence of \[I-P\].
But since \[P\] is known we could also check against a particular alternative \[\mathrm{var}(\boldsymbol\varepsilon)\].

More formally, we can consider a score-based method to evaluate whether or not variance is
constant against a parameterized alternative via score-based methods.

***** A Maximization-Maximization Approach

To make the hypothesis nested, suppose there exists a \[\boldsymbol\lambda_0\] such that
\[w(\mathbf{z},\boldsymbol\lambda_0)=\sigma^2\] is a constant for all \[\mathbf{z}\]. Then our null hypothesis of constant variance
becomes \[H_0:\boldsymbol\lambda=\boldsymbol\lambda_0\].

Algorithm 10.2 shows that OLS in this form can be fit with coordinate descent on the
maximum log-likelihood objective, which is the log likelihood of the data,
\[\ell(\boldsymbol\beta,\boldsymbol\lambda)=\sum_{i}\log p_{\boldsymbol\beta,\boldsymbol\lambda}(y_i, \mathbf{x}_i, \mathbf{z}_i)\]
with \[p_{\boldsymbol\beta,\boldsymbol\lambda}(y_i, \mathbf{x}_i, \mathbf{z}_i)\] the density under the model
\[y_i\sim N(\boldsymbol\beta^\top\mathbf{x}_i, w(\mathbf{z}_i,\boldsymbol\lambda))\]. The \[\boldsymbol\beta\] term can be fit conditionally on \[\boldsymbol\lambda\]
with WLS and then for fixed \[\boldsymbol\beta\], \[\boldsymbol\lambda\] can be found with some kind
of smooth optimality condition. Iterating this yields a maximization-maximization algorithm,
which finds a local optimum of the overall maximum likelihood objective.

Some useful algebra (equations 10.30, 10.32) show that in the general case, for some constant \[c\]

\[\ell(\boldsymbol\beta,\boldsymbol\lambda)=c-\frac{1}{2}\sum_{i=1}^n\log w_i+\frac{(y_i-\boldsymbol\beta^\top\mathbf{x}_i)^2}{w_i}\]

where above \[w_i=w(\mathbf{z}_i,\boldsymbol\lambda)>0\]. With \[\Sigma\] the diagonal matrix of entries
\[w_i^{-1}\], the WLS step is just a matter of computing \[\hat{\boldsymbol\beta}=(X^\top\Sigma^{-1}X)^{-1}X^\top\Sigma^{-1}\mathbf{y}\] (per
Section 3.10 Generalized Least Squares, Exercises 1b, (3) for motivation).

For fixed \[\boldsymbol\beta\], the \[\boldsymbol\lambda\] step is more interesting. For
continuously differentiable \[w\], first order necessary conditions (FONC) require

\[\mathbf{0}=\partial_{\boldsymbol\lambda}\ell(\boldsymbol\beta,\boldsymbol\lambda)=-\frac{1}{2}\sum_i\left(\frac{1}{w_i}-\frac{(y_i-\mathbf{x}_i^\top\boldsymbol\beta)^2}{w_i^2}\right)\partial_\lambda w_i\]

But this by itself only gets us to a local maximum, whereas since we're relying on
asymptotic maximum likelihood guarantees, we need a unique maximum for \[\ell\] (at least
asympototically).

This occurs under sufficient regularity, e.g., \[\ell\] being strictly concave.

Some more practical conditions will depend on \[p_{\boldsymbol\beta,\boldsymbol\lambda}\]
smoothness. In general, if \[\nabla_\theta,\mathbb{E}_\theta\] commute (where \[\theta=(\boldsymbol\beta,\boldsymbol\lambda)\]), then the Fisher
information is equal to the score covariance, and thus the Hessian of the log-likelihood
is negative semi-definite. This allows for second order sufficient conditions.
Sufficient conditions for the Fisher equality are,
for instance, continuity and integrability of the score \[\nabla_\theta p_\theta(y, \mathbf{x},\mathbf{z})\] with respect to \[\theta\]
(see [[https://en.wikipedia.org/wiki/Leibniz_integral_rule#Measure_theory_statement][Measure theory version of Leibniz's rule]]). This is handily
met by most well-behaved parameterized densities. Eventually, we'll need stronger
conditions for maximum likelihood asymptotic normality anyway, for which a weak
condition is Differentiability in Quadratic Mean (DQM). See [[https://web.stanford.edu/class/stats311/Lectures/lec-09.pdf][Fisher notes]] and [[https://www.stat.berkeley.edu/~bartlett/courses/2013spring-stat210b/notes/22notes.pdf][DQM notes]] for
details.

In any case, supposing the DQM condition, the FONC are sufficient for maximization,
because of implied concavity, so we just have to solve the above equation.

***** The Special Exp-Linear Variance Case

For the special case \[w(\mathbf{z},\boldsymbol\lambda)=\exp(\boldsymbol\lambda^\top\mathbf{z})\], we can do the above. Page 285 in the book
and Equation 10.34 summarize the updates. This derivation ends up being equivalent
to Fisher scoring, which directly optimizes \[\ell\] with a Newton-Raphson approximation
based on using the empirical score covariance \[\hat {\mathrm{var}}(\nabla_\theta p_\theta)\]
for the current iterate \[\theta\] as if it was the true Hessian (in fact it's just an unbiased
estimator for it). It's a descent direction (by positive semi-definiteness), so convergence is clear,
but rate of convergence is more complicated. See [[http://hua-zhou.github.io/teaching/biostatm280-2017spring/slides/18-newton/newton.html][these notes]] for extended discussion.

This, in turn, gives a direct asymptotic (likelihood ratio) test based on deviance, where
\[\mathrm{LR}=-2\left(\ell(\hat{\boldsymbol\beta}_{\mathrm{OLS}}, \mathbf{1}\hat\sigma^2)-\ell(\hat{\boldsymbol\beta},\hat{\boldsymbol\lambda})\right)\]. Of course, there are corresponding asymptotically equivalent Wald and
score tests for this ML setting (see [[http://thestatsgeek.com/2014/02/08/wald-vs-likelihood-ratio-test/][a blog post]] on picking between these).

[[https://people.csail.mit.edu/xiuming/docs/tutorials/reml.pdf][REML]] tests are also available. [[https://rss.onlinelibrary.wiley.com/doi/10.2307/2988471][Lyon and Tsai 1996]] discuss tradeoffs.

For linear regression, REML works by analyzing \[Q^\top\mathbf{y}\], where \[Q\] is an
\[n\times (n-p)\] orthonormal basis orthogonal to \[X\] (and thus is the space supporting all
random residuals). It doesn't contain any useful information about \[\beta\],
which is the nuisance parameter here.

By transforming the data to remove all traces of the column space of \[X\],
the remaining data can be used for identifying a maximum likelihood estimate
of \[\sigma^2\]. This approach improves bias.

**** Methods Based on Replication

If we have muliptle observations for each covariate value, and assume
equal covariates have equal variances, then estimating variance directly
is possible.

Surprisingly, using the sample variance for each covariate group
results in an inefficient estimator.

It's better to use variances estimated from the residuals (of OLS) directly
per [[https://academic.oup.com/biomet/article-abstract/75/1/35/352132?redirectedFrom=fulltext][Carroll and Cline]]. Then you can iterate with WLS.

**** Variance is a Function of the Mean

If \[\mathrm{var}(\varepsilon_i)=w(\mathbf{z}_i,\boldsymbol\beta)\], then
a self-consistent estimator (one learned by iteration of WLS with \[w_i\]
fixed and then resetting \[w_i=w(\mathbf{z}_i,\boldsymbol\beta)\] as
a maximization-maximization procedure.

In the case of \[\mathbf{z}=\mathbf{x}\] and \[w(\mathbf{z}_i,\boldsymbol\beta)=\exp(\boldsymbol\beta^\top\mathbf{x})\]
this method has 100% ARE compared to WLS with known variances.

Alternatively, if we know that \[w\] is an arbitrary increasing function \[f\]
of the mean (not exponential), then we can transform the response
to have constant variances. See Section 10.4.3 for examples.

*** 10.5 Departures From Normality

Visually, can be diagnosed by a Q-Q plot of the residuals.
This is preferred over a histogram because it provides more
in-depth diagnosis: skew, heavy tails, and outliers are all immediately
visible with little tuning for, e.g., bin size.

**** Response Transformation

The Box-Cox transformation is a modification applied to the response.
It is parameterized by \[\lambda\] and transforms the response as follows:

\[y^{(\lambda)}=\begin{cases}
\frac{y^{\lambda}-1}{\lambda}&\lambda\neq 0\\
\log y& \lambda=0
\end{cases}\]

This is only applicable to positive response data. The John-Draper
transformations (and further, Yeo-Johnson) are more flexible.

The interpretation and fit is straightforward: conditioned on \[\mathbf{x}\],
the model is that \[y^{(\lambda)}\sim N(\mathbf{x}\cdot\boldsymbol\beta, \sigma^2)\].

Then the density can be derived by a change of variables and the
data is fit with a consistent set of estimators through maximum likelihood.

**** Transform Both Sides

The covariates may be transformed in similar ways as above,
with a corresponding interpretation that the transformed variables
yield a normal distribution in the response.

*** 10.6 Detecting and Dealing with outliers

As before, there is a differentiation in outliers which have large residuals
and outliers that have high leverage (corresponding to high \[\mathrm{MD}\], or
Mahalanobis distance).

The main corresponding challenge to detecting a _single_ outlier is
that high-leverage points may result in low residual fits, since it
skews the fit in favor of itself.

Multiple outliers (the last section) can conspire against you to
make this situation even worse. A compendium of tests can find
outliers in various forms.

**** Direct Residual Analysis

Externally studentized residuals \[\frac{e_i}{S(i)(1-h_i)^{1/2}}\], where \[S(i)\] is the residual
population standard deviation derived from the exclusion of the \[i\]-th point
from the dataset will detect low-leverage outliers.

We must turn to measuring influence in some way (which requires excluding
the point in question and re-fitting) to find other outliers.

**** Identifying High-leverage Points

High leverage points cannot be deduced from the original Mahalanobis distance,
since they impact the distance themselves. A single point can be evaluated
with re-fitting, but multiple points creates a challenge.

In general, consider a robust Mahalanobis distance defined by the
quadratic form

\[\mathrm{MCD}(\mathbf{x})^2=
(\mathbf{x}-T(X))^\top C(X)^{-1}(\mathbf{x}-T(X))
\]

where MCD stands for minimum covariance determinant and admits a fast
algorithm called Fast MCD.

[[https://wis.kuleuven.be/stat/robust/papers/2010/wire-mcd.pdf][MCD]] is not what the textbook here suggested, but it's the modern variant, with
\[T\] being a robust center estimate and \[C\] being a robust covariance estimate.

The linked work also describes choosing appropriate cutoff values.

**** Leave-one-out Case Diagnostics

Using rank-one updates we can predict the leave-one-out difference
in fitted coefficients:

\[\hat{\boldsymbol\beta}-\hat{\boldsymbol\beta}(i)=\frac{(X^\top X)^{-1}\mathbf{x}_ie_i}{1-h_i}\]

where \[e_i\] is the \[i\]-th residual. Note that this presents a vector of
changes for each data point.

Each term can be standardized by normalizing the residual; in particular
for the \[j\]-th component we would use

\[\frac{\hat{\boldsymbol\beta}-\hat{\boldsymbol\beta}(i)}{S(i)\left((X^\top X)^{-1}\right)_{jj}}\]

Rewriting the above, it can be expressed as \[\frac{c_j}{\|\mathbf{c}\|_2}\cdot \frac{t_i}{(1-h_i)^{1/2}}\]
where \[\mathbf{c}\] is the \[i\]-th column of the catcher matrix \[C=(X^\top X)^{-1} X\] and
\[t_i\] is the \[i\]-th internally studentized residual.

If we wanted to wing a cutoff from here, recal that \[\hat{\boldsymbol\beta}=CY\]
so a large ratio \[c_j/\|\mathbf{c}\|\] corresponds to a large influence
of \[Y_i\] on \[\hat\beta_j\]. Coupled with the \[t\]-distributed
term, which we rarely expect to be over 2, we can isolate
the standardized leave-one-out fitted term as having undue influence
if it's above \[2/\sqrt{n}\] (book's suggestion, kind of ignores hat matrix).

**** Change in Fitted Values

A similar analysis of \[\mathbf{x}_i^\top(\hat{\boldsymbol\beta}-\hat{\boldsymbol\beta(i)})\]
leads us to corresponding terms for influence on the \[i\]-th fitted
value by the \[i\]-th point, with a reasonable cutoff.

**** Covariance Ratio

Since the determinant of a matrix corresponds to the scale of the
volume of level sets of ellipsoids in the corresponding inner product space,
which in turn are probability masses, the determinant is an
appropriate tool for measuring differences in covariance matrices.

In particular, we have the covariance ratio, a ratio of determinants:

\[\frac{\left|S(i)^2(X^\top X)^{-1}\right|}{\left|S^2(X^\top X)^{-1}\right|}\].

Through a similar process as above, this is reduced to a function of studentized
residuals, yielding a cutoff of ratios further than \[3p/n\] from \[1\] as
conspicuous.

**** Tests for Outliers

(Note I skip a couple uninteresting outlier tests in my opinion).

The outlier test for \[k\] particular points can be evaluated in the OLS
setting with a shift model, i.e., the model

\[\mathbf{y}=X\boldsymbol\beta+\begin{pmatrix}
0\\
I_k
\end{pmatrix}\boldsymbol\gamma+\boldsymbol\varepsilon\]

where the null hypothesis is \[\boldsymbol\gamma=0\]. As expected,
this turns the individual Studentized residual tests (for single outliers)
alluded to above into an F-test for multiple outliers.

That said, this forms an interesting theoretical basis for extending tests
above to multiple outliers simultaneously (this is done explicitly in
the exercises).

**** Multiple Outliers

One simple approach is to extend the above leave-one-out approaches
to multiple outliers, but there's an explosion in the set of subsets
to consider as outliers, i.e., for \[d\] outliers, there
are \[\binom{n}{d}\] possible outlier sets to evaluate.

This is pretty lame, but there could possibly be manual approaches
which try to identify approximately which subsets could be outliers.

Perhaps some of the leave-one-out statistics mentioned above have a
leave-\[d\]-out analogue that is submodular, or admits a competitive
greedy solution (I'm just spitballing).

One suggestion from the book is to use a robust regression and
analyze the outliers of the robust regression directly. At this point,
though, why not just use the robust regression directly?

Pushing along the greedy direction, there are two essential failure modes
for multiple outliers conspiring against you, summarized in Figure 10.8.

In _masking_, multiple outliers near each other taint the leave-one-out
estimators because high-leverage points remain near the original one.

In _swamping_, non-outliers that are high-leverage can look artificially
like outliers, because other outliers move the fit "from where it would
have been" otherwise. In other words, if we removed the other outliers,
then the former high-leverage point would've fit into the regression.

From a greedy perspective, masking seems like a clustering/heierarchy issue:
dealing with similar outliers simultaneously would prevent masking.

Similarly, from a greedy perspective, swamping seems like a sequencing issue:
if removing outlier A makes outlier B no longer an outlier, then outlier B
is likely swamped by A. On the contrary, removing outlier B first
would make outlier A more of an outlier.

** 10.7 Diagnosing Collinearity


