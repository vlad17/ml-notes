* Linear Regression Analysis

** 3 Linear Regression: Estimation and Distribution Theory

*** 3.6

Orthogonal designs minimize the variance of your final coefficients (regardless of error distribution, only assuming spherical errors).

See Chapter 3, Part e, Exercise 3. Follows by Cramer's Rule.

Use orthogonal designs to find optimal experiments. See Chapter 3, Part e, Exercise 3.

*** 3.7

Augmenting linear regression models with additional covariates can be done through orthogonalization.

Basically, if you have \[Y\sim X\beta+\varepsilon\] with the OLS estimate,
let 
\[P=(X^\top X)^{-1}X^\top\] be the projection matrix and \[R=I-P\] be the
residual operator.

Then to fit the new model \[Y\sim X\beta+Z\gamma+\varepsilon\], you project out \[X\] from
both the signal and the new covariates:

\[\hat\gamma = MZ^\top RY=(ZR^\top RZ)^{-1}(RZ)^\top RY\]

where \[M=Z^\top RZ\] and we rely on symmetry and idempotence of \[R\]. Then the
OLS fit of \[\gamma\] in the augmented model, \[\hat\gamma\] is the OLS fit
of the projected out setting of residuals \[RY\] on covariates \[RZ\].

Then the adjusted \[\hat\beta\] in the augmented model excludes the variance
explained by \[\hat\gamma\], i.e., we use the OLS solution \[\hat\beta=P(Y-Z\hat\gamma)\],
where without the \[Z\hat\gamma\] term we have the plain old OLS for the smaller
model \[Y\sim X\beta+\varepsilon\]


*** 3.8 Estimation with Linear Restrictions

Given a least squares problem where we also know linear constraints that the output must satisfy, i.e., \[A\beta=c\],
then we can adjust the least squares solution using Lagrange multipliers to get

\[\hat\beta'=\hat\beta+(X^\top X)^{-1}A^\top(A(X^\top X)^{-1}A^\top)^{-1}(c-A\hat\beta)\]

where \[\hat\beta\] is the OLS estimate. An example use here is if you measure the angles of a triangle with the same
measuring tool.


*** 3.9 Design Matrix of Less than Full Rank

For less-than-full-rank designs \[X\], \[\beta\] is not identifiable (since distinct values can give rise to the same observations).

Restricting to linear subspaces by adding constraints on \[\beta\] can help, as can dropping columns.

With the rank less than full rank, we can still estimate linear some functions of \[\beta\] with linear
estimators. In particular, linear reductions
\[\mathbf{a}^\top \boldsymbol\beta\] are estimable iff \[\mathbf{a}\in\mathcal{C}(X^\top)\], since any linear estimator is of the form \[\mathbf{b}^\top Y\]
which is \[\mathbf{b}^\top X \boldsymbol\beta\] in expectation.


Interestingly, Question 4 of Exercises 3i then implies that 
\[\mathbf{a}\in\mathcal{C}(X^\top)\] iff \[\mathbf{a}\] is a unit eigenvector
of \[X^\top X (X^\top X)^{-}\] where \[A^{-}\] is any generalized matrix inverse.

*** 3.10 Generalized Least Squares

For a least-squares model defined by \[\mathbb{E}[Y]= X\boldsymbol\beta\] and \[\mathrm{var}(Y)= V\],
by applying the transformation \[V^{-1/2}\], which exists assuming
positive definiteness (else, after rotation, one of the \[Y\] values
is a constant), we recover a transformed OLS model.

The transformed OLS model has all our original BLUE (best linear unbiased estimator)
guarantees. The least squares estimate for the non-homoskedastic case
can be derived from this, it's just

\[\boldsymbol\beta^*=(X^\top V^{-1} X)^{-1}X^\top V^{-1} Y\]

*** 3.11 Centering and Scaling the Explanatory Variables

Centering and scaling explanatory variables doesn't affect the model RSS or any identifiability
properties, because the relationship with \[X\] and \[\hat\beta\] is one-to-one.

But centered variables (except the intercept) do result in \[\hat\beta_0=\overline{Y}\].

Similarly, centered and scaled variables \[X^*\] (i.e., each column has unit norm and the average of
its values should be 0, starting from the design \[X\]) have a simplified projection matrix form,
where the original model \[Y= X\boldsymbol\beta+\boldsymbol\varepsilon= X^*\boldsymbol\gamma+\boldsymbol\varepsilon\]
where \[\hat{\boldsymbol\gamma}=R^{-1}X^{*\top}Y\], where \[R\] is the correlation matrix of \[X\].


*** 3.12 Bayesian Estimation

Frequentist confidence intervals (based on Student's t) can be derived for an OLS model by using a uniform
(improper) prior for \[\boldsymbol\beta,\log\sigma\] and computing the marginal posterior over \[\boldsymbol\beta\]
by marginalizing \[\sigma\]. This is an interesting coincidence and a neat alternative interpretation.

A more informed prior is specified using a conjugate prior for computational ease,
starting with a Normal distribution and ending with one in the posterior (when conditioned on the scale)
for \[\boldsymbol\beta\] and keeping   \[\sigma^2\] inverse Gamma.

When you marginalize the scale, you recover a Student's t distribution over coefficients.
 

*** 3.13 Robust Regression

Different sensitivity measures help us evaluate properties of regression algorithms.
We'll use the fixed effects model where \[\mathbb{E}[Y]=X\boldsymbol\beta\] but will allow
varying kinds of errors and even interventions on some of the explanatory variables.

All methods will apply to residuals \[e_i(\boldsymbol\beta)=\mathbf{x}^\top\boldsymbol\beta-y_i\],
and may introduce and optimize over auxiliary nuisance parameters. First we review different sensitivity
measures.

 - response breakdown point :: the proportion of the response data that must be modified (by any amount) to result in arbitrary change in the learned coefficients.
 - explanatory breakdown point :: as above, but proportion of explanatory rows
 - influence curves :: consider the convex interpolation between a single pathological point and the empirical distribution, as the weight of the pathological point tends to zero what is its influence on the learned coefficients?

A complimentary metric to the above robustness measures is *asymptotic relative efficiency*,
under the OLS model with normal errors.

The below is a summary table of discussed estimators in the chapter.



| estimator         | IC                    | heavy tails | BP  |         ARE |
|-------------------+-----------------------+-------------+-----+-------------|
| OLS               | unbounded in both x,y | no          | 1/n |           1 |
| LAD               | bdd in y not x        | yes         | 1/n |         43% |
| Huber             | bdd in y not x        | yes         | 1/n | ~1 if tuned |
| LMS               | bdd                   | yes         | 1/2 |          0% |
| LTS               | bdd                   | yes         | 1/2 |          8% |
| S-estimator       | probably not in x?    | probably    | 1/2 |        ~29% |
| R-estimator       | bdd                   | yes         | 1/2 |          7% |
| Mallows, LQD, LTD | not bdd               | probably    | 1/2 |        >50% |


 - IC :: influence curve
 - BP :: break point
 - ARE :: asymptotic relative efficiency
 - OLS :: ordinary least squares \[\sum_ie_i^2\]
 - LAD :: least absolute deviation \[\sum_i\left|e_i\right|\]
 - Huber :: a specific M-estimate of \[\sum_i\rho(e_i/s)\] with \[\rho\] interpolating between squared and absolute error in and outside the unit ball.
 - LMS :: least median square \[\mathrm{median}_ie_i\]
 - LTS :: least trimmed squares \[\sum_{i=1}^h e_{(i)}\] where \[e_{(i)}\] are ordered (ascending) residuals, typically only lowest, \[n/2\] terms
 - GM :: generalized M-estimators and one-step GM estimators merged with LMS/LTS give best of both worlds under covariate distribution assumptions
 - S-estimators :: (biweight function in particular) solve obscure Huber-like polynomial
 - R-estimators :: only use order information, and need to use absolute residuals for guarnatees
 - Mallows, LQD, LTD :: various combinations of differences of residuals, tuned to not be affected by outliers

Moment equations \[\sum_i\psi(e_i/s)\mathbf{x}_i=0\] where \[\psi=\rho'\] and \[\sum_i\chi(e_i/s)=0\] for some scale function \[\chi\]
are generalizations of M-estimates that don't require a well-defined density to be defined.

Not discussed above is some pathological "instability" criterion (Fig. 3.2) that seems to only be
a worst-case scenario for LMS, LTS that isn't terribly realistic.

Overall, pairwise residual approaches allow optimizing high BP while maintaining ARE, but don't do
well from an IC perspective. This is consistent because IC is a "high-fidelity" measure of the impact
of a small deviation, but BP allow arbitrary differences and expect arbitrary changes, so are a crude
set of requirements.

It would be interesting to combine moderate trimming LTS with Huber to get heavy tail resistance
and bounded influence for x and y while maintaining ARE.

** 4 Hypothesis Testing

*** 4.1 Introduction

*** 4.2 Likelihood Ratio Test

Frame a hypothesis test comparing two nested models as assessing a wider linear regression and
its residual sum of squares, and then a constrained model that meets a linearity condition
\[A\boldsymbol\beta=\mathbf{c}\] (usually, \[A\] is partly an identity matrix and partly zero,
and \[\mathbf{c}\] is zero).

This can be fit with incremental OLS approaches (for the identity case) or Lagrange multipliers
from the last chapter.

*** 4.3 F-test

The test statistic of a likelihood ratio (the ratio of the fitted model's normal distributions
on the observed values) is not convenient, but equivalent to an F-statistic, which is
just a ratio of \[\chi^2\] variables via a transformation.

This can be expressed conveniently as a ratio of the residual sum of squares (RSS).
See Theorem 4.1 for an exposition under general linear constraints from [[4.2 Likelihood Ratio Test]].

From the exercises, the only neat computational observation is that if the saturated (full) model
has projection matrix \[P\] for its OLS fit and the reduced (nested) model has a projection \[P_H\]
which arises from the constrained OLS, then necessarily we have \[P_HP=PP_H=P_H\] with both matrices
idempotent.

[[4.5 Canonical Form of H]] discusses wider applications.

*** 4.4 Multiple Correlation Coefficient

Nothing terribly cool here, just that if \[R\] is correlation between \[Y,\hat Y\] then
by Thm. 4.2., \[R^2=1-(n-1)\mathrm{RSS}/S^2\].

Then for the general Thm. 4.1 setting \[F=\frac{R^2-R_H^2}{1-R^2}\cdot\frac{n-p}{q}\].

*** 4.5 Canonical form for H

\[F_{q, n-p}\] is the positive distribution of the ratios of independent \[\chi^2_q/q\] and
\[\chi^2/(n-p)\] variables.

It is equivalent to an [[4.2 Likelihood Ratio Test][LRT]] b/c for Gaussian errors the test functions are the same.

Note there isn't a multivariate or two-sided UMP test here, so when we design tests here we look
for weaker criteria like asymptotic minimax optimality.

Given a general model \[Y=X\boldsymbol\beta+\boldsymbol\varepsilon\], \[\boldsymbol\varepsilon\sim N(0, \sigma^2 I)\],
where \[\boldsymbol\beta\in\mathbb{R}^p\] and a nested hypothesis \[H:A\boldsymbol\beta=\mathbf{c}\],
over \[q\le p\le n\] non-redundant constraints, Theorem 4.1 describes how to set up the hypothesis test,
by leveraging idempotent, symmetric projection matrices \[P,P_H\] for the OLS and OLS-with-constraints
models, where they observe the nestedness \[PP_H=P_HP=P_H\].

Thus  \[\underbrace{\mathrm{RSS}_H}_{\chi^2_{n-p-q}}-\underbrace{\mathrm{RSS}}_{\chi^2_{n-p}}\] is itself an independent \[\chi^2_q\] rv.

Then the statistic \[F=\frac{\mathrm{RSS}_H-\mathrm{RSS}}{\mathrm{RSS}}\cdot \frac{n-p}{q}\] is \[F_{q, n-p}\]-distributed. We also have simplifications

\[\mathrm{RSS}_H-\mathrm{RSS}=\|\hat Y-\hat Y_H\|^2=(\hat{\boldsymbol\beta}-\hat{\boldsymbol\beta_H})^\top X^\top X(\hat{\boldsymbol\beta}-\hat{\boldsymbol\beta}_H)=(A\hat{\boldsymbol\beta}-\mathbf{c})^\top(A^\top (X^\top X)^{-1} A)^{-1}(A\hat{\boldsymbol\beta}-\mathbf{c})\]

and

\[\mathrm{RSS}=\|Y-\hat Y\|^2=Y^\top (I-P)Y\]

By canonicalization, for \[\mathbf{c}=\mathbf{0}\] and \[q<p\] for \[A=\left(\begin{matrix}A_1&A_2\end{matrix}\right)\]
where \[A_2\] is \[q\times q\] with rank \[q\], breaking up \[\boldsymbol\beta=\left(\begin{matrix}\boldsymbol\beta_1\\ \boldsymbol\beta_2\end{matrix}\right)\]
\[X=\left(\begin{matrix}X_1&X_2\end{matrix}\right)\] conformably, we can set \[\boldsymbol\beta_2=-A_2^{-1}A_1\boldsymbol\beta_1\].

This lets us define \[P_H\] directly as the RSS of the equivalent model under \[H\], \[Y=X_H\boldsymbol\gamma+\boldsymbol\varepsilon\],
where \[X_H=X_1-X_2A_2^{-1}A_1\]. Compare this to the method of orthogonal projections in Section 3.8.2 and
the Pythagorean law application of equation (3.44). Pretty neat matrix equality.

Examples 4.4-4.6 have some typical derivations of tests, especially for the common marginal hypothesis
case. Here's another.

**** Chapter 4, Part d, Exercise 2

\[n_i\] observations \[Y^{(i)}=W^{(i)}\boldsymbol\gamma^{(i)}+\boldsymbol\varepsilon^{(i)}\] for \[i\in[2]\]
and \[\boldsymbol\varepsilon=\begin{pmatrix}\boldsymbol\varepsilon^{(1)}&\boldsymbol\varepsilon^{(2)}\end{pmatrix}\sim N(0, \sigma^2 I)\]. Let's create a test that \[Y^{(i)}\] came from the same linear model.

\[H:\boldsymbol\gamma^{(1)}=\boldsymbol\gamma^{(2)}\]. Then the full model is
\[Y=\begin{pmatrix}Y^{(1)}&Y^{(2)}\end{pmatrix}=X\boldsymbol\gamma+\boldsymbol\varepsilon\] where \[\boldsymbol\gamma=\begin{pmatrix}\boldsymbol\gamma^{(1)}\\\boldsymbol\gamma^{(2)}\end{pmatrix}\] and \[X=\begin{pmatrix}W^{(1)}&0\\0&W^{(2)}\end{pmatrix}\].

But our constraint for \[H\] can be expressed as \[\begin{pmatrix}I&-I\end{pmatrix}\boldsymbol\gamma=\mathbf{0}\] so by
canonicalization \[X_H=\begin{pmatrix}W^{(1)}\\W^{(2)}\end{pmatrix}\]. Then our statistic based on projection
matrices from design matrices will satisfy the null distribution:

\[\frac{Y^\top(P_X-P_{X_H})Y}{Y^\top(I-P)Y}\cdot\frac{n_1+n_2-2p}{p}\sim F_{p, n_1+n_2-2p}\]


*** 4.6 Goodness-of-fit Test

This chapter presents a neat technique for repeat observations:
use tied normal means we can average across observations.

*** 4.7 F-test and Projection Matrices

Theorem 4.3 generalizes 4.1 to linearly dependent designs and constraints through
projections.

** 5 Confidence Intervals and Regions

**** 5.1 Simultaneous Interval Estimation

We are interested in estimating CIs for linear transformations \[\mathbf{a}_j^\top\boldsymbol\beta\].

We'd like to do this simultaneously for a set of linear transforms \[\mathcal{A}=\{\mathbf{a}_1,\cdots,\mathbf{a}_k\}\] where
the rank of this set is \[d\].

This applies to:
 - inference, when \[\mathbf{a}_j\] are standard basis vectors and \[d=k=p\]
 - mean prediction, when \[\mathbf{a}_j\] are standard basis vectors and \[k\gg d=p\]
 - other specialized hypotheses, which can have different dimensionality.

It's insufficient to simply use the t-intervals from the marginal distribution of \[\hat\beta_j\],
for instance, because the probability of at least one failure of \[(1-\alpha)\]-CI coverage among
\[k\] will look closer to \[k\alpha\] for small \[\alpha,k\].

Since \[\hat{\boldsymbol\beta}\sim N(\beta, \sigma^2(X^\top X)^{-1})\], given that we can estimate
\[\mathbb{E}S^2=\sigma^2\], \[A\hat{\boldsymbol\beta}\] follows a multivariate t-distribution, where
\[A^\top\] consists of column vectors from \[\mathcal{A}\].

Given this setup, we have several solutions for simultaneity:

 - Bonferonni :: Use \[\alpha/k\] CIs, appealing to union bound.
 - Maximum Modulus (MM) :: Useful computation accounting for \[\mathrm{cov}(\hat\beta_0,\hat\beta_1)\] for \[k=2\], based on max-of-t-distributions
 - S-method :: Bound simultaneous CI for \[\mathbf{h}^\top\boldsymbol\beta\] for all \[\mathbf{h}\in \mathcal{C}(A)\].

MM also applies to larger \[k\] for independent \[\mathbf{a}_j^\top\boldsymbol\beta\], but this
requires hypothesis testing on eigenvectors of \[X^\top X\], which is not really typical. One can still
use MM for non-independent t-distributions (the usual case), because the maximum decreases
as correlations increase.

S-method is really neat: it constructs an ellipse around \[\beta\] directly and bounds the
Rayleigh quotient of \[A\boldsymbol\beta\]. There's an important reduction from
\[\mathcal{A}\] to just its basis, as well.

The S-method's generality (full linear space CI coverage) makes it great for prediction settings
where we estimate CIs for \[k\gg p\], but it's not good for inference settings with \[d\approx k\],
where MM performs better (narrower CIs).

That said, it's unclear to me why you wouldn't want a Monte-Carlo like method to account for the
nonzero correlations between our linear transforms. This should in principle allow much narrower
CIs.

***** Monte Carlo Inference

Suppose we have a full rank OLS setup \[Y=X\boldsymbol\beta+\boldsymbol\varepsilon\] with
 \[\boldsymbol\varepsilon\sim N(\mathbf{0},\sigma^2 I)\], known variance for simplicity.

Say we're interested in the simultaneous CIs of \[A\boldsymbol\beta\], where wlog by the S-method
trick \[A\] has full crank.

In the case for known variance, why don't we use \[\hat{\boldsymbol\beta}-\boldsymbol\beta\sim N(0,\sigma^2(X^\top X)^{-1})\] directly?

Let \[D^{1/2}\] be the diagonal matrix containing the inverse standard deviation of each element
of \[A(\hat{\boldsymbol\beta}-\boldsymbol\beta)\].

Then consider \[\mathbf{z}=D^{-1/2}A(\hat{\boldsymbol\beta}-\boldsymbol\beta)\], which then follows
the known distribution \[N(\mathbf{0},D^{-1/2}A^\top (X^\top X)^{-1} AD^{-1/2})\].

We can compute the inverse survival function of \[\max_i\left|z_i\right|\] somewhat easily with monte carlo
by sampling normals and looking at the upper \[\alpha\] quantile; this gives the maximum of normals
correlated exactly how we expect \[A(\hat{\boldsymbol\beta}-\boldsymbol\beta)\] to be.

This yields a max-of-z-scores \[m_\alpha\]; then our CIs for \[\mathbf{a}_j^\top\boldsymbol\beta\]
are given by \[\mathbf{a}_j^\top\hat{\boldsymbol\beta}\pm D_{jj}^{1/2}m_\alpha\]

For unknown variance, I'm sure the above can be studentized in some way,
and it seems extensions should be possible to heterskedastic settings.

Perhaps this is very expensive, so it was not suggested in the book (which was, after all, published in 1977).

This would be essentially an extension of [[https://www.jstor.org/stable/1266931?seq=1][Hahn 1972]].


**** 5.2 Confidence Bands for the Regression Surface
**** 5.3 Prediction Intervals and Bands for the Response

For CIs for the response, rather than the mean of the response, we simply observe
that \[Y(\mathbf{x}_0)=\mathbf{x}_0^\top\boldsymbol\beta + \varepsilon\], so we need to increase
variance accordingly.

I wonder what we give up by using this fixed-X model for prediction: in the anti-causal setting
\[Y\rightarrow X\] the marginal distribution of \[X\] contains information about \[Y|X\] which
we're explicitly ignoring by using a fixed-X setup. This is, in principle, something ML methods
can take advantage of.

**** 5.4 Enlarging the Regression Matrix

*** 6 Straight-line Regression

** 6 Straight-line Regression

*** 6.1 The Straight Line

We have the two-variable model \[Y=\beta_0+\beta_1 x + \varepsilon\], 
which allows us to compute exact simultaneous CIs using correlated
t-distributions directly.

The x-intercept can be analyzed as the ratio of two correlated normal variables,
there exist both convenient or exact forms of CIs.

The Working-Hotelling confidence band can be applied to the entire line
to provide simultaneous CIs across the full range using the Scheffe's method.
This won't give straight lines, but is more narrow in applicable ranges (and
is narrowest at the point \[\overline{x},\overline{Y}\]).

An approach by [[https://www.jstor.org/stable/1269524?seq=1][Wynn and Bloomfield]] allows one to narrow the bands further for specific
ranges; this is a neat extension of the Scheffe method, and doesn't require
complex re-derivation for analyzing interval CIs like other mentioned approaches,
and it tends to be narrower anyway.

*** 6.2 Straight Line through the Origin

Of course, this simplifies the CI for the slope to a simple t-interval, but
the entire band across the full x range is now also defined by the same
t-interval by homogeneity.

*** 6.3 Weighted Least Squares for the Straight Line

With known weights, this reduces to the original model per the usual transform.

With unknown weights, one can use the MLE and rely on asymptotics or
solve with least squares and get more conservative intervals.

It seems strange to me to use unknown, mean-dependent weights. They don't talk
much about it or why one would really want this or be in this setting, but
[[https://www.jstor.org/stable/2983809?seq=1][Williams 1959]] is the reference.

From Exercises 1, Part b here, we get a setting for known weights:
gamma-distributed or poisson-distributed positive-only regression
(if we're regressing sums or counts, or instance, then it might make sense
 to scale variance by mean squared or mean, respectively).

*** 6.4 Comparing Straight Lines
Use a higher-dimensional normal means model to simplify a comparison
across multiple lines into the form of a linear test.

*** 6.5 Two-Phase Linear Regression

If a one-covariate linear regression undergoes a change of phase,
then this can be modelled directly by appropriately encoding the change of
phase directly into the model.

I.e., instead of 

\[Y=\begin{cases}\alpha_0+\alpha_1x+\varepsilon & x<\gamma\\\beta_0+\beta_1x+\varepsilon&x\ge \gamma\end{cases}\]

with a continuity condition that \[\alpha_0+\alpha_1\gamma=\beta_0+\beta_1\gamma\], one could instead
fit

\[Y=\varepsilon+\theta+\begin{cases}\beta_1 (x-\gamma) & x<\gamma\\\beta_2(x-\gamma) & x\ge \gamma\end{cases}\]

so that we don't have redundant parameters.

For a complete answer to the problem see [[https://onlinelibrary.wiley.com/doi/10.1002/0471725315.ch9][Seber and Wild 1989]], Chapter 9.3.

*** 6.6 Local Linear Regression 

** 7 Polynomial Regression
*** 7.1 Polynomials in One Variable

Naively applying least squares to the model \[y\sim \boldsymbol\beta^\top\mathrm{poly}(x)\]
where \[\mathrm{poly}(x)_i=x^i\] for single-variable polynomial regression
may work mathematically to reuse the same machinery but results in a Vandermonde
design which in turn has an ill-conditioned Hilbert \[X^\top X\] Gram matrix.

Since LS is not practically feasible, stable solutions are needed.
Indeed, the desired approach is to construct an orthogonal basis dynamically,
with \[\phi(x)\] replacing \[\mathrm{poly}\] above such that \[\phi(x)_i=\phi_i(x)\]
with \[\{\phi_i\}_i\] an orthogonal basis with respect to the norm defined by 
the covariates themselves. For our simple two-variable dataset \[\{x_i,y_i\}\],
define the product \[\langle f, g\rangle=\sum_if(x_i)g(x_i)\]. This can be dynamically constructed using
the recurrence

\[\phi_0=1\], \[\phi_1(x)=2(x-a_1)\], \[\phi_{r+1}(x)=2(x-a_{r+1})\phi_r(x)-b_r\phi_{r-1}(x)\]

Such a basis takes \[O(nk)\] time to construct for degree \[k\], since the polynomials can be defined
through their values at the points or coefficients. Moreover, the Gram matrix
\[\phi(\mathbf{x})^\top\phi(\mathbf{x})\] is diagonal, so the system can be efficiently
solved.

The Chebyshev basis can be used to represent \[\phi_k\], which comes with its own
set of recurrence relations in terms of Chebyshev coefficients in the text.
The primary win here is that you can perform inference outside of the training
set efficiently, too. See [[https://en.wikipedia.org/wiki/Clenshaw%E2%80%93Curtis_quadrature][Clenshaw 1960]].

To add constraints to the construction, see, e.g., [[https://ieeexplore.ieee.org/document/1099532][Payne 1970]], [[https://academic.oup.com/imamat/article-abstract/1/2/164/656295][Clenshaw and Hayes 1965]]

*** 7.2 Piecewise Polynomial Fitting

Regular polynomials may have very slowly decreasing RSS as degree increases
or systemic residuals.

This is especially the case when dealing with functions that are piecewise
varying across stages.

Splines of order \[M\] with \[K\] knots \[\boldsymbol\xi_k\] for \[k\in[K]\] are:

 - order \[M-1\] polynomials on each piece \[\xi_k, \xi_{k+1}\]
 - globally \[\mathcal{C}^{M-2}\]

I.e., continuity up to the last order at knots. Repeat knots lower breakpoint
continuity constraints.

For visual purposes, [[https://web.stanford.edu/~hastie/ElemStatLearn/][ESL - Splines]], \[M=4\] or cubic splines typically suffice.

While splines admit a parsimonious \[K+M\] parameter representation, it's
unstable to compute. A redundant \[K+2M\] representation allows for a stable,
recursive formula.

Various forms of regularization are possible, but we're essentially
entering ML territory, with methods becoming heuristic and reliance on CV
for seleciton.

*** 7.3 Polynomial Regression in Several Variables

This section considers extensions to both splines and polynomials
to higher dimensions, but the curse of dimensionality is not considered.

Approaches mentioned here are either tensor decomposition or exponential-in-dim
methods that naively extend lower-dimensional approaches.

** 8 Analysis of Variance

*** 8.1 Introduction

*** 8.2 One-way Classification
    
The main reason ANOVA works (more efficient test at whether all means for
separate populations are equal the full pairwise implication), i.e.,
can efficiently reject \[H_0:\mu_1=\mu_2=\cdots=\mu_n\] is framing in terms
of contrasts, or linear transformations of the means. Transforming the
problem into \[H_0:\mu_1-\mu_n=\mu_2-\mu_n=\cdots=0\], we can equivalently
ask if the vector of the above mean differences is uniformly 0.

This holds under all linear transformations being, so ScheffÃ©'s method is
appropriate.

Specializations for particular contrasts rather than all are available too.

Balanced designs help simplify the intervals, but don't afford significant
efficiency gains because of it (balanced means every group has same number
of participants).

However, per Miscellaneous Exercises 7, balanced designs DO get rid of
interaction effects, which is helpful.

The F-test assumptions are the same as OLS.

Balanced tests absolve (partly) normality and homogeneity requirements for
the F-test. That said [[https://www.jstor.org/stable/2332579][Welch's test]] directly allows heteroscedasticity,
which seems to be a set of approximations given the stochastically estimated
noise [[https://math.stackexchange.com/questions/1746329/proof-and-precise-formulation-of-welch-satterthwaite-equation][for sample variances]].

Heteroscedasticity checks (though they shouldn't be run to decide on
whether to use Welch's) can be done by looking at Levene's test, which
has a robust version based on median and LAD.

*** 8.3 Two-Way Classification (Unbalanced)

Two-way ANOVA can be equivalently modeled by an OLS setting
with indicators for two possibly multi-level factors (which can
all be dummy coded). The model includes interaction terms.

I.e., for \[I\] levels on the first and \[J\] levels on the second,
the design matrix here has \[1 + I + J + IJ\] terms, which is of course
overparameterized (there are \[IJ\] mean parameters and a single variance
parameter). The overparameterization can be dealt with with many different
ways, either by coding contrasts or using constraints. The constraint
is that all population means are the same across the same main effect.

The presence of interaction terms signifies that there is a "difference
in differences": there is no interaction (across any of the levels) when the
term \[(\mu_{i_1j_1}-\mu_{i_1j_2})-(\mu_{i_1j_2}-\mu_{i_2j_2})\] is uniformly
\[0\] across any two levels \[i_1,i_2\] of \[I\] and \[j_1,j_2\] of \[J\]. This set of constraints can, wlog,
be taken "relative to" base values \[i_1,j_1=1\] so it's actually just
\[(I-1)(J-1)\] independent constraints total.

This "difference in differences" is what gets tested for interaction coefficients.

Then there are several procedures when trying to fit a model in this setting:
first the grand mean (intercept) is tested against zero. Assuming this
succeeds, then there are several approaches.

Type I ANOVA. Test for significance of including the first factor in the model,
next the other factor, and finally interactions. This test has the advantage
of creating independent RSS tests (which we can see by the usual \[\chi^2\]
analysis of the residual OLS fit operators that are idempotent matrices \[(I-P)\]
for each fit.

Type II ANOVA. Two tests for main effects given other main effect and interactions.
Not independent, but symmetric. Final Type I ANOVA model depends on ordering.
Dubious, since the models compared here are not nested, but in principle
this is possible, if a factor is "activated" by another.

Type III ANOVA. Tests for significance of each of the two main effects and
the single interaction effect give the rest (the other main and the interaction, or
the two mains). Also not independent, but nested.

To be frank, using the above methods blindly seems cargo-culty. If you're coming
in with a hypothesis, then just test that hypothesis.

*** 8.4 Two-Way Classification (Balanced)

Math simplifies

*** 8.5 Two-Way Classification (One Observation per Mean)

Under-defined, b/c you have \[IJ\] observations but \[IJ + 1\] unknowns
(means for every cell in the 2-way table and the overall variance).

Usually, only an additive model is assumed here, since this setting
mostly appears for RCTs. Other richer interactions can be modelled with
lower-rank representations of the interaction matrix in multilevel settings.

*** 8.6 Higher-Way Classifications with Equal Numbers per Mean

Higher-order ANOVAs generalize the notion of differences in differences.

Below are more my notes for this topic, I think the recursive case is simpler
than going through 3-way and 4-way manually like this book does.

For a factor \[A\], and some response \[Y\], define the delta operator
\[\Delta_AY=\mathbb{E}[Y|A]-\mathbb{E}[Y|\lnot A]\]. In other words, the lift of \[A\]. This gives the coefficient
for \[A\] in a binary factor and is the effect tested for in one-way ANOVA.
(for multilevel, it's all relative to some base null factor value).

The delta \[\Delta_AY\] is itself a random variable, so we can ask
\[\Delta_B\Delta_AY=\mathbb{E}[\mathbb{E}[Y|A]-\mathbb{E}[Y|\lnot A]|B]-\mathbb{E}[\mathbb{E}[Y|A]-\mathbb{E}[Y|\lnot A]|\lnot B]\]
which boils down to \[(\mathbb{E}[\mathbb{E}[Y|A, B]-\mathbb{E}[\mathbb{E}[Y|A, \lnot B])-(\mathbb{E}[\mathbb{E}[Y|\lnot A, B]-\mathbb{E}[\mathbb{E}[Y|\lnot A, \lnot B])\], the
difference in differences. This is equal to \[\Delta_A\Delta_B Y\] (the
delta operator is commutative).

This, for OLS, also corresponds to the appropriate corresponding interaction
coefficient for \[A,B\].

Higher-order ANOVAs, over \[k\] factors \[F_1,\cdots,F_k\] are correspondingly linear models, which have
means of the recursive difference-in-differences \[\Delta_{F_1}\cdots\Delta_{F_k}Y\] as OLS parameters.

*** 8.8 Analysis of Covariance

Again, just reduce to the OLS model to come up with a hypothesis.
ANCOVA is when you do ANOVA but also throw continuous covariates in there
in addition to the factors.

Chapter 8 Part e Excercise 1 is interesting, because it can use the Theorem 3.6
partial fit result to perform regular ANOVA followed by OLS residual fitting
of the covariates.

** 9 Departures from Underlying Assumptions
*** 9.1 Introduction
*** 9.2 Bias

Underfitting occurs when you're missing covariates relative to the new model.

I.e., fitting \[Y\sim X\beta+\varepsilon\] but the true model is \[Y\sim X\beta + Z\gamma+\varepsilon\]

 - Fitted model becomes what you'd expect if you regress the missing covariates on the present ones
 - Bias term for the OLS predictor \[\hat\beta\] is \[(X^\top X)^{-1}X^\top Z\gamma\]
 - The variance estimator \[S^2\] becomes an overestimate
 - \[\mathrm{var}\hat\beta\] remains correct.

Overfitting occurs when additional regressors are added. This inflates the \[\mathrm{var}\hat\beta\],
but variance of noise and the estimate are unbiased.

The fact that during underfitting variance stays the same allows for creating a test for underfit,
see [[https://www.jstor.org/stable/2984219?seq=1][Ramsey 1989]].

*** 9.3 Incorrect Variance Matrix

In this scenario, homoskedasticity is violated but WLS is not used instead of OLS.

The coefficients remain unbiased but variance for both the coefficients and the noise
(and thus the fit) can become biased in either direction.

*** 9.4 Effect of Outliers

The previous section [[3.13 Robust Regression]] covered OLS alternatives
and evaluation measures. This section looks precisely on a single point's
effect on the OLS fit.

For a projection matrix \[P\], let the projected fit \[\hat Y = PY\] so that
\[\hat y_i=p_{ii}y_i+\sum_{j\neq i}p_{ij} y_j\]. Assuming centered covariates, per (3.53),
\[p_{ii}=n^{-1}+(n-1)^{-1}\|\mathbf{x}_i-\overline{\mathbf{x}}\|_{S_{xx}}^2\|\ge n^{-1}\], where \[S_{xx}\] is the sample
variance-covariance of the covariates and the corresponding norm
\[\|\mathbf{x}\|_A^2=\mathbf{x}^\top A^{-1}\mathbf{x}\].

Further, since \[P^2=P\] it's clear \[p_{ii}\le 1\] as \[\sum_{j}p_{ij}^2\ge p_ii^2\ge p_i\].

This gives \[p_{ii}\] in the range \[[n^{-1}, 1]\], where the Mahalanobis distance
defined by the gaussian \[N(\overline{\mathbf{x}}, S_{xx})\] determines
high-leverage points. Points low in the Gaussian density defined above
are high-leverage, having \[p_{ii}\approx 1\], and thus \[\partial_{y_i}\hat y_i\approx 1\]
so that high-leverage points have their fit more or less directly influenced
by the y value at that point.

*** 9.5 Robustness of the F-Test to Non-normality

The F-test can be used for both equality of variance and equality of means.

For variances, the test is very sensitive to non-normality, but not so for means.

How can this be the case for the same distribution?

This boils down to being really careful about what we mean about sensitivity.
It's not directly stated in the chapter, but the robustness referred to here is
really robustness not under the alternative, but under some "pretend null"
where, e.g., the variance is equal between two samples, but the error distribution
is some mean-0 non-normal distribution. Similarly, for the mean equality, the "pretend null"
is that the means are equal but the noise is some mean-0 non-normal distribution.

This chapter does not analyze some alternative hypothesis for these two cases; that's to
ill-defined.

However, in the above sense (that the test still works, namely has a reasonable Type-I rate,
under a pretend null that's not exactly normal but otherwise correct), the robustness
properties hold.

The reason boils down to the different projection matrices.

For equality of variance, our setup is that we have a sample of iid \[X_i\] of size \[f_1\]
and similarly for \[Y_i\] of size \[f_2\]. The \[F=S_X^2/S_Y^2\] is the ratio of two
\[\chi^2_{f_i}\]-distributed population variance estimators. Expressed under the standard OLS model,
this corresponds to a single global \[Y=\begin{pmatrix}X_1\\X_2\end{pmatrix}\beta+\varepsilon\],
where \[X_1=\begin{pmatrix}1_{f_1}&0\end{pmatrix}\] is a \[f_1\times 2\] matrix corresponding to the
\[X_i\] observations and similarly for \[X_2=\begin{pmatrix}0&1_{f_2}\end{pmatrix}\] of the \[Y_i\].
Then the corresponding projection matrices \[P_1,P_2\] give rise to the aforementioned F-statistic.
Interestingly, here we have \[P_1P_2=0\].

Then for means, our setup is similar, but instead the design matrix compares splitting means
to having the same one, so it is a \[f_1+f_2\times 3\] matrix of the form 
\[X_1=\begin{pmatrix}1_{f_1}&0\\ 0&1_{f_2}\end{pmatrix}\] under the alternative and
\[X_2=\begin{pmatrix}1_{f_1+f_2}&0\end{pmatrix}\] under the null. Note that now, each model is considered separately, whereas above
we relied on the same OLS model. Here, we have another \[F=\frac{Y^\top (P_1 - P_2) Y}{Y^\top P_1 Y}\] following
an \[F\] distribution, but here of degrees of freedom \[1,f_1+f_2 - 2\] (this test, unlike equality of
variances, can be naturally extended to having multiple groups of comparison with equal means).
Note that here \[(P_1-P_2)P_1\neq 0\].

The different projection matrix relationships give rise to different robustness properties.

For variance, [[https://www.jstor.org/stable/2983753?seq=1][Atiqullah 1962]] provides an analysis for F-statistics where projection matrices
satisfy \[P_1P_2=0\] and under the null variances are equal. Then a condition called quadratic
balance, met by balanced (and randomized, for other reasons) designs, yields robustness to
non-normality, as measured by excess kurtosis \[\gamma_2=\mathbb{E}\left[\left(\frac{Y-\mathbb{E} Y}{\sigma}\right)^4\right]-3\],
which is the fourth centered moment less the fourth centered moment of the Gaussian. In particular, we have
the following theorem (9.2): for symmetric, idempotent projection matrices \[P_i\] with vanishing product,
assume \[\mathbb{E}[Y^\top P_i Y]=\sigma^2 f_i\] and let the diagonal of \[P_i\] be \[\mathbf{p}_i\].
If the shared excess kurtosis of each variate is \[\gamma_2\], and each \[Y_i\] further has shared
variance, third, and fourth moments (but means can differ), then \[Z=\frac{1}{2}\log F\] is approximately normal,
corresponding to \[F=\frac{Y^\top P_1 Y / f_1}{Y^\top P_2 Y / f_2}\] being approximately F-distributed
with \[Z\] being independent of \[\gamma_2\] if \[f_1\mathbf{p}_2=f_2\mathbf{p}_1\] (quadratic balance).

For means, [[http://www.biostat.jhsph.edu/~iruczins/teaching/140.752/read/papers/box.1962.pdf][Box and Watson 1962]] provide an analysis based on the 4th cumulant bound where
the resulting F-statistic for the equality-of-means test above (but generalized to \[p\] groups,
checking equality of all their individual means under the null, behaves like an F-distributed
random variable, but with \[\delta (p - 1),\delta (f_1+f_2-1)\]. The shrink factor for degrees of freedom \[\delta\]
is defined by \[\delta^{-1}=1+C_x\Gamma_y/n\], where \[C_x,\Gamma_y\] are 4th-cumulant based bounds
that are each approximately 0 the closer the (multivariate) distribution of the covariates or
noise is to the normal.

*** 9.6 Effect of Random Explanatory Variables

**** Structural Law

The assumed OLS Model describes a stochastic (almost sure) structural law of the form
\[\mathbb{E}[Y|X]=\beta^\top X +\varepsilon\] where the noise is exogenous, i.e., \[\mathbb{E}[\varepsilon|X]=0\].

This corresponds to random covariates observed without error.

The book refers to a special case that's way too narrow with \[\varepsilon=0\] exactly. But the book does
describe the fact that if extra unrelated covariates are observed then we're not actually in trouble,
as OLS will naturally feature select. The "feature selection" is not like lasso here but rather due to
the CIs including 0. This seems like a silly way of dressing up the overfit discussion earlier; note that
just like in the fixed effects case the variance will increase if extra variables are included.

**** Functional Law

Here, the assumed OLS model describes a fixed equality derived from a law \[\mathbb{E}[y]=\beta^\top \mathbb{E}[\mathbf{x}]\]
(note no noise). Such relationships arise from random variables representing unbiased measurements of an underlying
physical phenomenon. This is a fixed effects model with \[y=\beta^\top \mathbb{E}[\mathbf{x}]+\varepsilon\].

Let \[\mathbf{u}=\mathbb{E}[\mathbf{x}]\] be the underlying covariates and write \[\mathbf{x}=\mathbf{u}+\Delta\] where by construction
\[\mathbb{E}[\Delta]=0\]. Let \[\mathbf{var} \Delta= D\], say. Suppose we make several observations and stack the design 
and outcomes as \[X,Y\] and proceed with naive OLS. Let \[U=\mathbb{E}[X]\] be the expected design.

Naive OLS yields \[\hat\beta_\Delta = (X^\top X)^{-1} X^\top Y\]. By Equation (9.30),
the bias \[\mathbf{b}\] is given by \[\mathbb{E}[\hat\beta_\Delta]=\beta-\mathbf{b}\] with \[mathbf{b}\approx \left(\frac{U^\top U}{n}+D\right)^{-1}D\beta\],
which is of course unhelpful since it requires oracle knowledge.

Interestingly, though, if we have an unbiased estimate \[\hat D\] of \[D\] then the book goes that
we can get an unbiased estimate \[\hat{\mathbf{b}}=n(X^\top X)^{-1}\hat D \beta\].

Going off-script a little (above is based on [[https://www.jstor.org/stable/2335377?seq=1][Davies and Hutton 1975]]), we note first that the bias estimator
above is useless as it still depends on \[\beta\] which is what we're estimating. But since everything is linear
and unbiased, note that \[\mathbb{E}\hat{\mathbf{b}}=\mathbf{b}\] so then \[\mathbb{E}[\hat\beta_\Delta]=\mathbb{E}[\beta+\hat{\mathbf{b}}]\]. Expanding our estimator, we have

\[\mathbb{E}[\hat\beta_\Delta]=\mathbb{E}[I\beta + n(X^\top X)^{-1}\hat D \beta]=\mathbb{E}[I + n(X^\top X)^{-1}\hat D] \beta\] by linearity.

Since \[\hat D, (X^\top X)^{-1}\] are both symmetric and PSD, so their product
 (though possibly not symmetric) has [[https://math.stackexchange.com/a/113859/38471][positive eigenvalues]],
and thus the matrix in the expectation on the RHS above is invertible.

Then moving the matrix to the other side, we have

\[\mathbb{E}[I + n(X^\top X)^{-1}\hat D]^{-1}\mathbb{E}[\hat\beta_\Delta]= \beta\]

which we unfortunately can't just push into one expectation due to nonlinearity.
See [[https://mathoverflow.net/a/307168][this MO answer]] for details. That said, it can be directly seen from the continuity of the matrix
inverse and continuous mapping theorem that the psuedo-debiased (it's not actually unbiased)
estimator \[\left(I + n(X^\top X)^{-1}\hat D\right)^{-1}\hat\beta_\Delta\] is consistent as an estimator of \[\beta\].

In fact, it's basically an order-one method of moments estimator. So I bet that analysis applies here. The interesting
part here is that matrix inverse is a known operation and we might be able to do better in estimating it than a generic
nonlinear equation like method of moments usually does. You still end up having to know \[\hat D\], but in principle this
is something you can estimate by repeatedly sampling the same "setting" \[U\]. I.e., if our experimental setup
lets us sample \[U\] repeatedly, then we'd get multiple \[X\] (after which the regression could in principle even
be carried out on the average \[X\] and the average \[Y\]).

**** Other Cases

The other cases don't seem too interesting. They include fixed, non-random errors (such as round-off) which
can be analyzed as noiseless versions of the [[Functional Law]] description and random covariates measured with error
(which is undetermined, as there are now two noises, one from the stochasticity of covariates and one from
their error, but only one set of residuals to estimate average error). The last case can be resolved
with assumptions about the relative sizes of each error source.

One approach that wasn't mentioned but would be interesting to look into would be [[https://en.wikipedia.org/wiki/Total_least_squares][Total least squares]].

[[https://en.wikipedia.org/wiki/Errors-in-variables_models][Error-in-variables models]] seems like a good search term here as well.

*** 9.7 Collinearity

Since \[\mathrm{var}\hat\beta=\sigma^2(X^\top X)^{-1}\], it's no surprise that rank deficiency
results in uncertainty.

It also affects robustness. This chapter looks at both.

**** Variance due to Covariate Collinearity

Suppose we have centered, scaled covariates, so that
\[X^\top X = \begin{pmatrix}n &0\\0&R_{xx}\end{pmatrix} = \begin{pmatrix}n &0 &0\\0 & 1 &\mathrm{r}^\top \\ 0 & \mathrm{r} & R_{22}\end{pmatrix}\]
where \[R_{xx}\] is the matrix of covariate correlations and the right hand
side is its block decomposition,such that \[r_i=\langle X_1, X_i\rangle \] for the vector \[\mathrm{r}\]
Then by linear algebra \[\mathrm{var}\hat\beta_1=\sigma^2(1-\mathrm{r}^\top R_{22}^{-1}\mathrm{r})^{-1}\].

This of course applies wlog to other covariates than the first.

We can also see that \[R_1^2=\mathrm{r}^\top R_{22}^{-1}\mathrm{r})^{-1}\] is itself the coefficient
of determination of regressing \[X_1\] on the rest of the columns of \[X\] by looking at the
RSS of that virtual regression.

More generically, we can define the variance inflation factor, for nonscaled
coefficients, which is \[\mathrm{VIF}_j=\frac{\mathrm{var}\hat\beta_j}{\sigma^2}=s_j^2(1-R_j^2)^{-1}=s_j^2(X^\top X)^{-1}_{(j+1)(j+1)}\]
where \[s_j^2\] is the (now non-unit) variance of the \[j\]-th coefficient, and we can see
Cramer's rule at play!

The book provides other approaches to variance analysis due to collinearity but
I did not find thme that illuminating.

**** Lower Robustness due to Covariate Collinearity

Suppose we have an OLS setup \[\mathbf{y}=X\beta+\varepsilon\] but we
replace \[X\] with \[X+\delta X\] and \[\mathbf{y}\] with \[\mathbf{y}+\delta \mathbf{y}\]
(where \[\delta X, \delta \mathbf{y}\] should be though of as single variables,
not products of variables. Further, these can be arbitrary perturbations,
so it's *not* necessarily the case that \[\delta \mathbf{y} = (\delta X) \beta\].

Per [[https://books.google.com/books/about/Accuracy_and_Stability_of_Numerical_Algo.html?id=7J52J4GrsJkC&source=kp_book_description][Higham 1996]] (page 392), if \[\|\delta X\|_2\le \epsilon \|X\|_2\] (by spectral norm)
\[\|\delta \mathbf{y}\|\le\epsilon\|\mathbf{y}\|\], and \[\kappa \epsilon <1\], where \[\kappa=\frac{\sigma_{\max}(X)}{\sigma_{\min}(X)}=\sqrt{\frac{\lambda_{\max}(X^\top X)}{\lambda_{\min}(X^\top X)}}\] is
the condition number of the design, then the resulting OLS applied to
the perturbed inputs gives an estimator \[\hat{\beta}_\epsilon\] whose relative error is bounded by

***** Forward Stability of OLS

\[\frac{\|\hat\beta-\hat{\beta}_\epsilon\|}{\|\hat\beta\|}\le\frac{\kappa\epsilon}{1-\kappa\epsilon}\left(2+(1+\kappa)\frac{\|X\hat{\beta}_\epsilon-\mathbf{y}\|}{\|X\|_2\|\hat\beta\|}\right)\]

** 10 Departures from Assumptions: Diagnosis and Remedies

*** 10.1 Introduction

There are a few common types of assumption mismatches with OLS. Consider
the stochastic OLS setup where \[Y=X\cdot\beta + \varepsilon\]

1. Nonlinearity of \[\mathbb{E}[Y|X]\]
2. Heteroscedasticity, where \[\mathrm{var}(Y|X)\] is non-constant.
3. Endogeneity \[\mathbb{E}[\varepsilon|X]\neq 0\]
4. Non-independence of errors (between training examples)
5. No pathological outliers

With (2) heterogeneity, but known variance, (diagonal) WLS applies.

With (4) error correlation, but the correlation between errors is known
and the errors are Gaussian, WLS applies.

When none of the above hold (we have linearity, homoskedasticity, exogeneity of error,
independent errors, and no corruption of the sample),
then the OLS estimator is unbiased, if inefficient in the non-Gaussian case.

*** 10.2 Residuals and Hat Matrix Diagonals

The hat matrix is the projection matrix \[P\]. In the OLS model, for residuals \[\mathbf{e}\]
that \[\mathrm{var}\mathbf{e}=\sigma^2(I-P)\]. Thus we can assess the
\[i\]-th residual under the null by inspecting the internally Studentized residual
\[\frac{e_i}{S(1-h_i)^{1/2}}\], where \[S\] is the residual population standard deviation
estimator and \[h_i=P_{ii}\].

Theorem 10.1 describes an extension based on using the external estimator \[S(i)\] which
uses all-but-the-\[i\]-th point to estimate residual variance, to support the
single-outlier adversarial scenario (5). It can be computed easily using a rank-one
update of the OLS fit.

*** 10.3 Dealing with Curvature

\[\mathrm{cov}(\mathbf{e}, \hat Y)=\sigma^2(I-P)P=0\], which can be promoted to independence under
normality. 

As a result, one can plot residuals against the sorted fitted values, and there should be no
visible correlation in the plot (or any fixed subset of the plot). Thus residual plots give
a strong visible mechanism for checking model fit.

Note that the overall correlation is forced (by the fit itself) to be 0.

This suggests a rudimentary automated mechanism for linearity testing against an alternative of
a piecewise smooth fit, which would be a permutation test of some sort: under a true linear model,
the maximum correlation of contiguous sub-blocks of \[\mathbf{e}\] with \[\hat Y\] for a given
block length is known. If a contiguous subsection of the plot has a strong correlation, that's indicative
of nonlinearity. I wonder how this performs against a more explicit test for such a spline model.
Other approaches are described in this [[https://stats.stackexchange.com/questions/405961/is-there-formal-test-of-non-linearity-in-linear-regression][Cross Validated]] thread.

Note that there are examples (the [[https://www.google.com/books/edition/_/k1esnwVBVSwC?hl=en&gbpv=0][Cook 2009]] work is referenced) where residual plots miss curvature
because they are lower dimensional.

Partial residual plots are proposed and argued against as lame.
Box-cox transforms and GAMs are called out but not in great detail.

One useful plot is the partial residual plot, which plots the residuals
of the regression with \[x_j\] removed versus \[x_j\]. This can be shown
to be linear precisely when the \[F_j\] statistic, for the corresponding
coefficient in the original regression, is large.

[[./exercises-10.3.ipynb][Python exercises]]

*** 10.4 Non-constant Variance and Serial Correlation

*NOTE, accounting for non-constant variance is not the same as accounting for non-independence.*

A branch of modeling (multi-level modeling, or mixed effects modeling, with the linear case
known as linear mixed models or LMM) deals with noise that violates independence assumptions
between data points.

For instance, suppose we are modeling blood pressure for individuals and we'd like to regress
dietary factors against blood pressure. If we take multiple measurements for individuals
on a given day, we could consider dietary factors as covariates, but would need to use
a random effects model to account for random skew to blood pressure individuals may introduce
(we can get more complicated with this example: we have possibly a full graphical model on our
hands, where we could consider time since the individual was awake as a latent variable, etc.).

Interestingly, REML fits are used for both heteroskedasticity and LMMs.

**** Parameterized Variance

One issue that may occur is that the per-datapoint variance
\[\mathrm{var}(\varepsilon_i)=w\left(\mathbf{z}_i,\boldsymbol{\lambda}\right)\] may be
a function of some other covariates \[\mathbf{z}_i\].

Define variance terms as the un-studentized residuals.

[[https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test][BreuschâPagan]] allows for arbitrary exp-linear models defined by covariates
\[\mathbf{z}\] for setting variance as the alternative for heteroskedasticity,
[[https://www3.nd.edu/~rwilliam/stats2/l25.pdf][Class notes]] describe BP, see also [[https://en.wikipedia.org/wiki/Levene%27s_test][Wikipedia]] lists others.

The gist is that the residuals \[\mathbf{e}\] after an OLS fit contain the
information necessary to assess heteroskedasticity because of their
relationship with the response variases through the hat matrix.

Intuitively, \[\mathrm{var}(\mathbf{e})=\mathrm{var}((I-P)\boldsymbol\varepsilon)\] which is known
since \[\mathrm{var}(\boldsymbol\varepsilon)\] is assumed diagonal for WLS. If \[\mathrm{var}(\varepsilon)=\sigma^2 I\] this lets
 us derive \[\mathrm{var}(e_i)=(1-p_{ii})\sigma^2+\sum_{k\neq i}\frac{p_{ik}}{1-p_{ii}}\sigma^2=\sigma^2\] by idempotence of \[I-P\].
But since \[P\] is known we could also check against a particular alternative \[\mathrm{var}(\boldsymbol\varepsilon)\].

More formally, we can consider a score-based method to evaluate whether or not variance is
constant against a parameterized alternative via score-based methods.

***** A Maximization-Maximization Approach

To make the hypothesis nested, suppose there exists a \[\boldsymbol\lambda_0\] such that
\[w(\mathbf{z},\boldsymbol\lambda_0)=\sigma^2\] is a constant for all \[\mathbf{z}\]. Then our null hypothesis of constant variance
becomes \[H_0:\boldsymbol\lambda=\boldsymbol\lambda_0\].

Algorithm 10.2 shows that OLS in this form can be fit with coordinate descent on the
maximum log-likelihood objective, which is the log likelihood of the data,
\[\ell(\boldsymbol\beta,\boldsymbol\lambda)=\sum_{i}\log p_{\boldsymbol\beta,\boldsymbol\lambda}(y_i, \mathbf{x}_i, \mathbf{z}_i)\]
with \[p_{\boldsymbol\beta,\boldsymbol\lambda}(y_i, \mathbf{x}_i, \mathbf{z}_i)\] the density under the model
\[y_i\sim N(\boldsymbol\beta^\top\mathbf{x}_i, w(\mathbf{z}_i,\boldsymbol\lambda))\]. The \[\boldsymbol\beta\] term can be fit conditionally on \[\boldsymbol\lambda\]
with WLS and then for fixed \[\boldsymbol\beta\], \[\boldsymbol\lambda\] can be found with some kind
of smooth optimality condition. Iterating this yields a maximization-maximization algorithm,
which finds a local optimum of the overall maximum likelihood objective.

Some useful algebra (equations 10.30, 10.32) show that in the general case, for some constant \[c\]

\[\ell(\boldsymbol\beta,\boldsymbol\lambda)=c-\frac{1}{2}\sum_{i=1}^n\log w_i+\frac{(y_i-\boldsymbol\beta^\top\mathbf{x}_i)^2}{w_i}\]

where above \[w_i=w(\mathbf{z}_i,\boldsymbol\lambda)>0\]. With \[\Sigma\] the diagonal matrix of entries
\[w_i^{-1}\], the WLS step is just a matter of computing \[\hat{\boldsymbol\beta}=(X^\top\Sigma^{-1}X)^{-1}X^\top\Sigma^{-1}\mathbf{y}\] (per
Section 3.10 Generalized Least Squares, Exercises 1b, (3) for motivation).

For fixed \[\boldsymbol\beta\], the \[\boldsymbol\lambda\] step is more interesting. For
continuously differentiable \[w\], first order necessary conditions (FONC) require

\[\mathbf{0}=\partial_{\boldsymbol\lambda}\ell(\boldsymbol\beta,\boldsymbol\lambda)=-\frac{1}{2}\sum_i\left(\frac{1}{w_i}-\frac{(y_i-\mathbf{x}_i^\top\boldsymbol\beta)^2}{w_i^2}\right)\partial_\lambda w_i\]

But this by itself only gets us to a local maximum, whereas since we're relying on
asymptotic maximum likelihood guarantees, we need a unique maximum for \[\ell\] (at least
asympototically).

This occurs under sufficient regularity, e.g., \[\ell\] being strictly concave.

Some more practical conditions will depend on \[p_{\boldsymbol\beta,\boldsymbol\lambda}\]
smoothness. In general, if \[\nabla_\theta,\mathbb{E}_\theta\] commute (where \[\theta=(\boldsymbol\beta,\boldsymbol\lambda)\]), then the Fisher
information is equal to the score covariance, and thus the Hessian of the log-likelihood
is negative semi-definite. This allows for second order sufficient conditions.
Sufficient conditions for the Fisher equality are,
for instance, continuity and integrability of the score \[\nabla_\theta p_\theta(y, \mathbf{x},\mathbf{z})\] with respect to \[\theta\]
(see [[https://en.wikipedia.org/wiki/Leibniz_integral_rule#Measure_theory_statement][Measure theory version of Leibniz's rule]]). This is handily
met by most well-behaved parameterized densities. Eventually, we'll need stronger
conditions for maximum likelihood asymptotic normality anyway, for which a weak
condition is Differentiability in Quadratic Mean (DQM). See [[https://web.stanford.edu/class/stats311/Lectures/lec-09.pdf][Fisher notes]] and [[https://www.stat.berkeley.edu/~bartlett/courses/2013spring-stat210b/notes/22notes.pdf][DQM notes]] for
details.

In any case, supposing the DQM condition, the FONC are sufficient for maximization,
because of implied concavity, so we just have to solve the above equation.

***** The Special Exp-Linear Variance Case

For the special case \[w(\mathbf{z},\boldsymbol\lambda)=\exp(\boldsymbol\lambda^\top\mathbf{z})\], we can do the above. Page 285 in the book
and Equation 10.34 summarize the updates. This derivation ends up being equivalent
to Fisher scoring, which directly optimizes \[\ell\] with a Newton-Raphson approximation
based on using the empirical score covariance \[\hat {\mathrm{var}}(\nabla_\theta p_\theta)\]
for the current iterate \[\theta\] as if it was the true Hessian (in fact it's just an unbiased
estimator for it). It's a descent direction (by positive semi-definiteness), so convergence is clear,
but rate of convergence is more complicated. See [[http://hua-zhou.github.io/teaching/biostatm280-2017spring/slides/18-newton/newton.html][these notes]] for extended discussion.

This, in turn, gives a direct asymptotic (likelihood ratio) test based on deviance, where
\[\mathrm{LR}=-2\left(\ell(\hat{\boldsymbol\beta}_{\mathrm{OLS}}, \mathbf{1}\hat\sigma^2)-\ell(\hat{\boldsymbol\beta},\hat{\boldsymbol\lambda})\right)\]. Of course, there are corresponding asymptotically equivalent Wald and
score tests for this ML setting (see [[http://thestatsgeek.com/2014/02/08/wald-vs-likelihood-ratio-test/][a blog post]] on picking between these).

[[https://people.csail.mit.edu/xiuming/docs/tutorials/reml.pdf][REML]] tests are also available. [[https://rss.onlinelibrary.wiley.com/doi/10.2307/2988471][Lyon and Tsai 1996]] discuss tradeoffs.

For linear regression, REML works by analyzing \[Q^\top\mathbf{y}\], where \[Q\] is an
\[n\times (n-p)\] orthonormal basis orthogonal to \[X\] (and thus is the space supporting all
random residuals). It doesn't contain any useful information about \[\beta\],
which is the nuisance parameter here.

By transforming the data to remove all traces of the column space of \[X\],
the remaining data can be used for identifying a maximum likelihood estimate
of \[\sigma^2\]. This approach improves bias.

**** Methods Based on Replication

If we have muliptle observations for each covariate value, and assume
equal covariates have equal variances, then estimating variance directly
is possible.

Surprisingly, using the sample variance for each covariate group
results in an inefficient estimator.

It's better to use variances estimated from the residuals (of OLS) directly
per [[https://academic.oup.com/biomet/article-abstract/75/1/35/352132?redirectedFrom=fulltext][Carroll and Cline]]. Then you can iterate with WLS.

**** Variance is a Function of the Mean

If \[\mathrm{var}(\varepsilon_i)=w(\mathbf{z}_i,\boldsymbol\beta)\], then
a self-consistent estimator (one learned by iteration of WLS with \[w_i\]
fixed and then resetting \[w_i=w(\mathbf{z}_i,\boldsymbol\beta)\] as
a maximization-maximization procedure.

In the case of \[\mathbf{z}=\mathbf{x}\] and \[w(\mathbf{z}_i,\boldsymbol\beta)=\exp(\boldsymbol\beta^\top\mathbf{x})\]
this method has 100% ARE compared to WLS with known variances.

Alternatively, if we know that \[w\] is an arbitrary increasing function \[f\]
of the mean (not exponential), then we can transform the response
to have constant variances. See Section 10.4.3 for examples.

*** 10.5 Departures From Normality

Visually, can be diagnosed by a Q-Q plot of the residuals.
This is preferred over a histogram because it provides more
in-depth diagnosis: skew, heavy tails, and outliers are all immediately
visible with little tuning for, e.g., bin size.

**** Response Transformation

The Box-Cox transformation is a modification applied to the response.
It is parameterized by \[\lambda\] and transforms the response as follows:

\[y^{(\lambda)}=\begin{cases}
\frac{y^{\lambda}-1}{\lambda}&\lambda\neq 0\\
\log y& \lambda=0
\end{cases}\]

This is only applicable to positive response data. The John-Draper
transformations (and further, Yeo-Johnson) are more flexible.

The interpretation and fit is straightforward: conditioned on \[\mathbf{x}\],
the model is that \[y^{(\lambda)}\sim N(\mathbf{x}\cdot\boldsymbol\beta, \sigma^2)\].

Then the density can be derived by a change of variables and the
data is fit with a consistent set of estimators through maximum likelihood.

**** Transform Both Sides

The covariates may be transformed in similar ways as above,
with a corresponding interpretation that the transformed variables
yield a normal distribution in the response.

*** 10.6 Detecting and Dealing with outliers

As before, there is a differentiation in outliers which have large residuals
and outliers that have high leverage (corresponding to high \[\mathrm{MD}\], or
Mahalanobis distance).

The main corresponding challenge to detecting a _single_ outlier is
that high-leverage points may result in low residual fits, since it
skews the fit in favor of itself.

Multiple outliers (the last section) can conspire against you to
make this situation even worse. A compendium of tests can find
outliers in various forms.

**** Direct Residual Analysis

Externally studentized residuals \[\frac{e_i}{S(i)(1-h_i)^{1/2}}\], where \[S(i)\] is the residual
population standard deviation derived from the exclusion of the \[i\]-th point
from the dataset will detect low-leverage outliers.

We must turn to measuring influence in some way (which requires excluding
the point in question and re-fitting) to find other outliers.

**** Identifying High-leverage Points

High leverage points cannot be deduced from the original Mahalanobis distance,
since they impact the distance themselves. A single point can be evaluated
with re-fitting, but multiple points creates a challenge.

In general, consider a robust Mahalanobis distance defined by the
quadratic form

\[\mathrm{MCD}(\mathbf{x})^2=
(\mathbf{x}-T(X))^\top C(X)^{-1}(\mathbf{x}-T(X))
\]

where MCD stands for minimum covariance determinant and admits a fast
algorithm called Fast MCD.

[[https://wis.kuleuven.be/stat/robust/papers/2010/wire-mcd.pdf][MCD]] is not what the textbook here suggested, but it's the modern variant, with
\[T\] being a robust center estimate and \[C\] being a robust covariance estimate.

The linked work also describes choosing appropriate cutoff values.

**** Leave-one-out Case Diagnostics

Using rank-one updates we can predict the leave-one-out difference
in fitted coefficients:

\[\hat{\boldsymbol\beta}-\hat{\boldsymbol\beta}(i)=\frac{(X^\top X)^{-1}\mathbf{x}_ie_i}{1-h_i}\]

where \[e_i\] is the \[i\]-th residual. Note that this presents a vector of
changes for each data point.

Each term can be standardized by normalizing the residual; in particular
for the \[j\]-th component we would use

\[\mathrm{DFBETAS}_j=\delta_j^\top\frac{\hat{\boldsymbol\beta}-\hat{\boldsymbol\beta}(i)}{S(i)\left((X^\top X)^{-1}\right)_{jj}}\]

Rewriting the above, it can be expressed as \[\frac{c_j}{\|\mathbf{c}\|_2}\cdot \frac{t_i}{(1-h_i)^{1/2}}\]
where \[\mathbf{c}\] is the \[i\]-th column of the catcher matrix \[C=(X^\top X)^{-1} X\] and
\[t_i\] is the \[i\]-th internally studentized residual.

If we wanted to wing a cutoff from here, recal that \[\hat{\boldsymbol\beta}=CY\]
so a large ratio \[c_j/\|\mathbf{c}\|\] corresponds to a large influence
of \[Y_i\] on \[\hat\beta_j\]. Coupled with the \[t\]-distributed
term, which we rarely expect to be over 2, we can isolate
the standardized leave-one-out fitted term as having undue influence
if it's above \[2/\sqrt{n}\] (book's suggestion, kind of ignores hat matrix).

**** Change in Fitted Values

A similar analysis of \[\mathbf{x}_i^\top(\hat{\boldsymbol\beta}-\hat{\boldsymbol\beta(i)})\]
leads us to corresponding terms for influence on the \[i\]-th fitted
value by the \[i\]-th point, with a reasonable cutoff.

**** Covariance Ratio

Since the determinant of a matrix corresponds to the scale of the
volume of level sets of ellipsoids in the corresponding inner product space,
which in turn are probability masses, the determinant is an
appropriate tool for measuring differences in covariance matrices.

In particular, we have the covariance ratio, a ratio of determinants:

\[\frac{\left|S(i)^2(X^\top X)^{-1}\right|}{\left|S^2(X^\top X)^{-1}\right|}\].

Through a similar process as above, this is reduced to a function of studentized
residuals, yielding a cutoff of ratios further than \[3p/n\] from \[1\] as
conspicuous.

**** Tests for Outliers

(Note I skip a couple uninteresting outlier tests in my opinion).

The outlier test for \[k\] particular points can be evaluated in the OLS
setting with a shift model, i.e., the model

\[\mathbf{y}=X\boldsymbol\beta+\begin{pmatrix}
0\\
I_k
\end{pmatrix}\boldsymbol\gamma+\boldsymbol\varepsilon\]

where the null hypothesis is \[\boldsymbol\gamma=0\]. As expected,
this turns the individual Studentized residual tests (for single outliers)
alluded to above into an F-test for multiple outliers.

That said, this forms an interesting theoretical basis for extending tests
above to multiple outliers simultaneously (this is done explicitly in
the exercises).

**** Multiple Outliers

One simple approach is to extend the above leave-one-out approaches
to multiple outliers, but there's an explosion in the set of subsets
to consider as outliers, i.e., for \[d\] outliers, there
are \[\binom{n}{d}\] possible outlier sets to evaluate.

This is pretty lame, but there could possibly be manual approaches
which try to identify approximately which subsets could be outliers.

Perhaps some of the leave-one-out statistics mentioned above have a
leave-\[d\]-out analogue that is submodular, or admits a competitive
greedy solution (I'm just spitballing).

One suggestion from the book is to use a robust regression and
analyze the outliers of the robust regression directly. At this point,
though, why not just use the robust regression directly?

Pushing along the greedy direction, there are two essential failure modes
for multiple outliers conspiring against you, summarized in Figure 10.8.

In _masking_, multiple outliers near each other taint the leave-one-out
estimators because high-leverage points remain near the original one.

In _swamping_, non-outliers that are high-leverage can look artificially
like outliers, because other outliers move the fit "from where it would
have been" otherwise. In other words, if we removed the other outliers,
then the former high-leverage point would've fit into the regression.

From a greedy perspective, masking seems like a clustering/heierarchy issue:
dealing with similar outliers simultaneously would prevent masking.

Similarly, from a greedy perspective, swamping seems like a sequencing issue:
if removing outlier A makes outlier B no longer an outlier, then outlier B
is likely swamped by A. On the contrary, removing outlier B first
would make outlier A more of an outlier.

*** 10.7 Diagnosing Collinearity

Recall the correlation matrix \[R\] of the design, equivalent to 
\[(X^*)^\top(X^*)\] where \[X^*\] is centered and scaled.

Near-collinearity was found to be detectable by small eigenvalues
or high \[\mathrm{VIF}\]s of \[R\].

However, the computation of equation 10.62 shows an example where
a particular column may have a large average value and small variation,
i.e., a small coefficient of variation \[\mathrm{CV}_j=\frac{s_j}{|\overline{x}_j| \sqrt{n}}\].

In this scenario, \[R\] is itself unstable, as is the hat matrix. To diagnose
such scenarios, we can use the scaled but not centered matrix \[\breve{X}\],
whose \[i\]-th column f\[\breve {\mathbf{x}}_i=\mathbf{x}_i/\|\mathbf{x}_i\|\].

**** Standardization, Centering, and Scaling

This brings us to an important point on *standardization*.

When should we center variables? Scale variables?

Discussed [[./standardization-10.7.ipynb][in this notebook]].

**** Remedies for Degeneracy

Centering can help conditioning as explored in the notebook above.

If a system is degenerate, it's a good time to ask whether inputs are
collinear for a reason. Perhaps one of the variables is really a linear
combination of the others in the underlying data-generating mechansim
and isn't useful to add to the regression.

Otherwise, consider collecting more data.

The linked notebook from the [[Standardization, Centering, and Scaling]]
section explores other remedies, including

 - removing problematic rows
 - removing problematic columns
 - ridge regression

** 11 Computational Algorithms for Fitting a Regression

*** 11.1 Introduction

There essentially two techniques to linear regression fits.

1. Compute the Gram \[X^\top X\], which is PSD, and solve the normal equation \[X^\top X \boldsymbol\beta = X^\top \mathbf{y}\].
2. Decompose \[X\] directly and solve a simpler equation than (1).

Where there are different types of approaches for (1) and (2). Approaches
should be assessed on the basis of memory, accuracy, and efficiency. Modern times
have changed priorities since this book was written: the number of flops
used by a particular decomposition isn't very relevant to speed: a smaller
memory footprint can often result in a faster algorithm due to better cache use.

Per [[Standardization, Centering, and Scaling]], it's advantageous to
standardize \[X\] prior to solving, and then back out the original
coefficients. The rest of this chapter will assume we already standardized,
and talk briefly about numerically stable standardization.

Notably, we're dealing with \[p\le n\] cases. We would still like to be robust
to degenerate designs, but this is a different set of large-scale regression
problems than what most people mean by large-scale linear regression, which
typically involves both \[p\] and \[n\] huge. For that, there's
[[https://github.com/VowpalWabbit/vowpal_wabbit][Vowpal Wabbit]], [[https://www.csie.ntu.edu.tw/~cjlin/liblinear/][LIBLINEAR]], [[https://github.com/linkedin/photon-ml][Photon ML]]. In those (typically first-order) settings,
variance estimates for coefficients are typically not well-posed.

Here, with \[p \ll n\], they are. For inference, we're interested in all
of the following:

 - \[\boldsymbol\beta\] coefficients
 - RSS \[S^2\]
 - variance terms \[\mathrm{diag}(X^\top X)^{-1}\]

And then for diagnostic purposes, we'd like

 - hat matrix diagonals \[\mathbf{h}=\mathrm{diag}(P)\]
 - fitted values \[\hat {\mathbf{y}}=P\mathbf{y}\]

This isn't an in-depth intro to numerical analysis, which is best left to
Higham's [[https://www.maths.manchester.ac.uk/~higham/asna/index.php][Accuracy and Stability of Numerical Algorithms, Second Edition]].
I'll reference it throughout.

*** 11.2 Direct Solution of the Normal Equations

Decompositions of \[X^\top X\] require less flops and dramatically less
memory than decompositions of \[X\] if \[p \ll n\], at the cost of essentially
squaring the condition number. I.e., at a rough cut, if the accuracy of
\[X\]-decomposition fits is \[u\] we'd expect the accuracy of normal
equation solutions to be the much larger \[\sqrt{u}\].

\[X^\top X\] is computed as \[\sum_i \mathbf{x}_i\mathbf{x}_i^\top\],
an easily parallel and distributed computation. The simplicity of splitting
up a sum of outer products across machines, which enables data partition,
makes normal equation approaches simple, attractive approaches at scale.

**** Gaussian Elimination (GE)

GE is equivalent to LU decomposition, which decomposes \[X^\top X=L U\]
for lower-triangular \[L\] and upper-triangular \[U\] by applying
lower-triagular elementary operations \[E_i\] on the left of \[X^\top X\],
which "zero out" each column below the diagonal, by subtracting out
a scaled version of the \[i\]-th row from all the rows below it.

Inductively, this leaves zeros in the lower triangle of what remains of
the original matrix.
In matrix form, \[E_i\] is the identity plus a bunch of scale factors
in the \[i\]-th column below its diagonal. Writing \[\prod_i E_i^{-1} (\prod_iE_i X^\top X)\],
we notice that since the inverse of lower triangular matrices is lower
triangular and such matrices are closed over products, the above is exactly
the \[LU\] decomposition.

Then a triangular system can be inverted in a quadratic number of operations
with back-solving (start with the row with one coefficient, "plug it in", etc.).

Conditioning can be improved with pivoting, which permutes columns of the
original Gram matrix such that the diagonal element used to subtract out
the corresponding row is as large as possible (row pivoting may be used too).

With this strategy, when there are no valid pivots (everything in the corresponding row
and column is small), we have detected degeneracy.
Note that this detection is itself subject to error, and while pivoting will
generally detect bad conditioning robustly, it is not reliable to actually estimate
the rank in all cases.

Pivoted GE is uninteresting for solving a single normal equation, since the
[[Cholesky Decomposition]] below is faster and just as stable, but a variant of
GE known as sweeping (which is just a symmetrized variant of GE, see
page 335, Section 11.2.1 and Section 11.6.2) computes the inverse of a matrix
in place.

The critical properties of sweeping are that it is

 - reverisble (two sweeps on the same row is a no-op)
 - commutative with other sweeps

The sweeping operation applied to the augmented matrix \[X_A^\top X_A\] where \[X_A=\begin{pmatrix}
X & \mathbf{y}\end{pmatrix}\]
on all but the last column we are left with the incomplete
inverse \[\begin{pmatrix}(X^\top X)^{-1} & \hat{\boldsymbol\beta}\\\hat{\boldsymbol\beta}^{\top} & \mathrm{RSS}\end{pmatrix}\].

The main use of this (which necessarily gives up pivoting and
its good conditioning properties) is to compute
the regression efficiently on all subsets of the regressors, or a
particular path of regressors, such as a forward-selected sweep.

**** Cholesky Decomposition

The Cholesky decomposition, described in (non-pivoting) Algorithm 11.2, is for positive
definite Grams the unique decomposition \[LL^\top =X^\top X\] for
the lower-triangular \[p\times p\] matrix \[L\].

When \[X^\top X\] is (near-)degenerate, it is PSD instead of PD and the
Cholesky decomposition is no longer unique since the columns of \[L\]
corresponding to the nullspace of the Gram can be determined in multiple
ways. Similar to GE pivoting, this is generally stable given low rank matrices
but not guaranteed. See [[https://software.intel.com/content/www/us/en/develop/documentation/mkl-developer-reference-fortran/top/lapack-routines/lapack-linear-equation-routines/lapack-linear-equation-computational-routines/matrix-factorization-lapack-computational-routines/pstrf.html][?pstrf]].

As before, the trick of decomposing the augmented matrix \[X_A^\top X_A\]
yields a Cholesky factor \[\begin{pmatrix} R & \mathbf{z}\\ \mathbf{0} & d \end{pmatrix}\] where matrix diagonals can be recovered
from \[(X^\top X)^{-1}=L^{-\top}L^{-1}\] (thus the diagonals are the squared
column norms of the lower-triangular \[L^{-1}\]) and \[d^2\] is the RSS. See [[http://www.netlib.org/lapack/explore-html/da/dba/group__double_o_t_h_e_rcomputational_ga97c5ddb103f5f6bc2dc060886aaf2ffc.html][?trtri]].

Unfortunately, with pivoting, the cool trick above doesn't work, since
the last column can get pivoted in to an internal column. In any case,
residuals can still be computed in the straightforward manner via the predictions
\[\hat{\mathbf{y}}=X\hat{\boldsymbol\beta}\]. The book also provides \[\mathrm{RSS}=\|\mathbf{y}\|^2_2-\|L^\top\hat{\boldsymbol\beta}\|_2^2\] (from \[X^\top X = LL^\top\])
which is curious because it seems like a cancellation error waiting to happen.

*** 11.3 QR Decomposition

Pivoted thin \[QR\] decomposition of the direct \[X\] matrix breaks \[X\] up
into an \[n\times r\] matrix of orthonormal columns \[Q\] and an upper-triangular
\[r\times p\] matrix \[R\], where \[r\] is the rank.

With such a decomposition, the normal equations become \[R\hat{\boldsymbol\beta}=Q^\top \mathbf{y}\].

Gram-Schmidt orthonormalization is the classical algorithm here, it's not stable.

Without pivoting, the QR decomposition of the augmented matrix \[X_A\] will
again automatically find the desired regression quantities for us automatically.
To be precise, the QR decomposition of \[X_A\] is \[\begin{pmatrix}Q & \mathbf{q}\end{pmatrix}\begin{pmatrix}R & \mathbf{r}\\\mathbf{0} & d\end{pmatrix}\]
where \[\boldsymbol\beta=R^{-1}\mathbf{r}\] and \[\mathrm{RSS}=d^2\].
However, with pivoting we can't use the same routine, as before, so we can't use
this nice trick in degenerate cases.

Unlike normal equation approaches, QR is significantly more stable, because
it doesn't square the matrix.

To actually compute the QR decomposition, see the following.

**** Householder Reflectors

Householder matrices \[H(\mathbf{u})=I-2\frac{\mathbf{u}\mathbf{u}^\top}{\|\mathbf{u}\|_2^2}\] have the reflection property: for
equal-norm vectors \[\mathbf{x},\mathbf{y}\], \[H(\mathbf x-\mathbf y)\mathbf x = \mathbf y\] (see equation 11.31). They are
involutions and isometries, since they just reflect across the plane
 defined by the normal vector \[\mathbf u\].

A Householder matrix can zero out all but one entry of a vector
\[\mathbf{x}\] by choice of \[\mathbf{y}\] such that \[y_1=\|\mathbf x\|\] and \[y_{j}=0\] for \[j>1\].

The only catch is when computing \[\mathbf x - \mathbf y\], the first
element \[x_1-\|\mathbf x\|\] is naively numerically unstable as difference
in possibly large positive terms, in which case \[\frac{x_1^2-\|\mathbf{x}\|^2}{x_1+\|\mathbf x\|}=\frac{\sum_{j>1}x_j^2}{x_1+\|\mathbf x\|}\]
can be used instead.

The \[k\]-th step of householder-based QR decomposition (without pivoting)
then uses the matrix \[\begin{pmatrix} I_{k-1} & 0\\0 & H(\mathbf{u}_k)\end{pmatrix}\], where
\[\mathbf{u}_k\] is chosen to zero out the \[k\]-th column of the remaining
matrix, whose first \[k-1\] columns are already upper-triangular:

\[H_k\cdots H_1X=X'=\begin{pmatrix}
R_{1:k,1:k} & R_{1:k,k':p}\\
0 & X_{k':n,k':p}'
\end{pmatrix}\], where \[k'=k+1\]

After \[p\] such transformations, the remaining matrix is just \[R\].

The collection of \[p\] length-\[n\] householder vectors implicitly
define \[Q\] (which is an \[n\times n\] matrix). The unique thin QR
from the first \[p\] columns can be extracted, but since we only need
the multiplication \[Q^\top \mathbf{y}\], we can use the Householder
matrices directly by reflecting \[\mathbf{y}\] at most \[p\] times,
which is cheaper than a full \[n\times n\] matrix multiplication
and without requiring the full \[n^2\] storage space (just \[np\]).

This can be achieved using the last column of the augmented matrix \[X_A\]
to which QR could be applied, or, if pivoting is used, then we get a
decomposition \[A\Pi = Q R\] where \[\Pi\] is a (unitary) permutation matrix
(for column pivoting, done for cases when the corresponding matrix norm
\[\|\mathbf x\|\] is small). Note \[Q=H_p\cdots H_1\] and by involution \[Q^\top=H_1\cdots H_p\]. This yields
\[\hat{\boldsymbol\beta}=\Pi R^{-1} Q^\top \mathbf{y}\], where the \[Q^\top\] is applied by operating on \[\mathbf{y}\] as described
above.

Fundamental property. How they zero out. How they build up to get decomp.

[[https://www.netlib.org/lapack/lug/node42.html][?geqp3]] is a level 3 BLAS algo which performs QR with pivoting (there's
a legacy level 2 BLAS one).

**** Other Methods

A similar approach is based on matrices called Givens rotators. It's slower
than Householder but this can be patched (with additional complexity).
MGSA (the Modified Gram-Schmidt Algorithm) can rectify Gram-Schmidt instability
but still results in numerically non-orthogonal \[Q\] matrices. This, too,
can be patched with additional complication.

All in all, these are interesting methods, but I think everyone just gets
by knowing just Householder. [[http://www.netlib.org/lapack/lawnspdf/lawn276.pdf][CARRQR]] is a modern implementation.

*** 11.4 Singular Value Decomposition

The SVD of an \[n\times p\] matrix \[X\] is a set of orthonormal matrices \[U, V\]
and positive diagonal matrix \[\Sigma\] such that \[X=U\Sigma V^\top\]. Where
\[U\] is \[n\times k_1\] and \[V\] is \[p\times k_2\] with

 - \[k_1=k_2=\mathrm{rank}(X)\] (compact SVD)
 - \[k_1=k_2=\min(n, p)\] (thin SVD)
 - \[k_1=n,k_2=p\] (fat SVD)

Where the fat SVD has a rectangular \[\Sigma\] but unitary \[U,V\].
Up to permutations (with diagonal values of \[\Sigma\] in descending order
\[\sigma_1\ge\cdots \ge\sigma_p\] being canonical), and up to unique singular values, the
thin SVD is unique, but if multiple singular values are the same any
any equivalent basis can be used.

Applying the decomposition to the normal equations gives us
straightforward expressions for \[\mathrm{RSS}=\|(I-U)Y\|_2^2\],
\[\hat{\boldsymbol\beta}=V\Sigma^{-1} U^\top \mathbf{y}\], \[P=UU^\top\],
referring to the thin SVD matrices.

For degenerate cases, using the compact SVD yields the least-norm solution.

At a high level, SVD is computed by bidiagonalizing the matrix with left and right
simultaneous householder transforms (left to get QR, right to remove the right part
of the upper-right triangle). Then the bidiagonal system can have eigenvalues
directly extracted. [[https://www.cs.cornell.edu/cv/Books/GVL/][Golub and Van Loan]] has details.

LAPACK implements this directly with [[https://software.intel.com/sites/products/documentation/doclib/mkl_sa/11/mkl_lapack_examples/_gelsd.htm][?gelsd]]. See also [[https://www.netlib.org/lapack/lug/node27.html][netlib]].

*** 11.5 Weighted Least Squares

**** Simple OLS Reduction
For WLS, we're interested in finding the best linear unbiased estimator
for coefficients when our generative model is given by \[y_i=\mathbf{x}_i\cdot \boldsymbol\beta + \varepsilon\]
where \[\varepsilon\sim N(0, \sigma^2 w_i^{-1})\] for some \[w_i>0\]. Clearly, the process can be transformed
by multiplying both sides by \[\sqrt{w_i}\] yielding \[y_i'=y_i\sqrt{w_i},\mathbf{x}_i'=\mathbf{x}_i\sqrt{w_i},\varepsilon'=\varepsilon\sqrt{w_i}\].

Suppose we have a full-rank least squares problem \[\min_{\boldsymbol\beta}\|X\boldsymbol\beta-\mathbf{y}\|_2^2\].

The corresponding WLS objective is \[(\mathbf{y}-X\boldsymbol\beta)^\top W(\mathbf{y}-X\boldsymbol\beta)\]. Where \[W\] was a
diagonal matrix with diagonal \[\mathbf{w}\]. Minimizing this yields the BLUE
and (which for normal noise is also the MLE). The normal equations would be
\[X^\top W X = X^\top W \mathbf{y}\].

The simple method is of course to use \[W^{1/2}\mathbf{y}\] and \[W^{1/2}X\] with one of the
previous OLS methods (either normal equation solve or direct-\[X\]). However,
for poorly-conditioned \[W\] the resulting OLS reduction can also be ill-conditioned.

[[https://epubs.siam.org/doi/book/10.1137/1.9781611971484][Bjork's Numerical Methods for Least Squares]] describes in Section 4.4
that with special care the ill-conditioning can be avoided because it is caused by 
a structured diagonal matrix. The section provides examples where
typical approaches (such as column-pivoted QR) presented above can fail.

**** Row-sorted QR

Intricate algorithmic fixes are possible, but give up usage of optimized routines
such as those found natively in LAPACK.

Instead, per Higham Section 19.4 and 20.8,
if we pre-emptively sort the rows in descending row-norm order (i.e., in the new system
\[X'=W^{1/2}X\], consider infinity row norms \[\sqrt{w_i}\|\mathbf{x}\|_\infty\], and sort the
rows of \[X'\] such that the largest rows are first), then we can recover a stable solution which
would only have dependence on \[\kappa(W^{1/2})\] rather than its square. See Theorem 20.7
in Higham or the original work Stability of Householder QR Factorization
for Weighted Least Squares Problems, by Cox and Higham 1997. This solution is appealing since
it means we can use LAPACK column-pivoted routines. Note that interestingly this row sort analysis
applies to unweighted OLS instances, the difference is that usually the numerical stability is
analyzed in terms of the operator norm of \[\|X\|\]. The difference is that this is usually assumed
to be controlled for regular OLS problems but for WLS we modify this norm by \[\kappa(W^{1/2})\].

Unfortunately, this does not help us if we'd like to use direct normal equation solve methods,
which is the case when \[X^\top X\] is much smaller than \[X\].

**** Two-weight Case

Section 4.4.4 of Bjork provides a much more preferable solution for special \[W\] matrices
that contain only two distinct values (which come up when solving constrained least squares)
by updating an unweighted solution. This uses a direct solve via the Woodbury matrix inversion
formula, which can take advantage of the block structure of \[W=\begin{pmatrix}
w_1I & 0\\
0 & w_2 I
\end{pmatrix}\] in the normal equation \[X^\top W X\] term.
This only relies on \[p\times p\] updates and can take advantage of a pre-factored \[X^\top X\]
matrix. Unfortunately, this is unhelpful in the general WLS case.

**** Cholesky and Iterative Refinement

When our hands are tied due to the need to use normal equation solves (say, \[X\] is distributed),
then we'd want an alternative.

In this case, we may have to call in other methods, namely iterative refinement, which is a much
more generic technique ([[https://en.wikipedia.org/wiki/Iterative_refinement][Wikipedia]], [[https://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec13.pdf][Lecture Notes]], [[https://en.wikiversity.org/wiki/Numerical_Analysis/Iterative_Refinement][Wikiversity]], Higham Chapter 12). The idea is
simple. If we have a numerically unstable solution \[\tilde{ \mathbf{x}}\] to a linear system
\[A\mathbf{x}=\mathbf{b}\] (such as the normal equation for weighted least squares above
with \[A = X^\top W X,\mathbf{b}=X^\top W \mathbf{b}\]), then solving for the residual
\[A\boldsymbol\Delta = (\mathbf{b}-A\tilde {\mathbf{x}})\] can yield an improved solution \[\tilde{\mathbf{x}}+\boldsymbol\Delta\].
This process can be iterated.

How many times to iterate and whether iterative refinement will help is well-studied in the above
links.

In iteratively reweighted least squares, IRLS, used to fit generalized linear models (GLMs),
this inner-loop WLS procedure is common. E.g., see [[https://github.com/apache/spark/blob/3fdfce3120f307147244e5eaf46d61419a723d50/mllib/src/main/scala/org/apache/spark/ml/optim/IterativelyReweightedLeastSquares.scala][Apache Spark's]] implementation, which
just recomputes the weighted Gram \[X^\top W X\] in preparation for a weighted solve.


*** 11.6 Adding and Deleting Cases and Variables

Normal equation-based methods (1) shine here in their simplicity
(but continue to be worse from an accuracy perspective). However,
by the nature of their up-front \[X^\top X\] computation, you can't add 
new columns, just new cases.

Adding new cases (adding rows to \[X\]) is most interesting for the
QR algorithm, since it points us to incremental and distributed versions
of the algorithm.

While adding and removing variables is interesting for fast forward/backward selection,
being able to incrementally add rows to a regression is most useful for dealing with
out-of-memory compute.

If the full data matrix is not representable in memory then incremental methods can
allow dealing with part of the data frame at a time.

**** GE 

As mentioned above, GE shines here due to its ability to add an remove rows
with the sweep operation in \[O(np)\] time. The only downside is poor conditioning.

**** Cholesky

Cholesky has a similar situation, with updates being possible
for both rows and columns. [[https://en.wikipedia.org/wiki/Cholesky_decomposition][See wikipedia]].

Of course, you need to know the inner product of the new column with all previous columns,
which may be a very large matrix, to do the update.

**** QR

[[http://eprints.ma.man.ac.uk/1192/1/qrupdating_12nov][QR can be incrementalized as well]], and is the usual go-to for incremental OLS,
with some pivoting possible in principle (see Bjork Section 3.5).

**** SVD

See Bjork 3.4. Not as easy to stably incrementalize SVD as QR.

**** Distributed methods

While incremental is nice, for large data sets that require incrementality in the
first place we'd like to run independent parallel programs whose results could
be combined to give a fit.

Unfortunately, there do not seem to be many distributed QR or SVD approaches
(though of course Gram-matrix approaches can compute \[X^\top W X\] in parallel
easily).

Most naive distributed SVD or QR approaches square the matrix, at which point
the Cholesky approach is preferred anyway, since the stability we look to
direct-\[X\] methods is no longer there. See for instance, 
[[http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html][Spark]] and [[https://mahout.apache.org/docs/latest/algorithms/linear-algebra/d-qr.html][Mahout]]. 

Actual direct distributed QR looks interesting, using many smaller QRs: [[https://arxiv.org/abs/1301.1071][TSQR]].
Except for some randomized approaches, distributed QR looks like more or less
of a dead end after that, because of communication optimality
for existing [[http://www.netlib.org/lapack/lawnspdf/lawn204.pdf][dense methods, CAQR]].

*** 11.7 Centering the Data

For computing variances or Gram matrices \[X^\top X\], if \[X\] is to be centered
first, then several approaches exist to computing the \[ij\]-th centered Gram entry,
which is essentially asking to compute \[c_{ij}=n\mathrm{cov}(\mathbf{x}_i,\mathbf{x}_j)\]
between two vectors \[\mathbf{a}=\mathbf{x}_i,\mathbf{b}=\mathbf{x}_j\].

The textbook algorithm uses the computational formula \[c_{ij}=\sum_{k}a_kb_k-\frac{1}{n}\sum_ka_k\sum_kb_k\].

The updating/streaming algorithm uses a set of equivalent recurrence relations,
see [[https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Covariance][Wikipedia]].

The two-pass algorithm computes the means \[\overline{a},\overline{b}\] first,
then finds \[c_{ij}=\sum_k(a_k-\overline{a})(b_k-\overline{b})\].

The relative error, up to ulp, from numeric calculation of \[c_{ij}\] depends on the condition
of the columns, which is defined \[\kappa=1+\mathrm{CV}^{-2}\] in terms of the coefficient
of variation (so roughly mean square divided by variance).

Table 11.1 shows the relative error of each method. Note that the
Linear Regression Analysis textbook has an error in equation 11.76, which indicates that
\[C(n, \kappa) u\] is the relative error bound. This is incorrect (I checked the original work).
The error bound should be just \[C(n, \kappa)\], which in all cases is \[O(u)\] where \[u\] is an
ulp. The methods differ in their dependence on the condition number.

Textbook is awful, with relative error \[n\kappa^2 u\]. Updating is OK, at \[n\kappa u\],
but two-pass is best, at \[nu+(n\kappa u)^2\]. Using pairwise summation (where we do a tree-reduce
style sum of terms) drops floating point error from a factor of \[n\] to \[\log n\] in 
all algorithms.

Using [[https://en.wikipedia.org/wiki/Kahan_summation_algorithm][Kahan summation]] drops the factor of \[n\] to a factor of \[nu\].

*** 11.8 Comparing Methods

I included parts of this section in discussion above. One interesting discussion
point is how accuracy results are proven, which tells us about what those proofs mean.

These proofs are made with respect to implementation based on the [[https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html][IEEE754 standard]].
In general, all basic floating point operations are guaranteed to be accurate to within
a relative error of an ulp, a "unit in the last place", parameterized by \[u\].
It is about \[10^{-16}\] on doubles.

Typically accuracy proofs on floating point algorithms are made in two steps.

1. A backwards stability property is established, showing that the solution
   \[\tilde{\boldsymbol\beta}\] using floating point to an OLS problem \[X, \mathbf{y}\]
   is the true OLS solution to a perturbed problem \[X+\delta X,\mathbf{y}+\delta\mathbf{y}\].
2. A forward stability property is established, showing that the solution to a
   perturbed problem \[X+\delta X, \mathbf{y}+\delta\mathbf{y}\] has distance
   from the unperturbed solution bounded by the perturbation sizes.

Then combining the two properties we can bound \[\frac{\|\tilde {\boldsymbol\beta}-\hat {\boldsymbol\beta}\|_2}{\|\hat {\boldsymbol\beta}\|_2}\]
We've previously shown the [[Forward Stability of OLS]], a purely mathematical
property, which adds a \[\kappa(X)\] scaling constant. However, there are
usually two steps: for Gram-style methods we need to first verify the
stability of the Gram computation, and for direct-\[X\] decompositions
the stability of the decomposition must be analyzed, too.

The solve of the Gram \[X^\top X\] introduces a corresponding \[\kappa(X^\top x)=\kappa(X)^2\]
constant during the normal equation solve, thus explaining roughly why
approaches which "square the design" are less stable.

The Weilandt-Hoffman property shows strong forward stability for the SVD, i.e.,
singular value differences for \[X+\delta X\] from \[X\] are bounded by \[O(\|\delta X\|_2)\].
While \[U,V\] may be unstable for less-well-separated singular values, in practice
this does not matter much (if I can shoot from the hip, since for exactly equivalent
singular values the SVD can be reparameterized anyway, so as far as matrix applications
are concerned the instability occurs "in places where it doesn't matter as much").

This is both over simplifying in the sense of hiding terms, but also these
are very conservative analyses, so they're more qualitative guides to compare 
the methods than practical stability descriptions.

*** 11.9 The Rank-Deficient Case

Rank deficiency is not as black-and-white as it sounds. Matrices can
be nearly rank-deficient and as a result have poor fits. It's important
to be robust against such scenarios and cut off columns that induce collinearity.

**** Pivoting and Degeneracy

LU (GE), Cholesky, and QR approaches can all be modified with partial (column)
or full (row and column) pivoting to generally be robust against degenerate
matrices. See Higham 8.5 and 9.12.

Pivoting doesn't change the original problem because we can apply the same
(inverse) permutations to the resulting solution or response.

For a resilient solution, one should use the SVD.
See [[https://people.maths.ox.ac.uk/trefethen/text.html][Trefethen and Bau's Numerical Linear Algebra]]. The SVD will naturally
find the minimum-norm solution, and stably at that.

Multiple times above we were unable to use the augmented matrix \[X_A\] because
pivoting routines can destroy ordering, whereas it's important that the response
is the last column.

A note on pivoting. Note that Gram pivoting procedures will identify decompositions
that are permutation-similar, e.g., \[P^\top X^\top X P= LL^\top\] for a permutation \[P\].
In LAPACK, this permutation \[P\] is returned as an array of integer indices \[\mathbf{p}\]
such that the nonzero elements of \[P\] are its \[p_ii\]-th entries. 

QR will return \[X = Q R P^\top\]. In practice, it's important to keep in mind that
the permutation matrix is unitary, so \[P^{-1}=P^\top\], and can be inverted by
initializing an array \[\mathbf{q}\] with the \[p_i\]-th entry set to \[i\] in linear time.
Note that right-multiplication by a permutation matrix "selects columns" from the matrix on the left,
and that left-multiplication "selects rows" from the matrix on the right. However, the "selection" is
made by columns of the permutation matrix when right-multiplying but rows when left-multiplying.

Put more concretely, if \[\Pi_1,\Pi_2\] are permutation matrices such that the \[j\]-th column
of \[\Pi_i\] has its unique nonzero in the \[p_j^{(i)}\]-th row, (so \[P\] would correspond to
\[\mathbf{p}\] from before and \[P^{-1}\] to \[\mathbf{q}\]), then for a conformable matrix \[A\], the result
\[\Pi_1 A \Pi_2\] can be expressed with numpy indexing as any of the following,
where \[\mathbf{q}^{(i)}\] is the inverse permutation of \[\mathbf{p}^{(i)}\]:

\[A[:, \mathbf{p}^{(2)}][\mathbf{q}^{(1)}, :]\]
\[A[\mathrm{np.ix\_}(\mathbf{q}^{(1)}, \mathbf{p}^{(2)})]\]
\[A[:, \mathbf{p}^{(2)}].T[:,\mathbf{p}^{(1)}].T\]

Suppose we're interested in only
retaining the \[r\] "most orthogonal" columns of \[X\].
With a compact SVD \[X=U\Sigma V^\top\] (so that for numerical rank \[r\],
determined by cutting off singular values, \[U\] is \[n\times r\],
\[\Sigma\] is \[r\times r\], and \[V\] is \[p\times r\]), singular values \[\sigma_j\], and column vectors \[\mathbf{u}_i,\mathbf{v}_j\] of \[U,V\],
the \[k\]-th column of \[X\] is \[\sum_i \sigma_iv_{ki}\mathbf{u}_i\]. Since \[U\] is
orthonormal this really amounts to choosing an \[r\]-column submatrix of
the \[r\times p\] matrix \[\Sigma V^\top\] with the smallest condition number.
Unfortunately, this seems like a [[https://mathoverflow.net/questions/104803/optimizing-the-condition-number][tough problem in general]], but a viable
approach is just to use RRQR (rank-revealing QR, which has multiple variants,
including [[https://math.berkeley.edu/~mgu/MA273/Strong_RRQR.pdf][QR with pivoting]]). Nonetheless, the SVD's singular value stability
could be used to reliably diagnose rank deficiency in the worst case.

*** 11.10 Computing the Hat Matrix Diagonals

Recall that for a projection matrix \[P=X(X^\top X)^{-1}X^\top\] its diagonals
\[\mathrm{diag} P\], whose \[i\]-th entry is denoted \[h_i\] are useful for diagnostics
(though note that centering may obscure such diagnostics; see the references of
the [[Standardization, Centering, and Scaling]] discussion above).

Given factorizations, it's a matter of some algebra to derive, e.g., 
given a Cholesky factor \[L\], \[h_i=\|L^{-1}\mathbf{x}_i\|_2^2\]. For thin QR,
\[P=QQ^\top\], which is expensive to generate completely, but observing that the
\[h_i\] is then the square norm of the \[i\]-th row of \[Q_p\], which with
Householder does require generating such rows, which still requires \[O(np)\]
per row if generated on the fly (e.g., in a distributed manner), else \[O(np^2)\]
if generating the full matrix in a high-memory, non-distributed manner.

*** 11.11 Calculating Test Statistics

These are largely calculated by plugging into the formula.

The more interesting case is the variant for constrained 
regression, in which case the incremental update formula derived
from optimizing the Lagrangian on Equation 3.38 simplifies
when decompositions are known (see p382).

*** 11.12 Robust Regression Calculations

**** L1 Regression

L1 Regression is a linear program, so such packages can be applied.

**** M-estimation

M and GM can in some cases (GLMs) be solved iteratively via iteratively reweighted
least squares, [[https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares][IRLS]], which uses WLS as a subroutine. See [[http://bwlewis.github.io/GLM/][this site]] for IRLS intro.

This amounts to approximate Newton iteration, namely Fisher scoring,
discussed in a [[The Special Exp-Linear Variance Case][previous section]]. Fisher scoring is exactly Newton iteration
for canonical links.

Of course, regular Newton-Raphson works too.

**** High-breakdown methods

For cases like LTS/LMS, fitting methods are very slow. Practical approaches
start with randomized subsets of the data that correspond to the active
residuals in the LTS/LMS objectives and then greedily improve those subsets.

Interestingly, no mention of using MCD directly. I wonder what would happen
if we used \[\boldsymbol\beta = C(X)^{-1}(X^\top -\mathbf{1}T(X)^\top)\mathbf{y}\]
where \[C\] is a stable covariance estimator and \[T\] is a stable center estimator.

** 12 Prediction and Model Selection

*** 12.1 Introduction

Picking among different OLS models amounts to selecting covariates to
include in the regression. How to choose a model among a set of candidates
differs depending on one's objective.

Inference tasks assume some true underlying statistical model, say for a large
\[K\] that \[Y=\beta_0+\beta_1X_1+\cdots+\beta_KX_K\] as a structural law,
but there's only some small subset \[S_0\subset[K]\] of coefficients
\[\beta_i\] which are nonzero for \[i\in S_0\]. In inference settings, the goal is to find
\[S_0\] and possibly confidence intervals for \[\beta_{S_0}\], as a result
typical metrics are the false discovery rate (proportion of null \[\beta_i\] in
the selected set \[\hat S\]), the recall / true positive rate / sensitivity
(proportion of \[S_0\] which are discovered to be nonzero), and then among
the discovered true hypotheses, what the rate of coverage for confidence
intervals might be.

Conversely, a much more agnostic setting where no underlying true model needs
to be assumed is that of minimizing predictive error (PE) among a candidate
set of models. Here, only the conditional structure \[Y|X\] is assumed
to be consistent between training (observed) and test (application-time) data,
with independence between the two sets. Interestingly, for least squares,
within-dataset points merely need no correlation, not independence.

The final section (not from the book) [[Open Questions]] discusses notions
of agnosticity in the inferential setting.

Both of the above objectives can be tackled from different approaches:

 - All possible regressions (APR) investigates all \[2^K\] subsets of regressors.
 - Greedy routines perform some form of local search for the subset.
 - Shrinkage and Bayesian approaches introduce new structural assumptions.

*** 12.2 Why Select?

Why not use all variables? How is using less covariates ever helpful?

In machine learning, notions such as regularization, the bias-variance tradeoff,
stability, and complexity explain the broader phenomenon of generalization. For
OLS, we can directly analyze these terms in closed form.

**** Agnostic Bias-Variance

Suppose we train on \[X, \mathbf{y}\] where \[X\] is \[n\times K\] yielding
\[\hat{\boldsymbol\beta}\] as the OLS. The true data-generating mechanism need not be linear.
Consider predicting at \[X_0\], an \[m\times K\] matrix with corresponding \[\mathbf{y}_0\]
such that \[\mathbf{y}_0,\mathbf{y}\] are independent and
\[\mathrm{var}(\mathbf{y})=\sigma^2 I_n\] with \[\mathrm{var}(\mathbf{y}_0)=\sigma^2 I_m\].

As usual \[\hat{\boldsymbol\beta}=(X^\top X)^{-1}X^\top \mathbf{y}\] so \[\hat{\boldsymbol\beta},\mathbf{y}_0\]
are independent as well. Then

\[\mathbf{E}[\mathrm{PE}]=\mathbf{E}\|\mathbf{y}_0-X_0\hat{\boldsymbol\beta}\|_2^2=
\mathbb{E}\underbrace{\|\boldsymbol \mu_0-\mathbf{y}_0\|_2^2}_{\text{Bayes Error}\ \mathrm{BE}}
+ \mathbb{E}\underbrace{\|\boldsymbol \mu_0-X_0\hat{\boldsymbol\beta}\|_2^2}_{\text{Model Error}\ \mathrm{ME}}\]

where \[\boldsymbol\mu=\mathbb{E}\mathbf{y}\], \[\boldsymbol\mu_0=\mathbb{E}\mathbf{y}_0\], \[\mathbb{E}[\mathrm{BE}]=m\sigma^2\]. For \[\mathrm{ME}\], define \[\mathbf{y}=\boldsymbol\mu+\boldsymbol\varepsilon\] and

\[\mathrm{ME}=\|\boldsymbol\mu_0-P\mathbf{y}\|_2^2=\|(I-P)\boldsymbol\mu_0-P\boldsymbol\varepsilon\|_2^2
=\boldsymbol\mu_0^\top(I-P)\boldsymbol\mu_0-\boldsymbol\varepsilon^\top P\boldsymbol\varepsilon\]

Where the last equation holds because cross terms vanish from idempotence \[P^2=P\].
Also by idempotence, \[(I-P)^2=I-P\] and \[\mathrm{tr}(P)=\mathrm{rank}(P)=K\]. Then

\[\mathbb{E}[\mathrm{ME}]=\mathbb{E}[\boldsymbol\mu_0^\top(I-P)^2\boldsymbol\mu_0]-\mathbb{E}[\boldsymbol\varepsilon^\top P\boldsymbol\varepsilon]
=\mathbb{E}\|\boldsymbol\mu_0 - P\boldsymbol\mu_0\|_2^2-\mathrm{tr}(\mathrm{var}(\boldsymbol\varepsilon) P)
=\underbrace{\mathbb{E}\|\boldsymbol\mu_0 - \mathbb{E}[X_0\hat{\boldsymbol\beta}]\|_2^2}_{\mathrm{bias}^2}-\underbrace{\sigma^2 K}_{\mathrm{variance}}\]

Where we rely on the quadratic forms from chapter 1 for the trace simplification.
So altogether \[\mathbb{E}[\mathrm{PE}]\] is Bayes error, bias squared, and variance.

Recall the agnostic conditions under which the above holds. In fact,
the decomposition above can be applied to the analysis of [[https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff][any learner]]
(we just won't have the exact variance in analytic form).

Since OLS is unbiased for exogenous-noise settings where \[\mathbb{E}[\boldsymbol\varepsilon|X]=\mathbf{0}\],
as we add covariates then \[\mathrm{bias}^2\rightarrow 0\] as we include more and
more of \[S_0\]. On the other hand, the variance will increase linearly as more covariates are added.

So even with unlimited data, there's a bias-variance tradeoff penalizing prediction
error if superfluous covariates (with \[\beta_i=0\]) are added.

The book goes on to derive an explicit example under which using
a subset of regressors will always have smaller prediction error than
using the full set (but it's not a tight bound, since it's independent
of the covariate distribution). Nonetheless, equation 12.12 provides a useful
example for testing.

*** 12.3 Choosing the Best Subset

Choosing the best subset of regressors can be done by a variety of estimators.

The most direct approach is to estimate \[\mathrm{PE}\] directly and use that
as the criterion. More indirect approaches evaluate goodness-of-fit.

Which approach is better depends on the setting you're in, and will be discussed
in the last section.

Many estimates below rely on direct variance estimation; see 
[[http://www.stat.rutgers.edu/home/ldicker/papers/variance.pdf][modern least squares]] and [[https://arxiv.org/pdf/1311.5274.pdf][Lasso]] approaches.

**** Direct PE Estimates

Of course, choosing the model with the optimal PE results in estimates for
the PE which will be no longer valid for a randomly-chosen subset of
regressors. These estimates only hold for fixed subsets.

***** Mallow's \[C_p\]

For a \[p\le K\] variable model in a \[K\]-variable OLS setup,
the Mallows \[C_p=\frac{\mathrm{RSS}_p}{\hat{sigma}^2}+2p-n\] where
the variance estimator is from the full model \[\hat{\sigma}^2=\frac{\mathrm{RSS}_K}{n-K-1}\]

This provides a scaled biased estimate \[\mathrm{ME}/\sigma^2\], with
bias from \[\hat\sigma^2\approx\sigma^2\].

So minimizing the Mallow's \[C_p\] corresponds to minimizing model error,
subject to variance estimator bias. Up to some approximation, when \[n\gg p\],
it is equivalent to [[adjr][the adjusted coefficient of determination]].

***** \[\mathrm{CV}(1)\] <<cv1>>

Leave-one-out cross validation, or LOOCV, provides a slight overestimate of PE
by estimating prediction error having left out a single point from the
training dataset.

Based on rank-one update formulas, we can find the OLS solution with
a single data point removed given the original OLS problem \[X,\mathbf{y}\] 
with \[\hat{\boldsymbol\beta}(i)=\hat{\boldsymbol\beta}-\frac{(X^\top X)^{-1}\mathbf{x}_i^\top(y_i-\hat{\boldsymbol\beta}^\top\mathbf{x}_i)}{1-h_i}\] where \[h_i=P_{ii}=[X(X^\top X)^{-1}X^\top]_{ii}\]
is the hat matrix diagonal. Then we can simplify the leave-one-out residual
greatly, with \[y_i-\hat{\boldsymbol\beta}(i)^\top \mathbf{x}_i = \frac{y_i-\mathbf{x}_i^\top\hat{\boldsymbol\beta}}{1-h_i}\].

This yields the estimate \[\mathrm{CV}(1)=\frac{1}{n}\sum_i\left(\frac{y_i-\mathbf{x}_i^\top\hat{\boldsymbol\beta}}{1-h_i}\right)^2\].

This can be shown to over-estimate "resampled" prediction error. That is,
if for the same \[X\] we observed an iid \[\mathbf{y}'\], the 
expected PE on that new resampled dataset would be stochastically smaller
than \[\mathrm{CV}(1)\] by Equations 12.24, 12.26, 12.27.

***** \[\mathrm{CV}(d)\]

\[\mathrm{CV}(d)\] is defined as the average over all leave-\[d\]-out subsets,
so it requires \[\binom{n}{d}\] terms.

Stochastically approximating this sum by sampling random sets, or shuffling
the data is commonly used.

How to choose \[d\] has been a very heavily discussed topic. If estimates of
CV were independent, then LOOCV would be preferable to K-fold CV (which is
the approximation of \[\mathrm{CV}(n/K)\] using a shuffle of the data and
separate sets of training/test data for the folds) because it evaluates
more data points.

Estimates are correlated, however, and in special cases K-fold can beat LOOCV
for small \[K\]: [[https://stats.stackexchange.com/a/357749/37308][SO discussion]] points to relevant literature. A strong example
from that thread by user Paul:

#+BEGIN_QUOTE
here's a case where LOOCV fails: consider n data points and an interpolating
polynomial of degree n. Now double the number of data points by adding a
duplicate right on each existing point. LOOCV says the error is zero. You
need to lower the folds to get any useful info.
#+END_QUOTE

Interestingly, the above discussion does not touch on more sophisiticated
CV with larger CV sets. Though computationally intensive, results show
that for the \[d\sim n\] case [[https://projecteuclid.org/euclid.aos/1176349027][CV(d)]] may be more accurate at estimating PE.

**** Goodness-of-fit Estimates

These estimators have information-theoretic derivations. PE is "opinionated"
in that it weights mean square error. Depending on the context,
weighing error by log-probability may be more appropriate, penalizing the
model in accordance with its beliefs.

For the normal model that OLS uses, in practice this does not result in
significant deviation from the PE estimates.

***** Adjusted \[R^2\] <<adjr>>

Consider a model with \[p\le K\] variables from the \[K\]-variable OLS
setup with normal noise. Then, whereas the \[R^2=1-\frac{\mathrm{RSS}_p}{\mathrm{SSY}}\]
where \[\mathrm{SSY}=\sum_i(Y_i-\overline{Y})^2\] provides the coefficient
of determination, which scales with the F-test of the null (intercept-only) model
against the \[p\]-variable one for fixed \[p\], \[R^2\] increases in \[p\]
whereas the F-test does not necessarily.

This gives rise to the adjusted \[\overline{R}_p^2=1-\frac{\mathrm{RSS}_p/(n-p)}{\mathrm{SSY}/n}\], the proportion of estimated
variance explained.

For cases where \[n\gg p\], selecting the maximum adjusted \[\overline{R}^2_p\]
is approximately equal to minimal Mallows \[C_p\]. See page 402.

***** AC

KL divergence \[D_{\mathrm{KL}}(f\|g)\] /from/ \[g\] /to/ \[f\]
estimates the discrepancy of using the proposal density \[g\]
to approximate the true density \[f\].

This discrepancy is the average difference in bits under an optimal
encoding when using the incorrect distribution \[g\] to transmit random
messages which are actually distributed according to \[f\].

While natural to think of bits for probability mass functions, this
notion of KL discrepancy is less well founded for continuous random
variables, which require an infinite number of bits to represent.
For this reason, we rely on differential entropy, which is the limit of the
remainder term for entropy when [[https://en.wikipedia.org/wiki/Limiting_density_of_discrete_points][naively discretizing]] a continuous
random variable.

For a generic measure space, as long as absolute continuity \[P \ll Q\] holds,
then \[D_{\mathrm{KL}}(P\|Q)=\int \mathrm{d}P \log \frac{\mathrm{d}P}{\mathrm{d}Q}\].
Useful properties are that \[D_{\mathrm{KL}}\ge 0\] with equality holding only for \[P=Q\],
the fact that it is not symmetric (and therefore not a distance). Inspecting
its functional form, we can see that forward KL \[\max_QD_{\mathrm{KL}}(P\|Q)\] is mode /covering/, in
that the measurement with respect to \[P\] demands that \[Q\] "covers"
modes of \[P\] well. So a bimodal \[P\] and unimodal \[Q\] will result in
an optimum split down the middle. Conversely, reverse KL \[\max_QD_{\mathrm{KL}}(Q\|P)\]
is mode /seeking/, in that the choice of where to integrate allows the
optimal \[Q\] to focus on just one region. A bimodal \[P\] and unimodal \[Q\]
with reverse KL will result in an optimum choosing the largest mode.
Absolute continuity is the easiest way to think about it---the second
KL argument [[https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/][forces zeros]] in the first.

For arbitrary true density \[f\] and parameterized \[g(\cdot; \theta)\],
consider minimizing \[\min_\theta D_{\mathrm{KL}}(f\|g(\cdot; \theta))\]
on the predictive distribution.

Given iid data \[Z,Z'\sim f\], we may consider fitting \[\hat\theta(Z)\] with
one piece of data and evaluate divergence with the other; factoring out the
constant term we may be interested in the criterion
\[\Delta = \mathbb{E}_f[-\log g(Z'; \hat\theta(Z))|Z]\], a random variable equal to the KL discrepancy up to a 
constant shift (which happens to be the differential entropy \[H(Z)\]), where
of course \[\mathbb{E}[\Delta]=\mathbb{E}[-\log g(Z';\hat\theta(Z))]\].

An estimate of \[2\mathbb{E}[\Delta]\] is the \[\mathrm{AIC}=-2\log g(Z; \hat\theta(Z))+2r\]
where \[r=\dim\theta\] (note the reuse of \[Z\]).

For OLS we can derive that the AIC is consistent by relying on the fact
that the MLE for variance will be consistent with an \[O(n^{-1})\] bias; the
parameter count enters through model variance as derived above.

***** BIC

Consider a prior \[\alpha_S\] for subset \[S\] of our \[K\] regressors
with a corresponding normal prior over the parameters \[\beta_S\].

The log posterior for the model corresponding to \[S\] is, by Equation
12.42 and lots of integrals, \[\frac{-\mathrm{RSS}_S}{2\sigma^2}-\frac{1}{2}p\log n+O(1)\],
where the \[O(1)\] comes from the prior terms.

Thus, regardless of prior, so long as it's nonzero initially on all subsets
and coefficients, asympotically in \[n\] any Bayesian approach chooses
accodring to minimizing the \[\mathrm{BIC}=
\frac{\mathrm{RSS}}{2\sigma^2}+p\log n\].

It's interesting that it penalizes parameters more than AIC. The implications
of this are discussed later.

*** 12.4 Stepwise Methods

Thresholds \[p_{\mathrm{IN}}, p_{\mathrm{OUT}}\] below are usually chosen to be fixed.

[[https://www.jstor.org/stable/1268153][Berk 1978]] shows an example where greedy stepwise methods perform
aribtrarily poorly compared to APR.

**** Forward Selection

Consider the incremental \[F\]-test evaluating a nested null model over \[p\] variables
over a larger alternative over \[p+1\]. 

Starting from \[p=0\] (intercept is always implicitly included), consider
evaluating every such \[F\]-test for all variables currently excluded from
the model. Include only the variable with the smallest p-value if its p-value
falls below some threshold \[p_{\mathrm{IN}}\].

Then repeat the procedure with the larger model, incrementally adding variables,
until no proposed new variable exceeds \[p_{\mathrm{IN}}\].

**** Backward Selection

Using the same incremental \[F\]-test, we start with full model over \[p\]
variables and consider many nested nulls under \[p-1\] variables. The 
test with the largest p-value, if it exceeds \[p_{\mathrm{OUT}}\], is used
to remove the corresponding variable. Then the procedure is repeated with the
smaller model.

This continues until no test exceeds the threshold.

**** Stepwise Regression

Stepwise regression starts with a null model and alternates between a forward
step, adding a variable, and a backward step, removing one.

As long as \[p_{\mathrm{IN}}< p_{\mathrm{OUT}}\], it is possible to show that
the algorithm will terminate since RSS continually strictly improves.

*** 12.5 Shrinkage Methods

**** James-Stein Estimator

The James-Stein estimator for the mean of a normal \[\mathbf{y}\sim N(\boldsymbol\mu, \sigma^2 I)\]
(such as one that might come from an OLS model) can be derived by inspecting MSE.

The minimum-variance unbiased estimator is just \[\mathbf{y}\] (check via Cramer-Rao
lower bound or Gauss-Markov with identity design).

But if we allow biased estimates trading of bias and variance, say
by considering a downscaled estimate \[\tilde{\boldsymbol\mu}=\mathbf{y}c\]
for \[0<c<1\], then the MSE is given by \[\mathbb{E}\|\tilde{\boldsymbol\mu}-\boldsymbol\mu\|_2^2=c^2\sigma^2n + (1-c)^2\|\boldsymbol\mu\|_2^2\]
where \[\dim \mathbf{y}=n\].

The optimal choice from the MSE Perspective is then
\[c_*=1-\frac{n\sigma^2}{n\sigma^2+\|\boldsymbol\mu\|_2^2}\]

which unfortunately relies on the unknown quantity \[\|\boldsymbol\mu\|_2^2\].
Using various estimates for this value itself, such as \[\|\mathbf{y}\|_2^2-n\sigma^2\],
results in different flavors of James-Stein estimators.

Repeating the MSE analysis for the OLS case, where we're estimating the mean
the mean of \[\hat{\boldsymbol\beta}\sim N(\boldsymbol\beta, \sigma^2(X^\top X)^{-1})\]) also yields a class of JS estimators, which is
results in \[c\hat{\boldsymbol\beta}\]. Note that this is different from ridge,
which penalizes spherically. These differences can be inspected through
a Bayesian lens, where James-Stein becomes an empirical Bayes estimator through
the noise estimate \[\sigma^2(X^\top X)^{-1}\] derived from the data.

[[https://www.jstor.org/stable/1268284][Draper and Nostrand 1979]] discuss.

**** Ridge Regression

Ridge regression penalizes the OLS, yielding an estimator \[\hat{\boldsymbol\beta}(\lambda)=(X^\top X+\lambda I)^{-1} X^\top \mathbf{y}\].
It can be motivated as the maximum a posteriori estimate under 
a spherical Gaussian prior for the coefficients or through
direct analysis of the "resampled" \[\mathrm{ME}\], as done for CV estimation.
The expectation of the modeling error (before the data \[\mathbf{y}\] is seen)
can be shown to have a negative derivative for small positive values of
\[\lambda\] (see Equations 12.52), indicating James-Stein-like MSE reduction.

Choosing \[\lambda\], one can rely \[n\] LOOCV estmates, which admit a
closed form from the previous [[cv1][CV]] formula based on the modified
projection \[P_\lambda = X(X^\top X + \lambda I)^{-1}X^\top\].

Generalized cross-validation is based on the observation that while the
ridge loss is invariant to rotation of the original problem to \[RX, R\mathbf{y}\],
LOOCV is not. Under some smoothness assumptions on the true underyling function,
this can be slightly more efficient, but asymptotically
the estimators are equivalent anyway.

**** Garrote and Lasso

Lasso has come a long way since this book was written. Lasso introduces
a different regularization term \[\frac{1}{n}\|\mathbf{y}-X\boldsymbol\beta\|_2^2+\lambda\|\boldsymbol\beta\|_1\]
unlike the \[\ell_2\] norm for Ridge.

[[/sparsity-notes][Sparsity notes]] provide a great overview.

[[https://www.springer.com/gp/book/9783642201912][BÃ¼hlmann and van de Geer]] provide a theoretical text and
[[https://web.stanford.edu/~hastie/StatLearnSparsity/][SLS]] provides a more application-oriented introduction.

*** 12.6 Bayesian Methods

This section introduces the usual spiel about Bayesian approaches and
choosing from a set of models via MAP.

We can view Bayesian methods pragmatically, as a way of injecting prior
information or making computations tractable.

The interesting part comes in when triggering disagreement
between frequentist and Bayesian approaches [[https://en.wikipedia.org/wiki/Lindley%27s_paradox][Lindley's paradox]]
is one such example.

Such situations can be fixed to a certain degree by a choice of
[[https://stats.stackexchange.com/questions/20520][uninformative prior]].

Though it is a lengthy discussion, there are two important philosophical points
to make on Bayes vs frequentist. Under regularity conditions on an
estimation problem, admissible (dominating) decision procedures 
are equivalent to some (possibly improper) [[https://en.wikipedia.org/wiki/Admissible_decision_rule#Admissibility_of_(generalized)_Bayes_rules][Bayesian rule]]. Impropriety may
require giving up some of the Bayesian interpretation. In some cases,
no prior is necessary (when a minimax procedure is available), but in others
the Bayesian approach can improve on an austere frequentist setting.

*** 12.7 Effect of Model Selection on Inference

Trying to construct confidence intervals on coefficients for an OLS
model determined by some selection will result in significant deviation
from theoretical guarantees.

This is because the data is being used to choose the model, and coefficients
which are likely to be selected by a subset selection routine will already
be more likely to be significant, even though they may just have fit to noise.

In fact, philosophically, it's unclear what inference of some submodel
defined by a subset of coefficients \[\beta_M\] means when \[M\] is random
and dependent on the data.

The typical interpretation is that some procedure (e.g., APR with LOOCV)
selects \[M\]. Then, if \[M\] was taken for granted as the true statistical
model, then what is a CI for the coefficients? The answer to this quesion
is described in [[https://arxiv.org/abs/1410.2597][Optimal Inference after Selection]] for details.

A simple solution is to split data: do model selection on one half of the
data and do inference on the other. This may waste data, and can be fixed
by adjusting null distributions to account for the probability of selection.
Less wasteful approaches ("data carving") must be carefully developed
based on the selection procedure, e.g., see [[https://arxiv.org/abs/1311.6238][this lasso IaS approach]].

A frequentist approach that treated \[M\] as a parameter would be forced to
take an austere position, where the union of CIs must be chosen across
the models \[M\]. Let's revisit this idea in [[Open Questions]] with POSI.

*** 12.8 

Stepwise methods can be implemented by taking a classical OLS fit algorithm
and applying rules to add variables, discussed in the section
[[Adding and Deleting Cases and Variables]] above.

For APR, this still results \[2^K\] adds/deletes which is quite expensive.

**** Operate on active submatrices

While the GE approach allows for a very simple implementation, which can
be further improved with various significant constant-factor optimizations
(see [[https://www.tandfonline.com/doi/abs/10.1080/00401706.1972.10488918][Morgan and Tatar]]), it is still unstable.

E.g., naively applying
GE sweeping would require storing the full \[K^2\] matrix, with full sweeps
on addition/removal of variables. By maintaining only the active submatrix
in a regression, we can reduce the active working memory by about half on
average.

**** Regression Trees

We can frame APR search as a tree search, where the /full/ \[K\]-variable
model is fitted first. Then since RSS only decreases as new variables are added,
certain criteria such as AIC can be upper bounded.

E.g., if we know that the model with coefficients \[M_1=\{1, 2, 3\}\] has AIC \[a_1\]
and another model with coefficients \[M_2=\{4, 5, 6\}\] has an AIC greater than \[a_1+2\] then no
2-variable submodel of \[M_2\] (e.g., \[\{4, 5\}\]) will improve AIC (this works since AIC uses a single
variance estimate \[\hat\sigma^2\] across all models).

Of course, this can be generalized. For AIC, given any two explored models \[M_1,M_2\]
with AICs \[a_1,a_2\] and parameter count \[p_1,p_2\], any submodel \[M_3\] of \[M_2\]
with \[p_3\] variables will be dominated by \[M_1\] if \[a_1\le a_2 - 2(p_2-p_3)\], and
vice-versa as well for submodels of \[M_1\].

This can also be extended to similar selection routines like BIC by changing the coefficient
\[\log n\].

**** Modern Research

A very exciting recent line of work essentially attacks the above problem by
[[https://arxiv.org/abs/1507.03133][APR to MIP]] (Mixed Integer Programming) reduction. This is still an exp-time algorithm,
but modern solvers are quite efficient. Advanced forms of clause learning
in [[https://en.wikipedia.org/wiki/DPLL_algorithm][DPLL]]-like routines may learn problem-specific cutoff rules like those
described in [[Regression Trees]]. This approach scales to moderate (\[n,p\] in the thousands)
problems.

Curiously, in very high dimensional settings, actually outperforms best-subset selection
on inference tasks, [[https://arxiv.org/abs/1707.08692][recent work]] shows that in low signal-to-noise settings shrinkage, which
makes optimization easier, is also statistically helpful. Further [[https://arxiv.org/abs/1803.01454][research]] shows
how we cn blend the two approaches.

*** 12.9 Comparison

**** Asymptotic Regimes

Between regularization, information-based criteria, Bayesian methods,
prediction-estimation criteria, what should we actually use to select
our models?

Analyzing finite-\[n\] settings is challenging, but asymptotic
approaches provide insight. They reveal crucial phase changes
as we explore different regimes. As \[n\rightarrow \infty\], how does
the number of considered covariates \[K\] change, assuming the true model
is OLS on some subset \[S_0\] of the \[K\] coefficients?

***** \[K\] fixed, infer \[S_0\]

If the number of inputs \[K\] is fixed, as \[n\rightarrow \infty\], 
the "frequentist approaches"  t\[C_p\], \[\mathrm{CV}(1)\], and \[\mathrm{AIC}\] are asymptotically
equivalent and consistently overfit, selecting a model with a
superset of parameters \[S\supset S_0\] with probability 1.

This isn't as bad as it sounds in the book, though! By a simple procedure,
we can use \[\mathrm{AIC}\]-like criteria on half the data to select
regressors and then confidence intervals generated from the other half
to select \[S_0\] with probability 1 at the cost of some efficiency.

On the other hand, \[\mathrm{BIC}\], and therefore all Bayesian approaches
with prior coverage, converges to \[S_0\] with probability 1, but in practice
this means that in finite settings we can over-penalize, omitting relevant
variables along the way.

Experiments show that for \[\mathrm{CV}(d)\] with \[d\sim n\], or at least
the stochastic approximation to that procedure, practically outperforms
other approaches like \[\mathrm{CV}(1)\].

Another hot take not discussed in the book is [[https://projecteuclid.org/euclid.aos/1369836961][POSI]], which corrects
confidence intervals for _any_ model selection procedure. Unfortunately,
this is of course quite a conservative approach.

***** \[K\] fixed, minimize \[\mathrm{PE}\]

The main question with optimizing prediction error criteria
\[\hat{\mathrm{PE}}\] estimated from the data (for selecting regularization constants or optimal
subsets) is that the parameter optimum \[\hat s =\mathrm{argmin}_s\hat{\mathrm{PE}}(s)\] isn't guaranteed to
be the optimum of the true prediction error \[\mathrm{PE}\].

However, in their most general form, such \[\mathrm{argmin}\] estimators
tend to be well-behaved. In our setting, the parameter \[s\] lives in a small
space (and in fact the normal noise is not tail-heavy so even if \[s\] was in
a large space we still wouldn't be in trouble). Then the
[[https://www.stat.berkeley.edu/~aditya/resources/FullNotes210BSpring2018.pdf][Argmax Continuous Mapping Theorem]] applies, so most smooth estimators
\[\hat{\mathrm{PE}}\] with a unique minimizer will lead to the asymptotically
correct optimum \[\hat s\].

***** \[K\] not fixed, infer \[S_0\]

This setting is very hard :)

[[/sparsity-notes][Sparsity notes]] are a better place to discuss.

***** \[K\] not fixed, minimize \[\mathrm{PE}\]

Here, lasso really shines. For even settings where \[K\sim n\], so both
go to infinity at the same rate, the machinery described above can be
[[https://statistics.berkeley.edu/sites/default/files/tech-reports/797.pdf][extended]] under (to be fair, stringent) assumptions on the covariates.

Without lasso, we need the more restrictive rate \[K^2\sim n\] shown by
[[https://projecteuclid.org/euclid.aos/1176346793][Portnoy]]. Lasso still works in such settings with a smaller regularization
constant, these are its "slow rates" and don't require the same covariate
assumptions mentioned above.

Modern work explores various phase changes \[K^\alpha\sim n\] as \[\alpha\] varies
from 1 to larger numbers, in terms of other statistical properties of the problem
such as the signal-to-noise ratio (which is typically some variant of \[\|X\boldsymbol\beta\|/\|\boldsymbol\varepsilon\|\]).

***** Agnostic setting

An interesting regime is that of trying to estimate models where the true model
isn't guaranteed to be an OLS of the subset of the \[K\] regressors.

This yields fundamental epistemological questions about model fitting in these settings
in the first place. If we just care about finding a model which minimizes
PE, then the machinery for M-estimation above works just the same: we'll be able
to find, in the limit, a model which optimizes prediction error.

Other criteria may also work, we could in sufficiently regular settings have
asymptotic confidence intervals for the coefficients of the model-which-minimizes
KL divergence from the real distribution, whatever it may be.

***** Little Bootstrap

The book end on discussion of the little bootstrap, which is a bootstrap approach
for estimating PE which its author argues results in better approximations
\[\mathrm{PE}(\hat s)\approx \hat{ \mathrm{PE}}(s)\] in practice than other PE
estimation approaches.

It's interesting. Nowadays, everyone uses stochastic approximations to
\[\mathrm{CV}(d)\] (i.e., regular K-fold cross validation). Given the strong
theoretical results about M-estimation, the type of PE estimator probably
matters very little, and the more exciting question comes down to computational
concerns. The little bootstrap, for instance, has analytical limiting solutions
called tiny bootstraps for ridge regression. I may explore those sometime.
