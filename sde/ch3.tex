\documentclass{article}

\title{It\^{o} Integrals}
\author{Vladimir Feinberg}

\usepackage{fullpage}

\input{defs}

\begin{document}

\maketitle

\section{Construction of the It\^{o} Integral}

We recall our original objective, which is to define some stochastic notion of differential equations parameterized by \(b,\sigma\) to identify some process \(X_t\) satisfying
\[
X'(t) = b(t, X_t)+\sigma(t, X_t)\cdot \text{``noise"}\,\,.
\]

It's sensible to consider the forward Euler method, for some shrinking split sequence \(t_i\), which would set
\[
  \Delta_kX=b(t_k, X_{t_k})\Delta_k t+\sigma(t_k, X_{t_k})\Delta_k V\,\,,
  \]
  where \(\Delta_k f=f\pa{t_{k+1}}-f\pa{t_{k}}\) and \(V_t\) is some stochastic process.

  Based on natural assumptions about \(\Delta_k V\) such as stationarity, independent increments, and 0-mean, we might be tempted to choose Brownian motion \(V_t=B_t\). In the limit as \(\Delta_kt\rightarrow 0\), we would be interested in finding a definition of an integral which is consistent with the limit of the discrete sums implied by the Euler method:
  \[
X_t=X_0+\int_0^t b(s, X_s)\d{s}+\int_0^t\sigma(s, X_s)\d{B_s}\,\,,
\]
where \(\int_0^t\sigma(s, X_s)\d{B_s}\) is not yet sensibly defined or shown to exist uniquely as \(\lim_n\sum_{k=1}^n\sigma(t_k, X_{t_k})\Delta_i B\).

\begin{definition}[It\^{o} Integral]
  Consider the probability space \((\Omega,\mcF,\P)\) where \(\mcF=\mcF_\infty\) for a filtration \(\ca{\mcF_t}\). Let \(\mcV =\mcV(S, T)\) be the class of functions \(f: \R_+\times \Omega\rightarrow\R\) satisfying the following properties:
  \begin{enumerate}
  \item \(f\) is \(\mathcal{B}\pa{\R_+}\times\mcF\) measurable.
  \item \(f(t, \cdot)\) is \(\mcF_t\)-adapted.
  \item \(L^2(\P \times [S,T])(f)=\E^{1/2}\ha{\int_S^T f(t,\omega)^2\d{t}}<\infty\)
  \end{enumerate}

  Then under some shrinking split sequence (see previous chapter notes), the It\^{o} Integral is defined as the limit
  \[
\int_S^Tf(t, \omega)\d{B_t(\omega)}=\lim_{n\rightarrow\infty}\sum_{k=1}^n\phi_n(t_k, B_{t_k}(\omega))\Delta_i B(\omega)\,\,,
\]
where \(\phi_n\) are simple functions chosen by diagonalizing multiple other unique limit sequences. These \(\phi_n\) will be recovered from multiple other sequences as we grow the complexity of \(f\) to the full extent of \(\mcV\). To start, if \(f\) is a simple function, then of course \(\phi_n=f\) suffices. Afterwords, we show the convergence under \(L^2(\P \times [S, T])\) of limit sequences for growing subsets of $\mcV$:
\begin{enumerate}
\item Bounded and everywhere continuous processes. Since simple functions approximate our continuous process everywhere on \(\Omega\), by bounded convergence they approximate in \(L^2(\P \times [S, T])\).
\item Bounded processes \(g\in \mcV\). As \(n\rightarrow\infty\), consider a bump function \(\psi_n\) with shrinking support immediately left of the origin. Consider the convolution \(g_n=\psi_n * g\), also in \(\mcV\) by virtue of the negative support of \(\psi_n\). Note that \(g_n\rightarrow g\) pointwise as \(\psi_n\) progressively approximates the Dirac measure. Then again by bounded convergence we have \(g_n\rightarrow g\) in \(L^2(\P \times [S, T])\), where \(g_n\) are bounded and everywhere continuous by construction.
\item Any process in \(g\in \mcV\). Let \(g_n\) be \(g\) clamped to \([-n, n]\). By the previous step, we have convergence to each such \(g_n\) via limit sequence in \(\mcV\). Then since \(g\in L^2(\P \times [S,T])\) itself, by dominated convergence we have convergence in \(g\in L^2(\P \times [S,T])\).
\end{enumerate} 
\end{definition}

For any \(f\in\mcV\) we may diagonalize the above sequences for some {\em elementary} functions \(\phi_n\) to define the It\^{o} integral. Why does this definition give rise to a {\em unique} integral, independent of the choice of \(\phi_n\) (and thus, also, independent of split sequence choice)? To see this, we need a critical tool.

\begin{lemma}[It\^{o} Isometry]
  For any \(f\in \mcV\),
  \[
    \E\ha{\int_S^Tf(t, \omega)^2\d{t}}
    = \E\ha{\pa{\int_S^Tf(t, \omega)\d{t}}^2}\,\,,
  \]
  or, in operator form,
  \[L^2(\P \times [S, T])(f)=L^2(\P)\circ \int_S^T\d{t}\,\,.\]
\end{lemma}

For elementary functions, this can be proven by computing both sides explicitly and relying on stationarity of increments. In turn, this implies that for any \(f\in\mcV\), for any \(L^2(\P \times [S,T])\) sequence \(\phi_n\) of elementary functions, which is necessary Cauchy in that metric, must give rise to a sequence of integrals \(I_n=\int_S^T\phi_n\d{B_t}\) which are Cauchy in \(L^2(\P)\) and thus converge as random variables in this way to a fixed limit \(I = I(f, \phi)\) in \(L^2(\P)\). But for any other similar elementary function sequence \(\phi_n'\) converging to \(f\) in \(L^2(\P \times [S,T])\), we construct an analogous sequence of integrals \(I_n'\), also Cauchy in \(L^2(\P)\), thus also yielding an It\^{o} integral \(I'=I(f, \phi')\). Through our elementary isometry,
\begin{align*}
  \norm{I_n-I'}_{L^2(\P)}&\le\norm{I_n-I_n'}_{L^2(\P)}+\norm{I_n'-I'}_{L^2(\P)}\\
                         &=\norm{\phi_n-\phi_n'}_{L^2(\P)\circ\int_S^T\d{t}}+\norm{I_n'-I'}_{L^2(\P)}\\
                         &=\norm{\phi_n-\phi_n'}_{L^2(\P\times[S,T])}+\norm{I_n'-I'}_{L^2(\P)}\\
                           &\rightarrow 0\,\,,
\end{align*}
but since \(I_n\) converges to both \(I,I'\) in \(L^2(\P)\), by completeness of the corresponding metric space, \(I=I'\) almost surely, showing both invariance of the It\^{o} integral to the specific limit sequence \(\phi_n\) chosen and It\^{o} isometry on all of \(\mcV\).

\section{Some properties of the It\^{o} integral}

By taking limits from the definition, the following properties of the It\^{o} integral hold almost always:

\begin{enumerate}
\item \(\int_S^Tf\d{B_t}=\int_S^Uf\d{B_t}+\int_U^Tf\d{B_t}\),
\item \(\int_S^T \pa{cf+g}\d{B_t}=c\int_S^Tf\d{B_t}+\int_S^Tg\d{B_t}\) for constant \(c\),
\item \(\E \int_S^Tf\d{B_t} =0\),
\item \(\int_S^Tf\d{B_t} \in\mcF_T\),
\item and \(M_s=\int_0^s f\d{B_t}\) is a martingale.
\end{enumerate}

\begin{theorem}
  Let \(f\in\mcV(0, T)\). Then there exists a \(t\)-continuous version of \(I_t(\omega)=\int_0^tf(s, \omega)\d{B_s(\omega)}\).
\end{theorem}
\begin{proof}
  Let \(\phi_n\) be elementary functions tending to \(f\) in \(L^2(\P \times [0, T])\) from It\^{o}'s construction, with corresponding It\^{o} integrals \(I_t^{(n)}\). For any \(n,m\), \(I_t^{(n)}-I_t^{(m)}\) is a martingale since its components are. \href{https://math.ucsd.edu/~pfitz/downloads/courses/winter05/math280b/doob.pdf}{Doob's Martingale Inequality}, which is basically Markov for martingales, relates
  \[
    \P\ca{\sup_{0\le t\le T}\abs{I_t^{(n)}-I_t^{(m)}}\ge \epsilon}\le\frac{1}{\epsilon^2}\E\abs{I_t^{(n)}-I_t^{(m)}}^2=\frac{1}{\epsilon^2}\norm{\phi_n-\phi_m}_{L^2(\P\times[0,T])}^2\,\,.
  \]
  Since the RHS vanishes as \(n,m\) grow large, a subsequence \(I_t^{(n_k)}\) may be chosen ensuring with geometrically decreasing \(\epsilon\) between adjacent \(I_t^{(n_k)},I_t^{(n_{k+1})}\). This allows Borel-Cantelli to be applied over the set of events \(\ca{\sup_{0\le t\le T}\abs{I_t^{(n_k)}-I_t^{(n_{k+1})}}\ge 2^{-k}}\) for varying \(k\in\N\), so in fact with probability one \(\sup_{0\le t\le T}\abs{I_t^{(n_k)}-I_t^{(n_{k+1})}}\le 2^{-k}\) so long as \(k\ge K(\omega)\). This provides uniform convergence for almost all \(\omega\), and thus continuity in the limit.
\end{proof}

Note that we {\em cannot} assume that the elementary functions \(\phi_n\), known to exist, which define the It\^{o} integral \(\int_S^T f\d{B_t}\) for \(f\in \mcV(S,T)\), are of the form \(\phi_n(t, \omega)\stackrel{?}{=}\sum_jf(t_j,\omega)1\ca{t\in[t_j,t_{j+1})}\). \textbf{Observation}. For general \(f\in\mcV(S,T)\), we do not know a-priori the form of the elementary functions which define its It\^{o} integral.

However, from the original construction of the It\^{o} integral, we do have some basic lemmas which help.

\begin{lemma}
  \label{lemma:validelem}
  If \(f,f_n\in\mcV(S, T)\) and \(f_n\convarg{L^2(\P\times[S,T])} f\), then \(\int_S^T f_n\d{B_t}\convarg{L^2(\P)} \int_S^Tf\d{B_t}\). Proof immediately follows from It\^{o}'s lemma.
\end{lemma}


\begin{lemma}
  \label{lemma:continuous}
  If \(f\in\mcV(S, T)\) almost always has continuous sample paths, then \(\phi_n\convarg{L^2(\P\times[S,T])} f\) where \(\phi_n(t, \omega)=\sum_jf(t_j,\omega)1\ca{t\in[t_j,t_{j+1})}\).
\end{lemma}
\begin{proof}
  A stronger version of this Lemma is proven as an exercise. We need only rely on a weaker notion of continuity; it suffices to show \(f\) is CMS; see Exercise~\ref{ex3.13c} below.

  To show an almost always continuous \(f\) is CMS, interchange limits by DCT and appeal to continuity.
  \[
    \lim_{s\rightarrow t}\E\pa{f(s,\cdot)-f(t, \cdot)}^2= \E\lim_{s\rightarrow t}\pa{f(s,\cdot)-f(t, \cdot)}^2=0
    \]
  \end{proof}

\section{Extensions of the It\^{o} integral}

Without much effort, the filtration \(\mcF\) can be expanded to any other filtration \(\mcH\) such that \(\mcF_t\subset\mcH_t\).

Relaxing the integrator to be an arbitrary semimartingale instead of Browninan motion is possible. Revisiting our construction steps, the only direct properties used were bounded quadratic variation and adaptedness. These can be further relaxed to just \href{https://en.wikipedia.org/wiki/Semimartingale}{semimartingale} properties.

The second moment condition in It\^{o}'s construction can be relaxed to a mere condition requiring that the second moment of our integrand is almost surely finite, \(\P\ca{\int_S^Tf(s,\omega)^2\d{s}<\infty}=1\), however, this of course will end up creating non-integrable It\^{o} limits, and thus drop the martingale property. However, continuity can be recovered, along with a local martingale property.

Define this extended definition of almost surely \(L^2([S,T])\) It\^{o} integrands as \(\mcW(S,T)\). Clearly, \(\mcV\subset\mcW\).

There also exists another interpretation of stochastic integrals. The Stratonovich is built from a trapezoidal rule (unlike It\^{o}'s left-sided integration rule) as the building block:
  \[
\int_S^Tf(t, \omega)\circ \d{B_t(\omega)}=\lim_{n\rightarrow\infty}\sum_{k=1}^n\phi_n\pa{\frac{t_k+t_{k+1}}{2}, B_{t_k}(\omega)}\Delta_i B(\omega)\,\,.
\]
One reason the Stratonovich interpretation might be more natural is because of the following motivation. If we consider \(t\)-continuously differentiable approximations \(B_t^{(n)}\) to our noise process \(B_t\) which on bounded intervals converge uniformly almost always, then the solution \(X^{(n)}_t\) to the differential equation (for constant \(\omega\))
\[
\frac{d X_t}{dt}=b(t,X_t)+\sigma(t,X_t)\frac{dB_t^{(n)}}{dt}\,\,,
\]
converges to the Stratonovich solution \(X_t=X_0+\int_0^tb(t, X_s)\d{s}+\int_0^t\sigma(t, X_s)\circ\d{B_s}\), whereas the solution expressed in the It\^{o} interpretation would be
\[
  X_t=X_0+\int_0^tb(t, X_s)\d{s}+\frac{1}{2}\int_0^{t}\sigma'(s, X_s)\sigma(s, X_s)\d{s}+\int_0^t\sigma(t, X_s)\d{B_s}
  \]

\section{Exercises}

\subsection{}
\label{pt1}
We show
\[
  \int_0^t s\d{B_s}=tB_t-\int_0^tB_s\d{s}
\]
directly from the definition of It\^{o} integrals. We consider the progressively finer elementary functions defined by the discretization of the identity integrand as \(n\rightarrow\infty\) for some shrinking split sequence \(t_j\), which tend to the identity in \(L^2(\P\times[0,t])\).
\[
  \sum_it_i\pa{B_{t_{i+1}}-B_{t_{i}}}=  t_nB_t-\sum_i\pa{t_{i+1}-t_{i}}B_{t_i}\,\,,
\]
so that the limit converges to the desired solution, because almost-always continuity of \(B_t\) permits \(\omega\)-pointwise Riemann integration.

Note the above relies on Lemma~\ref{lemma:validelem} and Lemma~\ref{lemma:continuous} to derive the It\^{o} integral from the definition with the obvious elementary function approximation.

\subsection{}
\label{pt2}

We show
\[
  \int_0^t B_s^2\d{B_s}=\frac{1}{3}B_t^3-\int_0^tB_s\d{s}
\]
directly from the definition of It\^{o} integrals. A more general technique for solving such integrals from the definition follows in three steps:

\begin{enumerate}
\item Rewrite the expected naive Riemann integral solution (here, \(\frac{1}{3}B_t^3\)) as a telescoping sum. \(B_t^3=\sum_j\Delta_jB^3\).
\item Express each term in an It\^{o}-friendly way, i.e., as functions of increments or the left sided-timestep, i.e., \(B_{t_j},\Delta_jB\).
  \begin{align*}
    \Delta_jB^3&=\Delta_jB\pa{B_{t_{j+1}}^2+B_{t_{j+1}}B_{t_j}+B_{j}^2}\\
    &=\Delta_jB\pa{B_{t_{j+1}}^2-2B_{t_{j+1}}B_{t_j}+B_{j}^2}+3B_{t_{j+1}}B_{t_j}\Delta_jB\\
    &=\Delta_j^3B+3(\Delta_jB+B_{t_j})B_{t_j}\Delta_jB\\
    &=\Delta_j^3B+3B_{t_j}\Delta_j^2B+3B_{t_j}^2\Delta_jB
  \end{align*}
\item Summing the above formula over \(j\) and rearranging terms from the first telescope we have
  \[
    \sum_jB_{t_j}^2\Delta_jB=\frac{1}{3}B_t^3-\frac{1}{3}\sum_j\Delta_j^3B-\sum_jB_{t_j}\Delta_j^2B\,\,.
  \]

  In the limit as \(n\rightarrow\infty\), the LHS converges to the desired It\^{o} integral \(  \int_0^t B_s^2\d{B_s}\) in \(L^2(\P)\). The middle term vanishes since \(\abs{\sum_j\Delta_j^3B}\le\sum_j\abs{\Delta_j^3B}\rightarrow[B]_t^{(3)}=0\). To finish, it remains to show that the last term simplifies to \(\int_0^tB_s\d{s}\).

  Notice it suffices to show that \(\|\sum_jB_{t_j}\Delta_j^2B-\sum_jB_{t_j}\Delta_jt\|_{L^2(\P)}\rightarrow 0\) as the \(L^2\) convergence implies an almost surely convergent subsequence, and by uniqueness of the It\^{o} integral definition, this shows almost sure equality on the final integrals regardless of the elementary function sequence we look at. Indeed,
\begin{align*}
  \E\pa{\sum_jB_{t_j}(\Delta_j^2B-\Delta_jt)}^2
    &=\E\sum_jB_{t_j}^2(\Delta_j^2B-\Delta_jt)^2+2\E\sum_j\sum_{k> j}B_{t_j}B_{t_k}(\Delta_j^2B-\Delta_jt)(\Delta_k^2B-\Delta_kt)\,\,.
\end{align*}
In the first term, independence simplifies calculation, yielding \(\E B_{t_j}^2\E(\Delta_j^2B-\Delta_jt)^2=O(tn^{-2})\) term-wise, leveraging the variance of a standard \(\chi^2\) distribution, which still vanishes in an \(n\)-sized sum. The second term can be decomposed
\[
  \E \ha{B_{t_j}B_{t_k}(\Delta_j^2B-\Delta_jt)(\Delta_k^2B-\Delta_kt)}
  =  \E \ha{B_{t_j}B_{t_k}(\Delta_j^2B-\Delta_jt)}\E\ha{(\Delta_k^2B-\Delta_kt)}\,\,,
\]
where the last term vanishes. Of course, \(\Delta_jt=n^{-1}\) for all \(j\) above.
  \end{enumerate}

Note the above relies on Lemma~\ref{lemma:validelem} and Lemma~\ref{lemma:continuous} to derive the It\^{o} integral from the definition with the obvious elementary function approximation.
  
  \subsection{}

  Let \(X_t\) be a stochastic process.
\subsubsection{}
If \(X_t\) is a martingale with respect to any filtration \(\mcG_t\), it must be a martingale with respect to its own natural filtration \(\mcF_t\). Measurability and integrability conditions hold immediately, so it suffice to check the conditional martingale property. In addition, by virtue of \(X_t\in \mcG_t\) we must have \(\mcF_t\subset\mcG_t\) through an inductive argument. Taking \(s\le t\), by tower, \(\CE{X_t}{\mcF_s}=\CE{\CE{X_t}{\mcG_s}}{\mcF_s}=\CE{X_s}{\mcF_s}=X_s\).
\subsubsection{}
By the above, for any martingale \(X_t\), we have for all \(t\ge 0\),
\[
  \E X_t=\E\CE{X_t}{\mcF_0}=\E X_0\,\,.
\]
\subsubsection{}
The above may be satisfied for non-martingales; consider \(X_t=t\epsilon\) where \(\epsilon\) is Rademacher-distributed.
\subsection{}
\subsubsection{}
\(X_t=B_t+4t\) is not a martingale because its expectation is strictly increasing with time (see previous problem).
\subsubsection{}
\(X_t=B_t^2\) is not a martingale because its expectation is strictly increasing with time.
\subsubsection{}
\(X_t=t^2B_t-2\int_0^tsB_s\d{s}\) is a martingale. Integrability and measurability hold immediately. Then notice
\[
  \CE{X_t}{\mcF_s}=t^2B_s-2\int_0^sB_r\d{r}-2\int_s^tr\CE{B_r}{\mcF_s}\d{r}=t^2B_s-2\int_0^sB_r\d{r}-2B_s\int_s^tr\d{r}=X_s\,\,.
\]
\subsubsection{}
\(X_t=B_t^{(1)}B_t^{(2)}\) is a martingale, where \(B_t^{(i)}\) are independent brownian motions, so the conditional expectation operator will distribute.

\subsection{}

\(M_t=B_t^2-t\) is a martingale. Measurability and integrability are apparent, and \(\CE{M_t}{\mcF_s}=B_s^2+\CE{2B_s(B_t-B_s)+(B_t-B_s)^2}{\mcF_s}-t=M_s\) by splitting up \(B_t=B_s+(B_t-B_s)\).

\subsection{}

\(N_t=B_t^3-tB_t\) is a martingale. Measurability and integrability are apparent. Performing the same expansion \(B_t=B_s+\Delta\) as before, we find
\[
  \CE{N_t}{\mcF_s}=
  \CE{\Delta^3 + 3\Delta^2 B_s+3\Delta B_s^2-3tB_t+B_s^3}{\mcF_s}=
  3(t-s) B_s-3tB_s+B_s^3=N_s
\]\label{pt3}
\subsection{}
Consider the iterated It\^{o} integral
\[
  I_n=n!\int_0^{t}\int_0^{u_n}\cdots\int_0^{u_2}\d{B_{u_1}}\d{B_{u_2}}\cdots\d{B_{u_n}}\,\,.
\]

\subsubsection{}

For every tuple \(u_2,\cdots, u_n\), necessarily in sorted order already, the innermost integrand \(1\) is measurable with respect to \(\mcB(\R_+)\times \mcF\). It is also adapted to \(\mcF_{u_1}\) trivially. Finally, it's square integrable, as the entire integral \(\int_0^{u_2}1^2\d{B_1}\le u_2\le t\). The resulting integral \(I^{(n)}_1(u_2,\cdots, u_n)=\int_0^{u_2}1\d{B_1}\) is by properties of It\^{o} integration itself measurable and adapted to \(\mcF_{u_2}\), as well as dominated by \(t\) over \(u_2\in[0,u_3]\).

Then, inductively, assume for a tuple \(u_i,\cdots, u_n\), the corresponding integrand \(\int_0^{u_{i}}\cdots\d{B_{u_{i-1}}}\) is measurable, adapted, and square integrable due to being dominated by \(t^{i-1}/(i-1)!\). Then adding another integral again results in a measurable and adapted random process dominated in \(L^2(\P \times [S,T])\) by \(t^i/i!\).

\subsubsection{}

Manually, we can verify that
\[
I_1=B_t\,\,,
\]
and since \(\int_0^{u_2}\d{B_{u_1}}=B_{u_2}\) that
\[
  I_2=B_t^2-t\,\,,
\]
and finally,
\[
  I_3=3 \int_0^{t}B_{u_2}^2-u_2\d{B_{u_2}}=B_t^3-3tB_t\,\,,
\]
by combining Exercises~\ref{pt1} and~\ref{pt2}.

We notice that all of the above are of the form \(I_n=t^{n/2}h_n\pa{\frac{B_t}{\sqrt{t}}}\), where \(h_n\) is the Hermite polynomial of degree \(n\), where
\[
  h_n(x)=(-1)^n\exp\pa{\frac{x^2}{2}}\partial_x^n\exp\pa{\frac{-x^2}{2}}
\]
\subsubsection{}

By the above, \(N_t\) from Exercise~\ref{pt3} must be a martingale as it is an It\^{o} integral.

\subsection{}


\subsubsection{}
Let \(Y\) be an integrable rv and let \(M_t=\CE{Y}{\mcF_t}\)
\(M_t\) is a martingale, following immediately from the tower property.
\subsubsection{}

\textbf{Observation}. The below doesn't seem to hold without the additional continuity assumption, which disagrees with the primary text.

Conversely, suppose \(M_t\) is a real-valued \(\mcF_t\)-martingale which satisfies for some \(p\) that \(\sup_t\E\abs{M_t}^p<\infty\). We can show there exists some integrable \(Y\) such that \(M_t=\CE{Y}{\mcF_t}\).

By Corollary C.7 in the appendix (also, for discrete settings, by Theorem 4.7 from \c{C}inlar's Probability and Stochastics,  p.201, Theorem 4.7, the characterization of uniformly integrable martingales), {\em with the additional assumption that \(M_t\) is continuous, an assumption absent from {\O}ksendal}, we have that an integrable \(Y\) exists such that \(M_t\convarg{L^1}Y\).

Then, notice that \(\CE{Y-M_t}{\mcF_t}=0\) if \(\E\ha{1_H\pa{Y-M_t}}=0\) is for all events \(H\in \mcF_t\).
But \(1_H\pa{M_s-M_t}\convarg{L^1}1_H\pa{Y-M_t}\) as \(s\rightarrow\infty\) by analyzing the cases of \(1_H\in\{0,1\}\). So \(\E\ha{1_H\pa{Y-M_t}}=\lim_{s\rightarrow\infty}\E\ha{1_H\pa{M_s-M_t}}\) where for \(s\ge t\)
\[
  \E\ha{1_H\pa{M_s-M_t}}=\E \ha{1_H\CE{M_s-M_t}{\mcF_t}}=0\,\,.
\]

The continuity assumption seems load-bearing. For discussion, see \url{https://math.stackexchange.com/a/4256605/38471}{here}.

\subsection{}\label{ex3.9}

Recall the Stratonovich integral is defined as
\[
  \int_0^Tf(t,\omega)\circ\d{B_t(\omega)}=\lim_n\sum_jf(t_j^*,\omega)\Delta_jB\,\,,
\]
where \(t_j^*=\frac{t_j+t_{j+1}}{2}\). We suppose that for \(f\in\mcV(0, T)\) and continuous (such that Lemma~\ref{lemma:validelem} and Lemma~\ref{lemma:continuous} apply) the above Stratonovich integral exists in \(L^2(\P)\) and is unique regardless of choice of shrinking split partition (without proof).

We compute \(\int_0^TB_t\circ\d{B_t}\).

Consider the term of the sum on the RHS, \(f(t_j^*,\omega)\Delta_jB\), for \(f(t,\omega)=B_t(\omega)\).
\begin{align*}
  B_{t_j^*}\Delta_jB
  &=B_{t_j^*}\pa{\pa{B_{t_{j+1}}-B_{t_j^*}}+\pa{B_{t_j^*}-B_{t_j}}}\\
  &=B_{t_j^*}\pa{B_{t_{j+1}}-B_{t_j^*}}+B_{t_j}\pa{B_{t_{j}^*}-B_{t_j}}
    -B_{t_j}\pa{B_{t_{j}^*}-B_{t_j}}+B_{t_j^*}\pa{B_{t_j^*}-B_{t_j}}\\
  &=\sum_{i=2j-1}^{2j}B_{t_i^\dag}\Delta_i^\dag B+\pa{B_{t_j^*}-B_{t_j}}^2
\end{align*}

Where \(\ca{t_i^\dag}_i\) iterates over twice as many points as \(\ca{t_j}_{j=1}^n\) and includes the midpoints of the latter sequence. Similarly, \(\Delta_i^\dag g=g(t_{i+1}^\dag)-g(t_{i}^\dag)\).

Introduce the notation \(a_n\lpeq b_n\) when \(\lim_n\norm{a_n-b_n}_{L^2(\P)}\) and one of the limits converges as \(a_n\convlp a\) to the same \(a\) for any subsequence of \(a_n\). This implies that \(a_n,b_n\convas a\) and \(b_n\convlp a\) as well.

Then, starting from assumption,

\begin{align*}
  \int_0^TB_t\circ\d{B_t}
  &\lpeq \sum_jB_{t_j^*}\Delta_jB\\
  &\lpeq \sum_iB_{t_i^\dag}\Delta_i^\dag B+\sum_j(B_{t_j^*}-B_{t_j})^2\\
  &\lpeq \int_0^TB_t\d{B_t}+\sum_j(B_{t_j^*}-B_{t_j})^2\,\,.
\end{align*}

Next, notice that each term of the sum \(\sum_j(B_{t_j^*}-B_{t_j})^2\) is jointly independent and distributed exactly as each term of the sum \(\sum_{i=1}^n\pa{\Delta_i^\dag B}^2\), which is just the half interval, respectively. Following the proof from Chapter 2 Exercise 17, we can re-derive what is essentially the quadratic variation \([B]_t^{(2)}\) over the interval \([0, T/2]\), so that \(\sum_j(B_{t_j^*}-B_{t_j})^2=\frac{T}{2}\). Combining this with the known It\^{o} integral \(\int_0^TB_t\d{B_t}\), we have that
\[
  \int_0^TB_t\circ\d{B_t}=\frac{1}{2}B_T^2\,\,.
  \]

  \subsection{}

  Consider the setup in Exercise~\ref{ex3.9} for any deterministic choice \(t_j^*\in[t_j,t_{j+1}]\), not necessarily the midpoint. Recall \(f\in\mcV(0, T)\) and let \(f_t=f(t,\cdot)\) be shorthand for the corresponding random variable. Further, assume that
  \[
    \E\abs{f_s-f_t}^2\le K\abs{s-t}^{1+\epsilon}
  \]
  for any fixed \(\epsilon>0\).

  Define \(I_n^*(\omega)=\sum_jf(t_j^*, \omega)\Delta_j B\). We show that \(I_n^*\convlpi \int_0^T f\d{B_t}\). Let \(I_n(\omega)=\sum_jf(t_j, \omega)\Delta_j B\) be the standard elementary sum approximating our It\^{o} integral.

  Before we begin, notice that through Jensen concavity, we can derive a weaker \(L^1\) continuity:
  \[
  \E^2\abs{f_s-f_t}\le\pa{  \E^{1/2}\abs{f_s-f_t}^2}^2=\E\abs{f_s-f_t}^2\le K\abs{s-t}^{1+\epsilon}\,\,.
\]

By DCT,
\[
  \lim_n\E\abs{I_n- I_n^*}=  \lim_n\E\abs{\int f\d{B_t}- \lim_nI_n}\,\,,
\]
so we need only show the LHS vanishes.
    
  \begin{align*}
    \E\abs{I_n-I_n^*}
    &\le\sum_j\E\abs{\pa{f_{t_j}-f_{t_j^*}}\Delta_jB}\\
    &\le\E\abs{\Delta_1B}\sum_j\E\abs{f_{t_j}-f_{t_j^*}}\\
    &\le \sqrt{\frac{2}{\pi n}}\sum_jK^{1/2}\abs{t_j^*-t_j}^{0.5+\epsilon/2}=O\pa{n^{-\epsilon/2}}\,\,,
  \end{align*}
  Since \(\abs{t_j^*-t_j}\le \frac{1}{n}\) by choice of split sequence.

  \textbf{Observation}. One of my earlier proof attempts made it seem like one could use the much weaker condition,
    \[
    \E\abs{f_s-f_t}^2\le K\abs{s-t}^{\delta}\,\,,
  \]
  holding for any fixed \(\delta>0\). With this, I thought I could prove an even stronger result, that \(I_n^*\convlp\int_0^Tf\d{B_t}\), but was eventually able to find the bug.

  First, we define the elementary functions
  \[
    \phi_n^*(t, \omega)=\sum_jf(t_j^*,\omega)1\ca{t\in[t_j,t_{j+1})}\,\,.
  \]
  Now \(I_n^*=\int_0^T\phi_n^*\d{B_t}\). Let \(\phi_n\) be an elementary function sequence such that \(\phi_n\rightarrow f\) in \(L^2(\P\times[0,T])\) (and therefore \(I_n=\int_0^T\phi_n\d{B_t}\) satisfies \(I_n\convlp \int_0^T f\d{B_t}\)). As before, by DCT suffices to show that the following vanishes:
  \begin{align*}
    \norm{I_n-I_n^*}_{L^2(\P)}
    &=\norm{\phi_n-\phi_n^*}_{L^2(\P\times[0,T])}\\
    &=\E\sum_j\pa{f_{t_j}-f_{t_j^*}}^2\Delta_jt\\
    &\le\frac{K}{n}\sum_j\abs{t_j^*-t_j}^{1+\epsilon}=O(n^{-1-\epsilon})\,\,,
  \end{align*}
  where the critical first step supposedly follows from It\^{o}'s lemma.

  However, upon closer inspection we notice \(I_n^*=\int_0^T\phi_n^*\d{B_t}\) is not well-defined, because \(\phi_n^*(t,\omega)\), though elementary, is not in \(\mcV(0, T)\) as it is not \(\mcF_t\)-measurable. This demonstrates the importance of the filtration measurability condition.

  \subsection{}

  Consider a stochastic process \(W_t\) such that all of the following attributes hold:
  \begin{enumerate}
  \item \(W_t\) is stationary; namely the joint distribution of \(\ca{W_{t_1+t},W_{t_2+t},\cdots,W_{t_k+t}}\) does not depend on \(t\) for any finite collection \(\ca{t_i}_{i=1}^k\subset \R_+\),
  \item \(W_t\independent W_s\) where \(t\neq s\),
  \item \(\E W_t=0\) for all \(t\),
  \item and \(W_t\) is continuous.
  \end{enumerate}
  In this case, \(W_t\) must be trivial.

  Define for fixed \(N>0\) \(W_t^{(N)}=-N\lor \pa{N\land W_t}\) as the original process clamped to \([-N,N]\). Then by the bounded convergence theorem,
  \[
    \lim_{s\rightarrow t}\E\pa{ W_t^{(N)}-W_s^{(N)}}^2    =\E\lim_{s\rightarrow t}\pa{ W_t^{(N)}-W_s^{(N)}}^2=0\,\,.
    \]
    And yet, by expanding the square and applying independence, we have that for \(s\neq t\) that
    \[
          \lim_{s\rightarrow t}\E\pa{ W_t^{(N)}-W_s^{(N)}}^2    =\lim_{s\rightarrow t}\E\pa{W_t^{(N)}}^2+\E\pa{W_s^{(N)}}^2=2\E\pa{W_t^{(N)}}^2\,\,,
        \]
        where we must actually apply BCT again.

        But all this implies that \(\E\pa{W_t^{(N)}}^2=0\). But this implies \(W_t^{(N)}=0\) almost surely for any \(N\). Taking limits we have our result.

        \subsection{}

        For brevity, given some endpoints \(S,T\) in context (typically, \(S=0\), the notation
        \[
          \d{X_t}=b(X_t,t)\d{t}+\sigma(X_t,t)\d{B_t}\,\,,
        \]
        is taken to be an almost sure equality
        
        \[
          \int_S^T\d{X_t}=\int_S^Tb(X_t,t)\d{t}+\int_S^T\sigma(X_t,t)\d{B_t}\,\,,
        \]
        so that corresponding assumptions are placed such as \(\sigma\in\mcV(S, T)\) and \(b\) is Lebesgue integrable.

        In this case, per the chapter text, if the Stratonovich process
        \[
          \d{X_t}=b(X_t,t)\d{t}+\sigma(X_t, t)\circ\d{B_t}
        \]
        exists (which we have assumed under the implicit assumptions we've equipped this notation with), then the corresponding It\^{o} below must be almost surely equal to it:
        \[
          \d{X_t}=\pa{b(X_t,t)-\frac{1}{2}\sigma(X_t,t)\partial_1\sigma(X_t, t)}\d{t}+\sigma(X_t, t)\circ\d{B_t}\,\,.
        \]

        \textbf{Observation}. This results in a straightforward computation for going in the Stratonovich to It\^{o} direction of converting stochastic differential equations. But this also shows us that It\^{o} process which happen to also be Stratonovich (the other direction) are rarer; they require the additional regularity \(\partial_1\sigma\in\mcV\) on the noise term. This makes sense; an It\^{o} process is necessarily a martingale, but a Stratonovich one is not necessarily one. However, as described in \href{https://www.robots.ox.ac.uk/~lsgs/resources/ito-strat.pdf}{these notes} comparing the two integtrals, this difference is overblown most of the time. The below exercises, which convert from one to the other, demonstrate this.

        \subsubsection{}
        \begin{align*}
          \d{X_t}
          &=\gamma X_t\d{t}+\alpha X_t\circ\d{B_t}\\
          &=\pa{\gamma-\frac{1}{2}\alpha^2} X_t\d{t}+\alpha X_t\d{B_t}
          \end{align*}
        \subsubsection{}
        \begin{align*}
          \d{X_t}
          &=\sin X_t \cos X_t \d{t}+\pa{t^2+\cos X_t}\circ\d{B_t}\\
          &=\frac{\sin X_t}{2}\pa{\cos X_t-t^2} X_t\d{t}+\pa{t^2+\cos X_t}\d{B_t}
          \end{align*}
        \subsubsection{}
        \begin{align*}
          \d{X_t}
          &=r X_t\d{t}+\alpha X_t\d{B_t}\\
          &=\pa{r-\frac{\alpha^2}{2}}\ X_t\d{t} + \frac{1}{2}\pa{\alpha X_t}\alpha\d{t} +\alpha X_t\d{B_t}\\
          &=\pa{r-\frac{\alpha^2}{2}}\ X_t\d{t} + \alpha X_t\circ\d{B_t}
          \end{align*}
        \subsubsection{}

        \begin{align*}
          \d{X_t}
          &=2\exp(-X_t)X_t\d{t}+X_t^2\d{B_t}\\
          &=\pa{2\exp(-X_t)-X_t^3}\ X_t\d{t} + \frac{1}{2}X_t^2\pa{2X_t}\d{t} +X_t^2\d{B_t}\\
          &=\pa{2\exp(-X_t)-X_t^3}\ X_t\d{t}+ X_t^2\circ\d{B_t}
        \end{align*}
        \subsection{}

        A stochastic process \(X_t\) is considered to be continuous in mean square (CMS) if for all \(t\) it satisfies \(E X_t^2<\infty\) and \(\lim_{s\rightarrow t}\E\pa{X_t-X_s}^2=0\)
        \subsubsection{}
        \(B_t\) is CMS because \(\E\pa{B_s-B_t}^2=\abs{s-t}\rightarrow 0\).
        \subsubsection{}
        If \(f\) is \(C\)-Lipschitz then \(f(B_t)\) is CMS because \(\E\pa{f(B_s)-f(B_t)}^2\le C\E\abs{B_s-B_t}^2=C\abs{s-t}\rightarrow 0\).
        \subsubsection{}
\label{ex3.13c}
        Let \(X_t\in \mcV(S, T)\) be CMS. Then we show
        \[
          \int_0^T X_t \d{B_t}=\lim_n\int_S^T\phi_n(t, \omega)\d{B_t(\omega)}\,\,,
        \]
        where \(\phi_n(t, \omega)=\sum_jX_{t_j^{(n)}}(\omega)1\ca{t\in[t_j,t_{j+1})}\).

        Per Lemma~\ref{lemma:validelem}, it suffices to show convergence in \(L^2(\P\times [S,T])\).


        \begin{align*}
          \E\int_S^T\pa{X_t-\phi_n(t,\cdot)}^2\d{t}
          &=\sum_j\E\int_{t_j^{(n)}}^{t_{j+1}^{(n)}}(X_t-X_{t_j^{(n)}})^2\d{t}\\
          &=\sum_j\Delta_jt\E\sup_{s\in[t_j^{(n)},t_{j+1}^{(n)}]}\pa{X_{t_j}-X_s}^2\\
          &\le\frac{1}{n}\sum_j \epsilon_n^2=\epsilon_n^2\rightarrow 0\,\,,
        \end{align*}
        where we apply a uniform bound \(\epsilon_n\ge \E^{1/2}(X_t-X_{s})^2\) for any \(t,s\) within \(n^{-1}\) distance of one another holding on \([S, T]\), with \(\epsilon_n\rightarrow 0\).

        The aformentioned \(\epsilon_n\) sequence exists by virtue of uniform continuity of \(X\) as a function from indices in \([S, T]\) to random variables in \(\Omega\rightarrow\R\).

        CMS is by definition continuity of \(X:[S,T]\rightarrow\R^{\Omega}\) viewing \([S, T]\) as a standard Euclidean compact metric space and \(\R^\Omega\) as a standard topological space induced by the metric \(L^2(\P)\). By compactness \(X\) must be uniformly continuous.


        \subsection{}

        A function \(h(\omega)\) is \(\mcF_t\) measurable if and only if it can be written as
        \[
h(\omega)=\sum_{i=1}^\infty \prod_{j=1}^{k_i}g^{(i)}_j\pa{B_{t_j^{(i)}}}\,\,,
\]
where \(\mcF_t\) is the natural filtration over Brownian motion up to time \(t\), and the infinite sum converges as a limit to \(h\) almost surely.

Considering clamped versions of \(h\) and diagonalizing, without loss of generality \(h\) may be bounded.

Next, consider any shrinking split sequence \(t_j^{(n)}\) and let \(\mcH_n=\sigma\ca{B_{t_j^{(n)}}}_j\). By applying {\O}ksendal Appendix C.9, \(\CE{h}{\mcH_n}\convas \CE{h}{\mcH_\infty}\).

First, we show that, almost surely,
\[
  \CE{h}{\mcH_\infty}=\CE{h}{\mcF_t}=h\,\,,
\]
which follows from the left-continuity of the filtration \(\mcF_t=\bigcap_{\epsilon<0}\mcF_{t-\epsilon}\subset\bigcup_n\mcH_n\) for each \(s\) (since we can take intersection over rational \(\epsilon\)). The filtration is left-continuous by assuming a version of \(B_t\) which is (see \url{https://math.stackexchange.com/questions/1716856/left-continuity-of-a-l%C3%A9vy-filtration}{discussion here} and \url{https://mathoverflow.net/questions/46957/right-continuity-of-natural-filtrations}{here}).

Then by Doob-Dynkin, \(\CE{h}{\mcH_n}=G_n(B_{t_1},B_{t_2},\cdots, B_{t_k})\) for some Borel \(G_n\); which per the text can be approximated by a sequence of continuous functions \(F_n\) (we diagonalize the iteration again).

Stone-Weierstrass then tells us that each such continuous function can be approximated by a polynomial basis; which yields the desired result after expansion and a further diagonalization of iteration.

        \subsection{}

        Suppose \(f,g\in\mcV\) and that
        \[
          C+\int_S^Tf\d{B_t}=D+\int_S^Tg\d{B_t}
        \]
        holds almost surely.

        Then, since the equation holds in expectation, it must be that \(C=D\). This implies the integrals are almost surely equal \((*)\).

        Let \(\phi_n\rightarrow\ f\) and \(\gamma_n\rightarrow g\) in \(L^2(\P\times[S,T])\), elementary functions which must exist by It\^{o}'s construction. Then,

        \begin{align*}
          0
          &=\norm{\int_S^Tf\d{B_t}-\int_S^Tg\d{B_t}}_{L^2(\P)}&(*)\\
          &=\norm{\lim_n\int_S^T\phi_n\d{B_t}-\int_S^T\gamma_n\d{B_t}}_{L^2(\P)}&\\
          &=\lim_n\norm{\int_S^T\phi_n\d{B_t}-\int_S^T\gamma_n\d{B_t}}_{L^2(\P)}&\text{DCT }L^2(\P)\\
          &=\lim_n\norm{\phi_n-\gamma_n}_{L^2(\P\times[S,T])}&\text{It\^{o}'s Lemma}\\
          &=\norm{\lim_n\phi_n-\gamma_n}_{L^2(\P\times[S,T])}&\text{DCT }L^2(\P\times[S,T])\\
          &=\norm{f-g}_{L^2(\P\times[S,T])}\,\,,
        \end{align*}
        which implies that \(f,g\) are indeed \(\pa{[S,T]\times\Omega}\)-almost equal.

        \subsection{}

        
        By Jensen's inequality, for a square integrable \(X\) (it must be square integrable to define the conditional expectation), for any \(\sigma\)-algebra \(\mcF\):
        \[
          \E\CE{X}{\mcF}^2\le \E\CE{X^2}{\mcF}=\E X^2\,\,,
        \]
        by applying the tower property.

        \subsection{}

        Let \(X\) be an integrable random variable and \(\mcG\) be a finite \(\sigma\)-algebra generated by a partition \(\ca{G_i}\) of \(\Omega\), with non-null parts. We then show that, surely,
        \[
          \CE{X}{\mcG}=\sum_i\frac{1_{G_i}}{\P G_i}\int_{G_i}X\d{\P}\,\,.
        \]

        Recall by the projection property that \(\CE{X}{\mcG}\) is the unique random variable satisfying
        \[
          \E\ha{\CE{X}{\mcG}1_G}=\E\ha{X1_G}
        \]
        for all \(G\in \mcG\). Taking in particular \(G=G_i\); as shown in Exercise 2.7, as \(\CE{X}{\mcG}\) must be constant on \(G_i\). Let this constant be \(v_i\). To match expectations, we must have
        \[
          v_i\P G_i+0=\E\ha{X1_{G_i}}\,\,,
        \]
        by directly evaluating the expectation on the LHS and solving for \(v_i\) concludes the proof.
        
\end{document}