\documentclass{article}

\title{It\^{o} Integrals}
\author{Vladimir Feinberg}

\usepackage{fullpage}

\input{defs}

\begin{document}

\maketitle

\section{Construction of the It\^{o} integral}

We recall our original objective, which is to define some stochastic notion of differential equations parameterized by \(b,\sigma\) to identify some process \(X_t\) satisfying
\[
X'(t) = b(t, X_t)+\sigma(t, X_t)\cdot \text{``noise"}\,\,.
\]

It's sensible to consider the forward Euler method, for some shrinking split sequence \(t_i\), which would set
\[
  \Delta_kX=b(t_k, X_{t_k})\Delta_k t+\sigma(t_k, X_{t_k})\Delta_k V\,\,,
  \]
  where \(\Delta_k f=f\pa{t_{k+1}}-f\pa{t_{k}}\) and \(V_t\) is some stochastic process.

  Based on natural assumptions about \(\Delta_k V\) such as stationarity, independent increments, and 0-mean, we might be tempted to choose Brownian motion \(V_t=B_t\). In the limit as \(\Delta_kt\rightarrow 0\), we would be interested in finding a definition of an integral which is consistent with the limit of the discrete sums implied by the Euler method:
  \[
X_t=X_0+\int_0^t b(s, X_s)\d{s}+\int_0^t\sigma(s, X_s)\d{B_s}\,\,,
\]
where \(\int_0^t\sigma(s, X_s)\d{B_s}\) is not yet sensibly defined or shown to exist uniquely as \(\lim_n\sum_{k=1}^n\sigma(t_k, X_{t_k})\Delta_i B\).

\begin{definition}[It\^{o} Integral]
  Consider the probability space \((\Omega,\mcF,\P)\) where \(\mcF=\mcF_\infty\) for a filtration \(\ca{\mcF_t}\). Let \(\mcV =\mcV(S, T)\) be the class of functions \(f: \R_+\times \Omega\rightarrow\R\) satisfying the following properties:
  \begin{enumerate}
  \item \(f\) is \(\mathcal{B}\pa{\R_+}\times\mcF\) measurable.
  \item \(f(t, \omega)\) is \(\mcF_t\)-adapted.
  \item \(L^2(\P \times [S,T])(f)=\E^{1/2}\ha{\int_S^T f(t,\omega)^2\d{t}}<\infty\)
  \end{enumerate}

  Then under some shrinking split sequence (see previous chapter notes), the It\^{o} Integral is defined as the limit
  \[
\int_S^Tf(t, \omega)\d{B_t(\omega)}=\lim_{n\rightarrow\infty}\sum_{k=1}^n\phi_n(t_k, B_{t_k}(\omega))\Delta_i B(\omega)\,\,,
\]
where \(\phi_n\) are simple functions chosen by diagonalizing multiple other unique limit sequences. These \(\phi_n\) will be recovered from multiple other sequences as we grow the complexity of \(f\) to the full extent of \(\mcV\). To start, if \(f\) is a simple function, then of course \(\phi_n=f\) suffices. Afterwords, we show the convergence under \(L^2(\P \times [S, T])\) of limit sequences for growing subsets of $\mcV$:
\begin{enumerate}
\item Bounded and everywhere continuous processes. Since simple functions approximate our continuous process everywhere on \(\Omega\), by bounded convergence they approximate in \(L^2(\P \times [S, T])\).
\item Bounded processes \(g\in \mcV\). As \(n\rightarrow\infty\), consider a bump function \(\psi_n\) with shrinking support immediately left of the origin. Consider the convolution \(g_n=\psi_n * g\), also in \(\mcV\) by virtue of the negative support of \(\psi_n\). Note that \(g_n\rightarrow g\) pointwise as \(\psi_n\) progressively approximates the Dirac measure. Then again by bounded convergence we have \(g_n\rightarrow g\) in \(L^2(\P \times [S, T])\), where \(g_n\) are bounded and everywhere continuous by construction.
\item Any process in \(g\in \mcV\). Let \(g_n\) be \(g\) clamped to \([-n, n]\). By the previous step, we have convergence to each such \(g_n\) via limit sequence in \(\mcV\). Then since \(g\in L^2(\P \times [S,T])\) itself, by dominated convergence we have convergence in \(g\in L^2(\P \times [S,T])\).
\end{enumerate} 
\end{definition}

For any \(f\in\mcV\) we may diagonalize the above sequences for some {\em elementary} functions \(\phi_n\) to define the It\^{o} integral. Why does this definition give rise to a {\em unique} integral, independent of the choice of \(\phi_n\)? To see this, we need a critical tool.

\begin{lemma}[It\^{o} Isometry]
  For any \(f\in \mcV\),
  \[
    \E\ha{\int_S^Tf(t, \omega)^2\d{t}}
    = \E\ha{\pa{\int_S^Tf(t, \omega)\d{t}}^2}\,\,,
  \]
  or, in operator form,
  \[L^2(\P \times [S, T])(f)=L^2(\P)\circ \int_S^T\d{t}\,\,.\]
\end{lemma}

For elementary functions, this can be proven by computing both sides explicitly and relying on stationarity of increments. In turn, this implies that for any \(f\in\mcV\), for any \(L^2(\P \times [S,T])\) sequence \(\phi_n\) of elementary functions, which is necessary Cauchy in that metric, must give rise to a sequence of integrals \(I_n=\int_S^T\phi_n\d{B_t}\) which are Cauchy in \(L^2(\P)\) and thus converge as random variables in this way to a fixed limit \(I = I(f, \phi)\) in \(L^2(\P)\). But for any other similar elementary function sequence \(\phi_n'\) converging to \(f\) in \(L^2(\P \times [S,T])\), we construct an analogous sequence of integrals \(I_n'\), also Cauchy in \(L^2(\P)\), thus also yielding an It\^{o} integral \(I'=I(f, \phi')\). Through our elementary isometry,
\begin{align*}
  \norm{I_n-I'}_{L^2(\P)}&\le\norm{I_n-I_n'}_{L^2(\P)}+\norm{I_n'-I'}_{L^2(\P)}\\
                         &=\norm{\phi_n-\phi_n'}_{L^2(\P)\circ\int_S^T\d{t}}+\norm{I_n'-I'}_{L^2(\P)}\\
                         &=\norm{\phi_n-\phi_n'}_{L^2(\P\times[S,T])}+\norm{I_n'-I'}_{L^2(\P)}\\
                           &\rightarrow 0\,\,,
\end{align*}
but since \(I_n\) converges to both \(I,I'\) in \(L^2(\P)\), by completeness of the corresponding metric space, \(I=I'\) almost surely, showing both invariance of the It\^{o} integral to the specific limit sequence \(\phi_n\) chosen and It\^{o} isometry on all of \(\mcV\).

\section{Some properties of the It\^{o} integral}

By taking limits from the definition, the following properties of the It\^{o} integral hold almost always:

\begin{enumerate}
\item \(\int_S^Tf\d{B_t}=\int_S^Uf\d{B_t}+\int_U^Tf\d{B_t}\),
\item \(\int_S^T \pa{cf+g}\d{B_t}=c\int_S^Tf\d{B_t}+\int_S^Tg\d{B_t}\) for constant \(c\),
\item \(\E \int_S^Tf\d{B_t} =0\),
\item \(\int_S^Tf\d{B_t} \in\mcF_T\),
\item and \(M_s=\int_0^s f\d{B_t}\) is a martingale.
\end{enumerate}

\begin{theorem}
  Let \(f\in\mcV(0, T)\). Then there exists a \(t\)-continuous version of \(I_t(\omega)=\int_0^tf(s, \omega)\d{B_s(\omega)}\).
\end{theorem}
\begin{proof}
  Let \(\phi_n\) be elementary functions tending to \(f\) in \(L^2(\P \times [0, T])\) from It\^{o}'s construction, with corresponding It\^{o} integrals \(I_t^{(n)}\). For any \(n,m\), \(I_t^{(n)}-I_t^{(m)}\) is a martingale since its components are. \href{https://math.ucsd.edu/~pfitz/downloads/courses/winter05/math280b/doob.pdf}{Doob's Martingale Inequality}, which is basically Markov for martingales, relates
  \[
    \P\ca{\sup_{0\le t\le T}\abs{I_t^{(n)}-I_t^{(m)}}\ge \epsilon}\le\frac{1}{\epsilon^2}\E\abs{I_t^{(n)}-I_t^{(m)}}^2=\frac{1}{\epsilon^2}\norm{\phi_n-\phi_m}_{L^2(\P\times[0,T])}^2\,\,.
  \]
  Since the RHS vanishes as \(n,m\) grow large, a subsequence \(I_t^{(n_k)}\) may be chosen ensuring with geometrically decreasing \(\epsilon\) between adjacent \(I_t^{(n_k)},I_t^{(n_{k+1})}\). This allows Borel-Cantelli to be applied over the set of events \(\ca{\sup_{0\le t\le T}\abs{I_t^{(n_k)}-I_t^{(n_{k+1})}}\ge 2^{-k}}\) for varying \(k\in\N\), so in fact with probability one \(\sup_{0\le t\le T}\abs{I_t^{(n_k)}-I_t^{(n_{k+1})}}\le 2^{-k}\) so long as \(k\ge K(\omega)\). This provides uniform convergence for almost all \(\omega\), and thus continuity in the limit.
  \end{proof}

\section{Extensions of the It\^{o} integral}

Without much effort, the filtration \(\mcF\) can be expanded to any other filtration \(\mcH\) such that \(\mcF_t\subset\mcH_t\).

Relaxing the integrator to be an arbitrary semimartingale instead of Browninan motion is possible. Revisiting our construction steps, the only direct properties used were bounded quadratic variation and adaptedness. These can be further relaxed to just \href{https://en.wikipedia.org/wiki/Semimartingale}{semimartingale} properties.

The second moment condition in It\^{o}'s construction can be relaxed to a mere condition requiring that the second moment of our integrand is almost surely finite, \(\P\ca{\int_S^Tf(s,\omega)^2\d{s}<\infty}=1\), however, this of course will end up creating non-integrable integrals, and thus drop the martingale property. However, continuity can be recovered, along with a local martingale property.

There also exists another interpretation of stochastic integrals. The Stratonovich is built from a trapezoidal rule (unlike It\^{o}'s left-sided integration rule) as the building block:
  \[
\int_S^Tf(t, \omega)\circ \d{B_t(\omega)}=\lim_{n\rightarrow\infty}\sum_{k=1}^n\phi_n\pa{\frac{t_k+t_{k+1}}{2}, B_{t_k}(\omega)}\Delta_i B(\omega)\,\,.
\]
One reason the Stratonovich interpretation might be more natural is because of the following motivation. If we consider \(t\)-continuously differentiable approximations \(B_t^{(n)}\) to our noise process \(B_t\) which on bounded intervals converge uniformly almost always, then the solution \(X^{(n)}_t\) to the differential equation (for constant \(\omega\))
\[
\frac{d X_t}{dt}=b(t,X_t)+\sigma(t,X_t)\frac{dB_t^{(n)}}{dt}\,\,,
\]
converges to the Stratonovich solution \(X_t=X_0+\int_0^tb(t, X_s)\d{s}+\int_0^t\sigma(t, X_s)\circ\d{B_s}\), whereas the solution expressed in the It\^{o} interpretation would be
\[
  X_t=X_0+\int_0^tb(t, X_s)\d{s}+\frac{1}{2}\int_0^{t}\sigma'(s, X_s)\sigma(s, X_s)\d{s}+\int_0^t\sigma(t, X_s)\d{B_s}
  \]

\section{Exercises}

\subsection{}
\label{pt1}
We show
\[
  \int_0^t s\d{B_s}=tB_t-\int_0^tB_s\d{s}
\]
directly from the definition of It\^{o} integrals. We consider the progressively finer elementary functions defined by the discretization of the identity integrand as \(n\rightarrow\infty\) for some shrinking split sequence \(t_j\), which tend to the identity in \(L^2(\P\times[0,t])\).
\[
  \sum_it_i\pa{B_{t_{i+1}}-B_{t_{i}}}=  t_nB_t-\sum_i\pa{t_{i+1}-t_{i}}B_{t_i}\,\,,
\]
so that the limit converges to the desired solution, because almost-always continuity of \(B_t\) permits \(\omega\)-pointwise Riemann integration.

\subsection{}
\label{pt2}

We show
\[
  \int_0^t B_s^2\d{B_s}=\frac{1}{3}B_t^3-\int_0^tB_s\d{s}
\]
directly from the definition of It\^{o} integrals. A more general technique for solving such integrals from the definition follows in three steps:

\begin{enumerate}
\item Rewrite the expected naive Riemann integral solution (here, \(\frac{1}{3}B_t^3\)) as a telescoping sum. \(B_t^3=\sum_j\Delta_jB^3\).
\item Express each term in an It\^{o}-friendly way, i.e., as functions of increments or the left sided-timestep, i.e., \(B_{t_j},\Delta_jB\).
  \begin{align*}
    \Delta_jB^3&=\Delta_jB\pa{B_{t_{j+1}}^2+B_{t_{j+1}}B_{t_j}+B_{j}^2}\\
    &=\Delta_jB\pa{B_{t_{j+1}}^2-2B_{t_{j+1}}B_{t_j}+B_{j}^2}+3B_{t_{j+1}}B_{t_j}\Delta_jB\\
    &=\Delta_j^3B+3(\Delta_jB+B_{t_j})B_{t_j}\Delta_jB\\
    &=\Delta_j^3B+3B_{t_j}\Delta_j^2B+3B_{t_j}^2\Delta_jB
  \end{align*}
\item Summing the above formula over \(j\) and rearranging terms from the first telescope we have
  \[
    \sum_jB_{t_j}^2\Delta_jB=\frac{1}{3}B_t^3-\frac{1}{3}\sum_j\Delta_j^3B-\sum_jB_{t_j}\Delta_j^2B\,\,.
  \]

  In the limit as \(n\rightarrow\infty\), the LHS converges to the desired It\^{o} integral \(  \int_0^t B_s^2\d{B_s}\) in \(L^2(\P)\). The middle term vanishes since \(\abs{\sum_j\Delta_j^3B}\le\sum_j\abs{\Delta_j^3B}\rightarrow[B]_t^{(3)}=0\). To finish, it remains to show that the last term simplifies to \(\int_0^tB_s\d{s}\).

  Notice it suffices to show that \(\|\sum_jB_{t_j}\Delta_j^2B-\sum_jB_{t_j}\Delta_jt\|_{L^2(\P)}\rightarrow 0\) as the \(L^2\) convergence implies an almost surely convergent subsequence, and by uniqueness of the It\^{o} integral definition, this shows almost sure equality on the final integrals regardless of the elementary function sequence we look at. Indeed,
\begin{align*}
  \E\pa{\sum_jB_{t_j}(\Delta_j^2B-\Delta_jt)}^2
    &=\E\sum_jB_{t_j}^2(\Delta_j^2B-\Delta_jt)^2+2\E\sum_j\sum_{k> j}B_{t_j}B_{t_k}(\Delta_j^2B-\Delta_jt)(\Delta_k^2B-\Delta_kt)\,\,.
\end{align*}
In the first term, independence simplifies calculation, yielding \(\E B_{t_j}^2\E(\Delta_j^2B-\Delta_jt)^2=O(tn^{-2})\) term-wise, leveraging the variance of a standard \(\chi^2\) distribution, which still vanishes in an \(n\)-sized sum. The second term can be decomposed
\[
  \E \ha{B_{t_j}B_{t_k}(\Delta_j^2B-\Delta_jt)(\Delta_k^2B-\Delta_kt)}
  =  \E \ha{B_{t_j}B_{t_k}(\Delta_j^2B-\Delta_jt)}\E\ha{(\Delta_k^2B-\Delta_kt)}\,\,,
\]
where the last term vanishes. Of course, \(\Delta_jt=n^{-1}\) for all \(j\) above.
  \end{enumerate}


  \subsection{}

  Let \(X_t\) be a stochastic process.
\subsubsection{}
If \(X_t\) is a martingale with respect to any filtration \(\mcG_t\), it must be a martingale with respect to its own natural filtration \(\mcF_t\). Measurability and integrability conditions hold immediately, so it suffice to check the conditional martingale property. In addition, by virtue of \(X_t\in \mcG_t\) we must have \(\mcF_t\subset\mcG_t\) through an inductive argument. Taking \(s\le t\), by tower, \(\CE{X_t}{\mcF_s}=\CE{\CE{X_t}{\mcG_s}}{\mcF_s}=\CE{X_s}{\mcF_s}=X_s\).
\subsubsection{}
By the above, for any martingale \(X_t\), we have for all \(t\ge 0\),
\[
  \E X_t=\E\CE{X_t}{\mcF_0}=\E X_0\,\,.
\]
\subsubsection{}
The above may be satisfied for non-martingales; consider \(X_t=t\epsilon\) where \(\epsilon\) is Rademacher-distributed.
\subsection{}
\subsubsection{}
\(X_t=B_t+4t\) is not a martingale because its expectation is strictly increasing with time (see previous problem).
\subsubsection{}
\(X_t=B_t^2\) is not a martingale because its expectation is strictly increasing with time.
\subsubsection{}
\(X_t=t^2B_t-2\int_0^tsB_s\d{s}\) is a martingale. Integrability and measurability hold immediately. Then notice
\[
  \CE{X_t}{\mcF_s}=t^2B_s-2\int_0^sB_r\d{r}-2\int_s^tr\CE{B_r}{\mcF_s}\d{r}=t^2B_s-2\int_0^sB_r\d{r}-2B_s\int_s^tr\d{r}=X_s\,\,.
\]
\subsubsection{}
\(X_t=B_t^{(1)}B_t^{(2)}\) is a martingale, where \(B_t^{(i)}\) are independent brownian motions, so the conditional expectation operator will distribute.

\subsection{}

\(M_t=B_t^2-t\) is a martingale. Measurability and integrability are apparent, and \(\CE{M_t}{\mcF_s}=B_s^2+\CE{2B_s(B_t-B_s)+(B_t-B_s)^2}{\mcF_s}-t=M_s\) by splitting up \(B_t=B_s+(B_t-B_s)\).

\subsection{}

\(N_t=B_t^3-tB_t\) is a martingale. Measurability and integrability are apparent. Performing the same expansion \(B_t=B_s+\Delta\) as before, we find
\[
  \CE{N_t}{\mcF_s}=
  \CE{\Delta^3 + 3\Delta^2 B_s+3\Delta B_s^2-3tB_t+B_s^3}{\mcF_s}=
  3(t-s) B_s-3tB_s+B_s^3=N_s
\]\label{pt3}
\subsection{}
Consider the iterated It\^{o} integral
\[
  I_n=n!\int_0^{t}\int_0^{u_n}\cdots\int_0^{u_2}\d{B_{u_1}}\d{B_{u_2}}\cdots\d{B_{u_n}}\,\,.
\]

\subsubsection{}

For every tuple \(u_2,\cdots, u_n\), necessarily in sorted order already, the innermost integrand \(1\) is measurable with respect to \(\mcB(\R_+)\times \mcF\). It is also adapted to \(\mcF_{u_1}\) trivially. Finally, it's square integrable, as the entire integral \(\int_0^{u_2}1^2\d{B_1}\le u_2\le t\). The resulting integral \(I^{(n)}_1(u_2,\cdots, u_n)=\int_0^{u_2}1\d{B_1}\) is by properties of It\^{o} integration itself measurable and adapted to \(\mcF_{u_2}\), as well as dominated by \(t\) over \(u_2\in[0,u_3]\).

Then, inductively, assume for a tuple \(u_i,\cdots, u_n\), the corresponding integrand \(\int_0^{u_{i}}\cdots\d{B_{u_{i-1}}}\) is measurable, adapted, and square integrable due to being dominated by \(t^{i-1}/(i-1)!\). Then adding another integral again results in a measurable and adapted random process dominated in \(L^2(\P \times [S,T])\) by \(t^i/i!\).

\subsubsection{}

Manually, we can verify that
\[
I_1=B_t\,\,,
\]
and since \(\int_0^{u_2}\d{B_{u_1}}=B_{u_2}\) that
\[
  I_2=B_t^2-t\,\,,
\]
and finally,
\[
  I_3=3 \int_0^{t}B_{u_2}^2-u_2\d{B_{u_2}}=B_t^3-3tB_t\,\,,
\]
by combining Exercises~\ref{pt1} and~\ref{pt2}.

We notice that all of the above are of the form \(I_n=t^{n/2}h_n\pa{\frac{B_t}{\sqrt{t}}}\), where \(h_n\) is the Hermite polynomial of degree \(n\), where
\[
  h_n(x)=(-1)^n\exp\pa{\frac{x^2}{2}}\partial_x^n\exp\pa{\frac{-x^2}{2}}
\]
\subsubsection{}

By the above, \(N_t\) from Exercises~\ref{pt3} must be a martingale as it is an It\^{o} integral.

\subsection{}


\subsubsection{}
Let \(Y\) be an integrable rv and let \(M_t=\CE{Y}{\mcF_t}\)
\(M_t\) is a martingale, following immediately from the tower property.
\subsubsection{}
Conversely, suppose \(M_t\) is a real-valued \(\mcF_t\)-martingale which satisfies for some \(p\) that \(\sup_t\E\abs{M_t}^p<\infty\). We can show there exists some integrable \(Y\) such that \(M_t=\CE{Y}{\mcF_t}\).

By Corollary C.7 in the appendix (also, for discrete settings, by Theorem 4.7 from \c{C}inlar's Probability and Stochastics,  p.201, Theorem 4.7, the characterization of uniformly integrable martingales), {\em with the additional assumption that \(M_t\) is continuous, an assumption absent from {\O}ksendal}, we have that an integrable \(Y\) exists such that \(M_t\convarg{L^1}Y\).

Then, notice that \(\CE{Y-X_t}{\mcF_t}=0\) if \(\E\ha{1_H\pa{Y-X_t}}=0\) is for all events \(H\in \mcF_t\).
But \(1_H\pa{X_s-X_t}\convarg{L^1}1_H\pa{Y-X_t}\) as \(s\rightarrow\infty\) by analyzing the cases of \(1_H\in\{0,1\}\). So \(\E\ha{1_H\pa{Y-X_t}}=\lim_{s\rightarrow\infty}\E\ha{1_H\pa{X_s-X_t}}\) where for \(s\ge t\)
\[
  \E\ha{1_H\pa{X_s-X_t}}=\E \ha{1_H\CE{X_s-X_t}{\mcF_t}}=0\,\,.
\]

The continuity assumption seems load-bearing, but uninterestingly so. With \(\epsilon\) a Rademacher random variable, \(M_t=\epsilon 1\ca{t\ge 1}\), and a large constant filtration \(\mcH_t=\sigma\{\epsilon\}\) (note the natural filtration has \(\mcF_t=\emptyset\) for \(t<1\)), then \(M_t=0\neq \epsilon=\CE{\epsilon}{\mcH_{t}}\) for any \(t<1\).

\end{document}