\documentclass{article}

\title{Complexity Measures}
\author{Vladimir Feinberg}

\input{../defs}

\begin{document}

\maketitle

Complexity measures evaluate the expressiveness of a hypothesis class; they are useful to the extent with which they relate sample and generalization error.

\section{Setup}

We suppose that our data comes in the form of ordered pairs from $\mcX\times \mcY$. Samples follow a particular distribution $(x, y)\sim D$. A hypothesis class $\mcH$ is set of functions $\mcX\rightarrow\mcY$.

A common approach to supervised learning is ERM, where $m$ iid samples from $D$, $S$, are used to find the $h\in\mcH$ minimizing a specified loss $\ell:\mcY^2\rightarrow\R$ over this set. Complexity measures then let us quantify exactly how much loss we can expect when sampling from $D$ again.

We seek to quantify the generalization gap with the help of our notions of complexity. For a fixed $h\in \mcH$:

$$
\varepsilon= \E\left[\ell\pa{h(x), y)}|(x,y)\sim D\right]-\E\left[\ell\pa{h(x), y)}|(x,y)\sim \Uniform(S)\right]
$$

Analysis of Rademacher complexity is agnostic to $h,\ell$; the hypothesis class might as well consist of functions $g:\mcX\times\mcY\rightarrow\R$ yielding their composition. VC dimension analysis, however, requires $\mcY=\{0, 1\}$ and $\ell(a, b)=\indicator\{a=b\}$. VC dimension is still useful for regression problems, by thresholding hypotheses $h\mapsto \indicator{h>\beta}$ for fixed $\beta$.\footnote{\url{https://stats.stackexchange.com/questions/140430}}


Thus, it is useful to find bounds on $\varepsilon$, the difference between the generalization loss $\E\left[\ell\pa{h(x), y)}\right]$, where $(x,y)\sim D$, and sample loss, where the loss is the expectation before taken for $(x,y)$ is uniform over $S$.

Let the gap between the generalization and sample error be $\varepsilon$.

\section{Complexity Measures}

The empirical Rademacher complexity $\hat{R}_S$ assumes a fixed sample $S$ from $D^m$. It relates complexity of a function class $\mcG$ containing vectorized functions $g\in \mcG$ which take elements $z_i=(x_i, y_i)$ in $S$ and return costs through the correlation of $\mcG$ with noise. Let $\vsigma\sim \Uniform\pa{\pm 1}^m$. Rademacher complexity is then the average empirical one.
$$
\hat{R}_S(\mcG)=\E_\vsigma \sup_g \frac{1}{m}\sum_{i=1}^m{g(z) \sigma_i},\;\;\; R_m(\mcG)=\E_S\hat{R}_S(\mcG)
$$

VC dimension accomplishes a similar task for binary classification by rating the complexity of a hypothesis class $\mcH$. Let hypotheses $\mcH\ni h:\mcX\rightarrow \mcY=\ca{\pm 1}$ be applied elementwise over a vector of inputs $\vx$. First we define the growth function $\Pi_\mcH:\N\rightarrow\N$, which defines the maximum number of distinctions a hypothesis class can make over all sets of points in the input space:
$$
\Pi_\mcH(m)=\max_{\vx\in\mcX^m}\card{\set{h(\vx)}{h\in\mcH}}
$$
Then the VC dimension of $\mcH$ is then $\max\set{m\in\N}{\Pi_\mcH(m)=2^m}$.

\section{Overview of Results}

Proofs can be found in a \nurl{http://ittc.ku.edu/~beckage/ml800/VC_dim.pdf}{cogent write-up} by Prof. Beckage from the University of Kansas.

\subsection{VC Generalization Bounds}

Upper bound. If $d$ is the VC-dimension of $\mcH$, then for any $D$ wp $1-\delta$:
$$
\varepsilon\le \tilde{O}\pa{\sqrt{\frac{d-\log \delta}{m}}}
$$
The above inequality is random since it depends on $S$, the $D^m$-valued rv. TODO. find source removing tilde?

Agnostic lower bound. We may find a $D$ such that with a fixed nonzero probability (a non-negligible set of candidate samples $S$), the following holds:
$$
\varepsilon\ge \Omega\pa{\sqrt{\frac{d}{m}}}
$$

The above implies that in the common case of agnostic hypothesis learning, where we do not know distribution $D$, VC-dimension is, \textit{up to logarithmic factors, asymptotically efficient} in quantifying the generalization gap.

Realizability. Suppose $D$ is realizable wrt $\mcH$, so that there exists an $f\in\mcH$ such that for almost any $(x,y)$ sampled from $D$, $f(x)=y$. Then all statements above hold but with $\sqrt{\varepsilon}$ instead of $\varepsilon$.


\subsection{Growth Function Bounds}

Sauer's Lemma implies that VC dimension $d$ bounds the growth function: in a graph of the logarithm of the growth function vs $m$, growth is linear since $\Pi_\mcH(n)=n$ for $n\le d$. Then for $n>d$, growth is at most logarithmic, i.e., $\log\Pi_\mcH=O(\log m)$. With Massart's Lemma we have wp $1-\delta$:
$$
\varepsilon\le O\pa{\sqrt{\frac{\log\Pi_\mcH(m)-\log\delta}{m}}}
$$
Since the above would be large if $\log\Pi_\mcH(m)\simeq m$, it is clear why Sauer's Lemma enables the essential relationship between learnability and complexity.

\subsection{Rademacher bounds}

With $R_m$ either the empirical or expected Rademacher complexity over the sample for a given $h,\ell$ we have again wp $1-\delta$:
$$
\varepsilon\le 2R_m+O\pa{\frac{\log\nicefrac{1}{\delta}}{m}}
$$
$R_m$ may be NP-hard to compute, depending on $\mcH$. This tells us Rademacher complexity could only be a useful improvement over VC-bounds, asymptotically, if we have an efficient approximation for the empirical Rademacher complexity or some knowledge of $D$ as required to compote the true Rademacher complexity.

\section{Hardness of Learning}

Rademacher and Gaussian Complexities: Risk Bounds and Structural Results by Bartlett and Mendelson.

\end{document}